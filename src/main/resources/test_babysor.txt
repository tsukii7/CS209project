小白求教：运行工具箱时报错“AttributeError: 'Toolbox' object has no attribute 'selected_source_utterance'”
'Toolbox' object has no attribute 'selected_source_utterance'工具箱右上边，输入框下边的Vocode only按钮也是灰色的，只能合成没有声音输出工具箱左下角Toolbox outpup 也是无法加载选项 
请更新依赖 requirements.txt ，web.py 所需的一些包并未涵盖在其中
 中所缺失的包： **_注：其中倒数第二行， click 版本应注明保持在 8.0.0 ，新版本会导致 "get_os_args" 的报错。其他包最新版本仍可用_** 
add docker support
增加docker支持使用 构建镜像使用 测试 
Intel MacOS平台报错 No module named 'typer'
**Summary[问题简述（一句话）]**Intel MacOS平台报错 No module named 'typer'**Env & To Reproduce[复现与环境]** 您好，您遇到的问题是由于缺失所需要的包，您可以按照以下方式解决：1. 下载文件 2. 进入命令行，输入以下命令： 
AI SHELL3 语音集 label_train-set.txt 貌似项目没有用起来
这个一般怎么用？ code 有 接口吗？ 这个感觉 很重要 。自己做语音集的时候 感觉这个白白浪费手工了。 
合成器训练的loss一直在0.49附近震荡，无法降低
你好，想请教一下，使用了 aidatatang_200zh，aishell3，magicdata 这三个数据集合并在一起训练合成器，大概从50k步开始，运行到123k步，loss值都在0.49附近震荡，没法进一步收敛。请指教一下方向，应该如何处理。另外，请问有没有可供交流的微信群或QQ群？谢谢！![屏幕截图 2022-12-09 2022-12-09 steps是不是有点少 我觉得大概训练到600k 的底子为基础。然后针对不同的 声音自己做训练集 才能达到说的过去的效果吧。我目前用的 issue里不记得谁分享的650k 模型。基础上 对自己想要的声音做训练。 之后效果 感觉 真的不错。嘿嘿。> steps是不是有点少 我觉得大概训练到600k 的底子为基础。 然后针对不同的 声音自己做训练集 才能达到说的过去的效果吧。 我目前用的 issue里不记得谁分享的650k 模型。基础上 对自己想要的声音做训练。 之后效果 感觉 真的不错。 嘿嘿。感觉有点问题，作者在readme里说他自己训练在50k步之后就降到0.4以下，B站有位up主分享的36k步的模型，用起来效果比我的123k步的明显好。你说的650k的模型，能否放网盘分享一下？ :)> > steps是不是有点少 我觉得大概训练到600k 的底子为基础。 然后针对不同的 声音自己做训练集 才能达到说的过去的效果吧。 我目前用的 issue里不记得谁分享的650k 模型。基础上 对自己想要的声音做训练。 之后效果 感觉 真的不错。 嘿嘿。> > 感觉有点问题，作者在readme里说他自己训练在50k步之后就降到0.4以下，B站有位up主分享的36k步的模型，用起来效果比我的123k步的明显好。 你说的650k的模型，能否放网盘分享一下？ :)链接: 提取码: ruje  
请问使用预设模型还需要怎样的操作，是我哪里配置出问题了吗，都仅生成1秒左右的电流声音
显然有点少> 你找找那种500k左右的预训练集。200 显然有点少可否指路在哪里可以找到这种预训练集直接在网上搜mockingbird模型，然后下载下来用。预训练模型我也用不了> > 你找找那种500k左右的预训练集。200 显然有点少> > 可否指路在哪里可以找到这种预训练集就在咱们这个的issue列表里搜索就好了 
模型替换成Tacotron2时，推理出现问题

运行数据集：assert args.dataset in recognized_datasets, 'is not supported,
windows的问题，不允许跨盘这样访问我把aidatatang_200zh_2解压在D:\mockingbird中，在cmd运行数据集还是这样。请问都有哪些文件要放在同一个盘里?-d 后面的数据类型不要加大括号 
Refactor Project to 3 parts: Models, Control, Data
Need readme 
PPG训练时的报错，请帮忙看看
PPG预处理很顺利，ppg2mel.yaml路径也改了，但是这个错误提示怎么都解决不了，请大家帮忙看看能否有经验分享下。D:\MockingBird-main>python ppg2mel_train.py --config .\ppg2mel\saved_models\ppg2mel.yaml --oneshotvcTraceback (most recent call last):  File "D:\MockingBird-main\ppg2mel_train.py", line 67, in <module>    main()  File "D:\MockingBird-main\ppg2mel_train.py", line 50, in main    config = HpsYaml(paras.config)  File "D:\MockingBird-main\utils\load_yaml.py", line 44, in __init__    hps = load_hparams(yaml_file)  File "D:\MockingBird-main\utils\load_yaml.py", line 8, in load_hparams    for doc in docs:  File "C:\Users\benny\AppData\Local\Programs\Python\Python39\lib\site-packages\yaml\__init__.py", line 127, in load_all    loader = Loader(stream)  File "C:\Users\benny\AppData\Local\Programs\Python\Python39\lib\site-packages\yaml\loader.py", line 34, in __init__    Reader.__init__(self, stream)  File "C:\Users\benny\AppData\Local\Programs\Python\Python39\lib\site-packages\yaml\reader.py", line 85, in __init__    self.determine_encoding()  File "C:\Users\benny\AppData\Local\Programs\Python\Python39\lib\site-packages\yaml\reader.py", line 124, in determine_encoding    self.update_raw()  File "C:\Users\benny\AppData\Local\Programs\Python\Python39\lib\site-packages\yaml\reader.py", line 178, in update_raw    data = self.stream.read(size)UnicodeDecodeError: 'gbk' codec can't decode byte 0x86 in position 176: illegal multibyte sequence贴一下你的yml，看起来格式没搞对 
请问VC模式怎么用
我的Demo 
修复web界面vc模式不能用问题。改拼写错误、给字符串前加f标
警惕错别字问题，警钟敲烂 sorry，浪费您时间，刚才搞错了。我覆盖了app_vc修正。但是但现在还是有空字节错误提示 你截一下黑窗口里的报错我看下File "C:\Users\admin\anaconda3\lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py", line 564, in _run_script    exec(code, module.__dict__)File "C:\Users\admin\AppData\Local\Temp\tmp3j0w564f.py", line 14, in <module>    render_streamlit_ui()File "E:\MockingBird-main\mkgui\base\ui\streamlit_ui.py", line 841, in render_streamlit_ui    opyrator = getOpyrator(mode)File "E:\MockingBird-main\mkgui\base\ui\streamlit_ui.py", line 810, in getOpyrator    from mkgui.app_vc import convertFile "E:\MockingBird-main\mkgui\app_vc.py", line 8, in <module>    import ppg_extractor as ExtractorFile "E:\MockingBird-main\ppg_extractor\__init__.py", line 8, in <module>    from .encoder.conformer_encoder import ConformerEncoderFile "E:\MockingBird-main\ppg_extractor\encoder\conformer_encoder.py", line 28, in <module>    from .subsampling import Conv2dNoSubsampling, Conv2dSubsamplingFile "E:\MockingBird-main\ppg_extractor\encoder\subsampling.py", line 11, in <module>    from espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding> import ppg_extractor as Extractor根据我粗浅的python理解应该是【import ppg_extractor as Extractor】import这句都出错了，得作者看下了，不过应该和我这次的pr不相关 
运行demo_toolbox.py提示错误
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.运行demo_toolbox.py提示错误Arguments:    datasets_root:          None    vc_mode:                False    enc_models_dir:         encoder\saved_models    syn_models_dir:         synthesizer\saved_models    voc_models_dir:         vocoder\saved_models    extractor_models_dir:   ppg_extractor\saved_models    convertor_models_dir:   ppg2mel\saved_models    cpu:                    False    seed:                   None    no_mp3_support:         Falseqt.qpa.plugin: Could not find the Qt platform plugin "windows" in ""This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型python=3.10.8**Screenshots[截图（如有）]**If applicable, add screenshots to help改为使用 **_3.9.x_** 版本的 python 
数据集和模型的关系
数据集和模型是什么关系？分别有什么作用吗？生成的模型和导入的音频是否有关？数据集是不是由音频做成的？ 
3q
Arguments:    datasets_root:    None    enc_models_dir:   encoder\saved_models    syn_models_dir:   synthesizer\saved_models    voc_models_dir:   vocoder\saved_models    cpu:              False    seed:             None    no_mp3_support:   Falseqt.qpa.plugin: Could not find the Qt platform plugin "windows" in "C:\Qt\Qt5.12.2\5.12.2\mingw73_64\plugins\platforms"This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.Available platform plugins are: direct2d (from C:\Qt\Qt5.12.2\5.12.2\mingw73_64\plugins\platforms), minimal (from C:\Qt\Qt5.12.2\5.12.2\mingw73_64\plugins\platforms), offscreen (from C:\Qt\Qt5.12.2\5.12.2\mingw73_64\plugins\platforms), windows (from C:\Qt\Qt5.12.2\5.12.2\mingw73_64\plugins\platforms). 
替换作者的模型报错
各位大佬  替换作者模型运行以后报错如图  感谢各位指点一下![Uploading 屏幕截图 2022-11-20 2022-11-20 
【推荐】最新的科学上网方法

aishell3中的语音合成效果较差
Preparing the encoder, the synthesizer and the vocoder...Loaded encoder "pretrained.pt" trained to step 1594501Synthesizer using device: cudaBuilding hifiganLoading 'vocoder/saved_models/pretrained/g_hifigan.pt'Complete.Removing weight norm...Trainable Parameters: 0.000MLoaded synthesizer "mandarin.pt" trained to step 75000+----------+---+| Tacotron | r |+----------+---+|   75k    | 2 |+----------+---+Read ['江苏修鞋奶奶婉拒捐款一人养活患病老伴和儿子']Synthesizing ['jiang1 su1 xiu1 xie2 nai3 nai3 wan3 ju4 juan1 kuan3 yi1 ren2 yang3 huo2 huan4 bing4 lao3 ban4 he2 er2 zi5']| Generating 1/1Done. 使用了百度网盘的合成器，请问是什么问题呢？ 
关于安装 requirements.txt 报错的问题排除过程
先说一下本人使用的版本 cuda11.3 torch1.10.0 python3.8问题：在执行 python install -r requirements.txt 安装剩余库时出现ERROR: Could not build wheels for ctc-segmentation, which is required to install pyproject.toml-based projects的报错。尝试1：这种问题一般是缺少相关的 wheel，从  [https://github.com/lumaku/ctc-segmentation] 下载了并通过python setup.py build安装了 ctc-segmentation，此时出现第二个错误，如下。error: Microsoft Visual C++ 14.0 or greater is required. Get it with "Microsoft C++ Build Tools": Microsoft Visual C++ 14.0（至少），于是去安装了该套件，安装完毕后再次测试剩余库的安装，成功！（ps：本来出现这种问题的时候在讨论区就看到了要装C++14.0的套件，但也看到了有人装了也没有的，于是决定先自行排查一下问题，不行再安装套件，结果问题还就是出在这~~） 
Update README.md and README-CN.md 
Update README.md and README-CN.md for M1 Mac Configurations, based on previous 
使用数据集自己训练合成器模型时，持续embedding，卡壳
**Summary**数据集自己训练合成器模型时，持续embedding，卡壳，保持0%，不动**Screenshots**<img width="1503" alt="image" src="https://user-images.githubusercontent.com/111781492/201653548-4f5919f4-b17f-4676-9f92-708966ee7175.png">可能是解码器有问题，重新安装ffmepg后记得重启一下 
训练encoder的报错，已经进行音频和梅尔频谱的预处理
**Summary[问题简述（一句话）]**1、Exception: Caught Exception in DataLoader worker process 0.2、Exception: Can't create RandomCycler from an empty collection**Env & To 数据有丢失，检查下好，我再重新跟着tutorial走，预处理后，重新检查一下0 0 
现在78.5k，loss=0.34，还要不要继续训练
  20分17秒附近我就是换了75k的，然后一直训练到91k> 刚看了一个up主视频，她说可以进行将一定数量步数的pt模型更换，然后接着训练，详见https://www.bilibili.com/video/BV1dq4y137pH/?spm_id_from=0.0.header_right.history_list.click&vd_source=163d98459f033a24fece28faa60f32b3 20分17秒附近其实这一步我觉得有点奇怪，用别的数据集训练好的模型，换数据继续训练，感觉像嫁接一样，不知道理论上这么做到底对不对。另外这个UP主提到完全用自己的数据要正确收敛，需要达到5G的数据量 额…… @limitMe 你可以准备公开的语音数据集，然后再加上自己的数据，总共>5G是不是就可以了？ 
现在78.5k，loss=0.34，还要不要继续训练

跑的好像收敛了，试听也还行，但是打开程序合成音频效果很差，是不是因为没有解压aidatatang_200zh的数据

UnboundLocalError: local variable 'ppg' referenced before assignment
when i trypython demo_toolbox.py -vc -d D:\VoiceChanger\aidatatang_200zhclick ui "Extract and Convert"thenTraceback (most recent call last):  File "C:\Users\Administrator\PycharmProjects\MockingBird\toolbox\__init__.py", line 140, in <lambda>    func = lambda: self.convert() or self.vocode()  File "C:\Users\Administrator\PycharmProjects\MockingBird\toolbox\__init__.py", line 377, in convert    min_len = min(ppg.shape[1], len(lf0_uv))UnboundLocalError: local variable 'ppg' referenced before assignment 
跑8天出现这样的错误
{| Epoch: 55/167 (496/1926) | Loss: 0.2899 | 0.92 steps/s | Step: 424k | }Input at step 424500:   可以接受外面超市送的生活物资吗{| Epoch: 55/167 (996/1926) | Loss: 0.2907 | 0.93 steps/s | Step: 425k | }Traceback (most recent call last):  File "E:\ai_ai\MockingBird-main\MockingBird-main\venv\lib\site-packages\torch\serialization.py", line 379, in save    _save(obj, opened_zipfile, pickle_module, pickle_protocol)  File "E:\ai_ai\MockingBird-main\MockingBird-main\venv\lib\site-packages\torch\serialization.py", line 499, in _save    zip_file.write_record(name, storage.data_ptr(), num_bytes)OSError: [Errno 28] No space left on deviceDuring handling of the above exception, another exception occurred:Traceback (most recent call last):  File "E:\ai_ai\MockingBird-main\MockingBird-main\synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "E:\ai_ai\MockingBird-main\MockingBird-main\synthesizer\train.py", line 234, in train    model.save(backup_fpath, optimizer)  File "E:\ai_ai\MockingBird-main\MockingBird-main\synthesizer\models\base.py", line 58, in save    torch.save({  File "E:\ai_ai\MockingBird-main\MockingBird-main\venv\lib\site-packages\torch\serialization.py", line 380, in save    return  File "E:\ai_ai\MockingBird-main\MockingBird-main\venv\lib\site-packages\torch\serialization.py", line 259, in __exit__    self.file_like.write_end_of_file()RuntimeError: [enforce fail at ..\caffe2\serialize\inline_container.cc:298] . unexpected pos 433112704 vs 433112592Process finished with exit code -1073740791 (0xC0000409)没空间了？ OSError: [Errno 28] No space left on device就是没空间了 
loss 值没有下降
数据6万条，49小时，Step: 229k的时候Loss: 0.2852 ，一直没有下降，这个根数据集的大小有多大关系？{| Epoch: 36/84 (1926/1926) | Loss: 0.2852 | 0.87 steps/s | Step: 229k | }<img width="879" alt="4444" src="https://user-images.githubusercontent.com/32743722/198862711-75e76d44-75e2-4e64-8e6c-591f7757f64f.png">不是的，模型决定了这个loss在这个值会有极限，你这个已经非常可以了， 
按照视频的方法换了作者的数据集之后，再运行报错
错误信息如下：E:\aiSound\MockingBird\MockingBird-main\synthesizer\synthesizer_dataset.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\utils\tensor_new.cpp:204.)  embeds = torch.tensor(embeds)E:\aiSound\MockingBird\MockingBird-main\synthesizer\synthesizer_dataset.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\utils\tensor_new.cpp:204.)  embeds = torch.tensor(embeds)C:\Users\86186\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")Traceback (most recent call last):  File "E:\aiSound\MockingBird\MockingBird-main\synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "E:\aiSound\MockingBird\MockingBird-main\synthesizer\train.py", line 208, in train    optimizer.step()  File "C:\Users\86186\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\optimizer.py", line 113, in wrapper    return func(*args, **kwargs)  File "C:\Users\86186\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context    return func(*args, **kwargs)  File "C:\Users\86186\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\adam.py", line 157, in step    adam(params_with_grad,  File "C:\Users\86186\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\adam.py", line 213, in adam    func(params,  File "C:\Users\86186\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\adam.py", line 262, in _single_tensor_adam    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)RuntimeError: The size of tensor a (1024) must match the size of tensor b (3) at non-singleton dimension 3 
进行音频和梅尔频谱图预处理： python pre.py <datasets_root> -d {dataset} -n {number}  
**进行音频和梅尔频谱图预处理： python pre.py <datasets_root> -d {dataset} -n {number}** 1、这一步是不是必须要执行，才能执行训练合成器？ 2、是不是必须全量执行？ 可以选择部分 进行pre 吗？3、如果期间中断，有方法继续按原有进度运行吗？ 
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
**A.Summary** 之后出现这个问题**B.Env & To Reproduce**Win 11**C.Screenshots & Code** <img width="741" alt="bug" src="https://user-images.githubusercontent.com/35703636/198003704-2489b406-83a8-41d5-8d75-b084db04aa10.png">原来的问题解决了就变成上面这个情况了。<img width="627" alt="bug0" src="https://user-images.githubusercontent.com/35703636/198003740-49c58232-0ca1-483e-b2fa-377595a87a8b.png"> 
在win10.64下多次打开并移动MockingBird-main文件夹后出现ModuleNotFoundError: No module named 'synthesizer.models.tacotron
打开过一次工具箱后按键全是灰色，于是关闭。并将MockingBird-main文件夹从d盘的desktop中移动到d盘根目录，再次打开后出现ModuleNotFoundError: No module named 'synthesizer.models.tacotron‘![MEFJ{{3)69 
进行音频和梅尔频谱图预处理出错
Using data from:    E:\BaiduNetdiskDownload\ai克隆语音\aidatatang_200zh\corpus\trainaidatatang_200zh:   0%|                                                                    | 0/1 [00:00<?, ?speakers/s]no wordSno wordSno wordS.....no wordSaidatatang_200zh: 100%|████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.93s/speakers]The dataset consists of 0 utterances, 0 mel frames, 0 audio timesteps (0.00 hours).Traceback (most recent call last):  File "E:\BaiduNetdiskDownload\ai克隆语音\MockingBird\pre.py", line 74, in <module>    preprocess_dataset(**vars(args))  File "E:\BaiduNetdiskDownload\ai克隆语音\MockingBird\synthesizer\preprocess.py", line 88, in preprocess_dataset    print("Max input length (text chars): %d" % max(len(m[5]) for m in metadata))ValueError: max() arg is an empty sequence也看了一下，发现反馈的问题都一样，但很多人的解决方案都已经做过了，都还是出错请问有人怎样解决这个方案？ 
【交流与讨论】AI语音克隆微信交流群
好像这里没啥人讨论 主要内容是卖课？> 但是 好像这里没啥人讨论 主要内容是卖课？偶尔会推荐一些语音相关的公开课，直播课，会有TTS方向也有ASR方向的，也会有声纹识别，唤醒等其他方向的，都是帮助大家学习的免费资料。主要还是以大家技术交流为主，绝对不会出现收费的课程的！遇到各种问题出来后群里也有同学做技术解答，氛围还是非常棒的~有最新的微信邀请二维码吗 
分享合成器模型
0.0.1版本用不了> aidatatang_200zh训练集请问batch_size是多少跑的  过拟合有改batch_size继续跑吗> > aidatatang_200zh训练集> > 请问batch_size是多少跑的 过拟合有改batch_size继续跑吗batchsize0-10k用的32,10k-15k用的64，发现效果不理想后改成50训练到20k> > aidatatang_200zh训练集> > 请问batch_size是多少跑的 过拟合有改batch_size继续跑吗我现在正在重新训练新模型，可能是运气好，这次的模型3.5k就出现注意力模型了，8k loss就0.4了，可以期待一下我用三个数据集做的预训练 然后跑训练模型 到loss0.39 就出现过拟合了，怎么调整都没用。。。难道是数据集太大loss下不去了？0.39持续了50k左右 一直下不去。。。> 我用三个数据集做的预训练 然后跑训练模型 到loss0.39 就出现过拟合了，怎么调整都没用。。。难道是数据集太大loss下不去了？0.39持续了50k左右 一直下不去。。。你的batch szie是多少> > 我用三个数据集做的预训练 然后跑训练模型 到loss0.39 就出现过拟合了，怎么调整都没用。。。难道是数据集太大loss下不去了？0.39持续了50k左右 一直下不去。。。> > 你的batch szie是多少我显卡比较差 3060ti的  用的batch_size 24  再高就报错了> 我用三个数据集做的预训练 然后跑训练模型 到loss0.39 就出现过拟合了，怎么调整都没用。。。难道是数据集太大loss下不去了？0.39持续了50k左右 一直下不去。。。其实loss也不是越低越好，太低可能是过拟合。我试了一下我的模型和作者的模型效果差不多> > 我用三个数据集做的预训练 然后跑训练模型 到loss0.39 就出现过拟合了，怎么调整都没用。。。难道是数据集太大loss下不去了？0.39持续了50k左右 一直下不去。。。> > 其实loss也不是越低越好，太低可能是过拟合。我试了一下我的模型和作者的模型效果差不多确实是的  我自己测试了下  效果还是可以的  我打算重新训练一个 前面60kstep之前都用默认的batch_size（12），后面再慢慢调高试试> > > 我用三个数据集做的预训练 然后跑训练模型 到loss0.39 就出现过拟合了，怎么调整都没用。。。难道是数据集太大loss下不去了？0.39持续了50k左右 一直下不去。。。> > > > > > 你的batch szie是多少> > 我显卡比较差 3060ti的 用的batch_size 24 再高就报错了一直下不去不一定是过拟合，可能真的陷入局部最优解了。我的模型是因为效果差所以才怀疑过拟合的，你应该实测看看效果如何> > > 我用三个数据集做的预训练 然后跑训练模型 到loss0.39 就出现过拟合了，怎么调整都没用。。。难道是数据集太大loss下不去了？0.39持续了50k左右 一直下不去。。。> > > > > > 其实loss也不是越低越好，太低可能是过拟合。我试了一下我的模型和作者的模型效果差不多> > 确实是的 我自己测试了下 效果还是可以的 我打算重新训练一个 前面60kstep之前都用默认的batch_size（12），后面再慢慢调高试试如果觉得是batchsize的锅不如和我一起白嫖colab吧> > > > 我用三个数据集做的预训练 然后跑训练模型 到loss0.39 就出现过拟合了，怎么调整都没用。。。难道是数据集太大loss下不去了？0.39持续了50k左右 一直下不去。。。> > > > > > > > > 你的batch szie是多少> > > > > > 我显卡比较差 3060ti的 用的batch_size 24 再高就报错了> > 一直下不去不一定是过拟合，可能真的陷入局部最优解了。我的模型是因为效果差所以才怀疑过拟合的，你应该实测看看效果如何嗯嗯 可能是的  所以我打算相同数据集 不同方式都训练试试，最终比较下效果，找到最优的训练方案 😄> 反正我的方法就是无脑调大batchsize,效果挺明显的。其他参数懒得调了。32是9k出现注意力，12k > > > 我用三个数据集做的预训练 然后跑训练模型 到loss0.39 就出现过拟合了，怎么调整都没用。。。难道是数据集太大loss下不去了？0.39持续了50k左右 一直下不去。。。> > > > > > > > > 其实loss也不是越低越好，太低可能是过拟合。我试了一下我的模型和作者的模型效果差不多> > > > > > 确实是的 我自己测试了下 效果还是可以的 我打算重新训练一个 前面60kstep之前都用默认的batch_size（12），后面再慢慢调高试试> > 如果觉得是batchsize的锅不如和我一起白嫖colab吧colab 没玩过 这个项目不是还要[ffmpeg]之类的吗  好像挺麻烦的吧  你这边有命令文件分享下吗> 不用ffmpeg,ffmpeg是处理视频的。命令在上个issue也就是764号issue里面有，训练集都预处理好了> > colab 没玩过 这个项目不是还要[ffmpeg]之类的吗  好像挺麻烦的吧  你这边有命令文件分享下吗温馨提示:需要科学上网> colab 没玩过 这个项目不是还要[ffmpeg]之类的吗  好像挺麻烦的吧  你这边有命令文件分享下吗但是colab有一个好处，免费白嫖tesla t4(相当于2080ti频率降低换16g显存),70/月就能用tesla v100(32g显存,batch size估计100都不是问题)非常感谢，期待新的模型试听了一下模型效果，还是很好的！而且听出作者是用男声训练的？期待更多模型> 试听了一下模型效果，还是很好的！而且听出作者是用男声训练的？期待更多模型现在才看到回复😅我用的是aidatatang200zh数据集，数据集里面男声多还是女声多这个没注意 
如何解决colab炼丹每次都要上传数据集预处理数据集还爆磁盘的蛋疼问题
许多同鞋因为家里设备不佳训练模型效果不好，不得不去世界最大乞丐炼丹聚集地colab上训练。但是对于无法扩容google drive和升级colab的同鞋来说，上传数据集真的如同地狱一般，网速又慢空间又不够，而且每次重置都要上传，预处理令人头疼。我耗时9天终于解决了这个问题，现在给各位同学分享我的解决方案。首先要去kaggle这个网站上面注册一个账号，然后获取token我已经把预处理了的数据集(用的aidatatang_200zh)上传在上面了，但是下载数据集需要token,token需要注册账号，具体获取token的方法请自行百度，在此不过多赘述。然后打开colab修改-> 笔记本设置->运行时把 None 改成 GPU输入以下代码: 第三行请根据之前获取到的token填写这一步是准备好kaggle命令行然后是下载数据集并解压 为了怕某些童鞋用和我一样的免费版，如果从下载未处理的数据集开始磁盘要爆炸，所以我把预处理过的数据集上传到kaggle了而且解压后会自己删掉zip，非常滴银杏实测下载速度能达到200MB/s，网慢点也有50MB/s，非常滴快这一步要不了10分钟就可以弄好了 然后改hparams 我用的batch size是32，同鞋们可以根据情况自行更改开始训练 注意，开始这个步骤前请先挂载谷歌云盘，不想挂载的就把-m后面的路径改了我选择drive是因为下次训练又能继续上传训练的进度继续训练然后就是欢快的白嫖时间了氪金的同鞋可以运行!nvidia-smi查看显卡信息，白嫖版的都是tesla t4 16g显存实测9k步的时候开始出现注意力曲线，loss值为0.45注意!白嫖版的用户长时间不碰电脑colab会自动断开再次打开环境会还原成最初的样子这个时候选择drive保存的优势就体现出来了:不用担心模型重置被删掉第一次写，写得不好请见谅希望这篇教程可以帮助到你们很有帮助 赞！colab 免费版 浏览器时常断  有点烦。。。> colab 免费版 浏览器时常断 有点烦。。。训练效果是不是比用3060好?另:你用的batch size是多少?> > colab 免费版 浏览器时常断 有点烦。。。> > 训练效果是不是比用3060好?另:你用的batch size是多少?我现在用的40在跑  显存大就是好啊 哈哈> 我现在用的40在跑 显存大就是好啊 哈哈极限一点可以用60的,我用的都是50加精一下> 加精一下wow,我只是共享一下我的解决方法，没想到还能被加精。希望我的方法能帮助更多人 
GPU不参与训练 请问配置完对应GPU所需的cuda和需要的镜像源 在进行模型训练 CPU跑到100% GPU没进行运算 请问这个问题如何解决
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型**Screenshots[截图（如有）]**If applicable, add screenshots to help检查一下你的pytorch版本，pytorch如果是cpu版本就改成cuda版本，其他issue里也有类似的问题，可以看一下和你遇到的问题符不符合。我也遇到了这个问题，网上搜是最新版的bug，我选择的办法是降级，用pytorch1.11配cuda11.3，希望能帮到你 
无论是否自己训练，都有电流和沙沙声，自己训练更严重
无论是否自己训练，都有电流和沙沙声，自己训练更严重求救，求群交流！请具体的表述问题> 请具体的表述问题就是生成后的音频都是大量电流声，训练的时候生成的wav都带有一点电音，提交自己的语音合成后更加惨不忍睹> > 请具体的表述问题> > 就是生成后的音频都是大量电流声，训练的时候生成的wav都带有一点电音，提交自己的语音合成后更加惨不忍睹我建议你先搞清楚到底是encoder,synthesizer,vocoder还是其他地方的问题再来提问，这样的问题可能的原因太多了 
预处理数据只能检测到少数几条，不完全
我大约做了校对了百来条，但是只会被检测到5条，整体重命名后被检测出的还是那固定5条你用的是自己的数据集吗，如果是自己的数据集mockingbird目前还不支持，如果是支持的数据集那么检查文件结构。确保所有压缩文件都解压了自己手动分的，看样子必须要用那个分割软件> 自己手动分的，看样子必须要用那个分割软件你用的什么数据集 
有没有在macbook上成功的同学，我配置完成后没有报错信息，但是一直是沙沙的声音，不清楚是什么原因
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型mac OS Monterey 12.5Python 3.9.2 **Screensho<img width="1300" alt="截屏2022-10-07 00 26 04" src="https://user-images.githubusercontent.com/16750537/194367418-49c0c63b-b7d5-4d76-9738-4340db7c0325.png">2524e6d77.png"><img width="1669" alt="截屏2022-10-07 00 27 14" src="https://user-images.githubusercontent.com/16750537/194367712-1d2b7bd2-5704-4aa8-9d25-dcb1b41d9866.png">ts[截图（如有）]**If applicable, add screenshots to help模型没放对，synthesizer怎么会选一个wavernn呢 
请问有什么可以迁移训练的方法吗？
**Summary[问题简述（一句话）]**本地机器空间较小，无法下载大数据集进行训练。但想根据一些较少的数据训练模型，因为数据不够，效果不佳(会吞字)。所以能否进行迁移训练，从readme中获取的模型文件中提取权重然后使用自己的数据训练，以提升模型的能力。可以继续别人的模型训练的，但是效果如何不好说训练时python synthesizer_train.py 后会跟一个模型名字，把模型名字改成和别人的模型一样，就可以做到你的"迁移训练“了 
每次跑训练时候 会报Could not load symbol cublasGetSmCountTarget from cublas64_11.dll. Error code 127
每次跑训练时候 会报Could not load symbol cublasGetSmCountTarget from cublas64_11.dll. Error code 127但是还是可以继续跑，有大佬知道这个错有什么影响吗？我把cuda升级到了我显卡适配的目前最新版本的11.7  但是还是一样会报这个不知道你有没有训练，你可以百度一下，需要调整一下数据所在盘的缓存设置设置虚拟内存 
gpu换大的后碰到这个问题，是什么原因呢？
<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.Traceback (most recent call last):  File "gen_audio_from_srt.py", line 430, in <module>    Path("vocoder/saved_models/pretrained/g_hifigan.pt"), fpath, gen_materials  File "gen_audio_from_srt.py", line 144, in generate_wav    gen_one_wav(synthesizer, embed, processed_texts, file_name, hint_txt)  File "gen_audio_from_srt.py", line 74, in gen_one_wav    generated_wav = encoder.preprocess_wav(generated_wav)  File "/home/evers/MyGithub/MockingBird/encoder/audio.py", line 46, in preprocess_wav    wav = normalize_volume(wav, audio_norm_target_dBFS, increase_only=True)  File "/home/evers/MyGithub/MockingBird/encoder/audio.py", line 115, in normalize_volume    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() 
替换了别人的模型继续训练后发现报错，这是哪里出现问题了吗？有大佬可以帮忙解读一下吗？谢谢了
Loading weights at synthesizer\saved_models\chara\chara.ptTraceback (most recent call last):  File "I:\voice\audio\MockingBird-main\synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "I:\voice\audio\MockingBird-main\synthesizer\train.py", line 121, in train    model.load(weights_fpath, device, optimizer)  File "I:\voice\audio\MockingBird-main\synthesizer\models\base.py", line 51, in load    self.load_state_dict(checkpoint["model_state"], strict=False)  File "C:\Users\胡一潇\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\module.py", line 1604, in load_state_dict    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([149, 512]) from checkpoint, the shape in current model is torch.Size([169, 512]).代码版本没对上> 代码版本没对上那有啥解决办法吗大佬 
相比于SV2TTS，具体新增了哪部分内容可以处理中文数据集？
我想请问一下，MockingBird相比于SV2TTS，具体新增了哪些部分内容，可以进行中文数据集的预处理，导入以及最终的合成讷？非常需要您的帮助，谢谢！！！主要是预处理代码 
执行命令python pre.py D:\video-srt-gui-ffm -d aidatatang_200zh 预处理报错 用的是GPU
 D:\video-srt-gui-ffm\aidatatang_200zh\corpus\trainaidatatang_200zh: 100%|██████████████████████████████████████████████████████████| 66/66 [00:01<00:00, 35.62speakers/s]The dataset consists of 0 utterances, 0 mel frames, 0 audio timesteps (0.00 hours).Traceback (most recent call last):  File "D:\video-srt-gui-ffm\MockingBird-main\pre.py", line 74, in <module>    preprocess_dataset(**vars(args))  File "D:\video-srt-gui-ffm\MockingBird-main\synthesizer\preprocess.py", line 88, in preprocess_dataset    **print("Max input length (text chars): %d" % max(len(m[5]) for m in metadata))**ValueError: max() arg is an empty sequence****解决了 需要把拖拽的音频 先放到文件夹下 在移动到train文件夹下 
哪位大哥有新的PPG模型 能分享一下吗
求一个新的PPG模型  
我在使用Sound_File_Processing-master软件音频切割里面的pyhton long_file_cut_by_srt.py切割音频的时候报错
I:\voice\制作数据集\Sound_File_Processing-master>python long_file_cut_by_srt.pyTraceback (most recent call last):  File "I:\voice\制作数据集\Sound_File_Processing-master\long_file_cut_by_srt.py", line 6, in <module>    from utils.file_io import file_r, listdir, file_wModuleNotFoundError: No module named 'utils.file_io'原来用的时候从来没有出现过，而且我在untils文件里也找到了file_io.py，为什么显示没有啊？恳求大佬指点一番。这个明显不是这个项目的。 
Update README-CN.md
修正中文文档复选框格式 
用自己的数据集重跑了一个PPG模型 但是vc模式转换出来的效果为什么还是和readme中提供的PPG模型一样？
放到 ppg2mel/saved_model 目录下， demo_toolbox中 convertor 选择该模型 转换出来的结果 还是 和 readme中提供的PPG模型效果一样 没有变化是需要 extractor 下的模型也需要重跑吗？ 
使用Sound_File_Processing-master软件音频切割 在执行pyhton long_file_cut_by_srt.py命令出现以下问题 希望大佬帮助看一下
D:\video-srt-gui-ffm\Sound_File_Processing-master>pip install -r requirements.txtRequirement already satisfied: pydub==0.25.1 in d:\python\lib\site-packages (from -r requirements.txt (line 1)) (0.25.1)D:\video-srt-gui-ffm\Sound_File_Processing-master>python long_file_cut_by_srt.pysrt/loy2.srtwav/input/loy2.wavwav/input/loy2.mp3Traceback (most recent call last):  File "D:\video-srt-gui-ffm\Sound_File_Processing-master\long_file_cut_by_srt.py", line 77, in <module>    cut_file_by_srt(sound_root_path, srt_dict)  File "D:\video-srt-gui-ffm\Sound_File_Processing-master\long_file_cut_by_srt.py", line 48, in cut_file_by_srt    sound_cut = sound[start_time:end_time + 50]  # 网易的裁切太激进了，所以加50毫秒**UnboundLocalError: local variable 'sound' referenced before assignment**解决了 音频没有转换成 wav 直接修改的后缀名 导致音频有问题 
Update streamlit_ui.py
python on linux can not find file by '.\\mkgui\\static\\mb.png', change it to path.join('mkgui', 'static', 'mb.png') 
Update requirements.txt
I have installed dependence on Ubuntu 20.04.1 CUDA 11.3. Run web.py, missing "fastapi loguru click==8.0.4 typer". numpy and pandas are in conficting. 
数据预处理时检测不出数据集

数据集要解压到gz还是wav格式，我解压到gz说识别不了，大家有这个问题吗？
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型**Screenshots[截图（如有）]**If applicable, add screenshots to help1已解决，到wav格式可以 
修复demo_tools加载aidatatang_200zh数据集的问题
在使用demo_tools -d 加载数据集时发现一个问题：- aidatatang_200zh标准数据集是不包含wav文件的（被打包在压缩包中），而demo_tools读取源音频是遍历目录下所有wav- 在制作个人数据集时我们会把wav放在aidatatang_200zh/corpus/train/中- 现在读取的目录只有“aidatatang_200zh\corpus\dev”和“aidatatang_200zh\corpus\test”所以要不就干脆识别aidatatang_200zh\corpus吧 
作者能否私发一下微信二维码群 或者QQ群啥的
感激不尽同求同求 
添加 -d 指定数据集时错误提示
Warning: you do not have any of the recognized datasets in G:\AI\Dataset\aidatatang_200zh\aidatatang_200zh  Please note use 'E:\datasets' as root path instead of 'E:\datasetsidatatang_200zh\corpus/test' as a example . The recognized datasets are:感谢贡献！已合并 
明明核对了1000多条 训练时却只扫描到了800来条
是因为敏感词还是什么原因呢  弄了两天还是没找到原因> 是因为敏感词还是什么原因呢 弄了两天还是没找到原因断点打印下，不会敏感词的，不过有长度限制。 
loaded state dict contains a parameter group that doesn't match the size of optimizer's group
**Summary[问题简述（一句话）]** using version v0.0.1,and pretrained model 提取码：2021,but occur issue:ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's groupwechat chatting group code expired, new one please@babysorThanks for atting me here. I prefer to discuss here since we can focus on the issue itself. It looks that you are not using the right pretrained file for your model. pls attach more infoappreciate for your answer.encouter another issue when trying Synthesizing wav locally,by executing demo_cli.py,but having problem at final step:Caught exception: ValueError('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()')it seems the problem of ' generated_wav = encoder.preprocess_wav(generated_wav) '  this sentence, 206 line of demo_cli.py@babysor 
替换模型后训练时报出错误，想请教一下是什么问题？
替换模型后训练时报出错误，想请教一下是什么问题？![屏幕截图 2022-09-09 
无限循环不降也不升添加素材也无用
![4SOTF7CZ3YJH0N   以前800条就能很好  感觉越多反而越不行  请问有人知道问题出哪了吗  一直4000左右循环徘徊 
使用模型时候报错，web是直接报错，box是训练出来全部是杂音
RuntimeError: Error(s) in loading state_dict for Tacotron: size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]). size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]). size mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([384, 1280]). size mismatch for decoder.rnn_input.weight: copying a param with shape torch.Size([1024, 640]) from checkpoint, the shape in current model is torch.Size([1024, 1152]). size mismatch for decoder.stop_proj.weight: copying a param with shape torch.Size([1, 1536]) from checkpoint, the shape in current model is torch.Size([1, 2048]). 
Could not load symbol cublasGetSmCountTarget from cublas64_11.dll. Error code 127
训练时提示Could not load symbol cublasGetSmCountTarget from cublas64_11.dll. Error code 127但仍可以继续，这是什么情况？ 
作者大大可不可以发个微信群二维码
@babysor 万分感谢！可以给我发个吗 
1 validation error for ParsingModel[Input] root -> 语音解码模型 none is not an allowed value (type=type_error.none.not_allowed)
**Summary[web上面AI拟音提示这个报错1 validation error for ParsingModel[Input] root -> 语音解码模型 none is not an allowed value (type=type_error.none.not_allowed)）]**A clear and concise description of what the issue is.**Env & To Reproduce[复现与环境]**python3.9  Windows Server 2019**Screenshots[截图（如有）]**If applicable, add screenshots to help 
ImportError: cannot import name 'logger' from 'joblib' (unknown location)
环境是python=3.9  看网上的解决方法重新安装低版本的sklearn  但是低版本的sklearn需要更低版本的python支持，请问需要怎么办呢？急救 
训练时出现问题：Creating a tensor from a list of numpy.ndarrays is extremely slow. 
**Summary[问题简述（一句话）]**Creating a tensor from a list of numpy.ndarrays is extremely slow. 训练时出现这个，CPU占用率也不高，如何解决**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型**Screenshots[截图（如有）]**If applicable, add screenshots to helpE:\mockingbird\MockingBird-main\MockingBird-main\synthesizer\synthesizer_dataset.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\utils\tensor_new.cpp:210.)  embeds = torch.tensor(embeds)E:\mockingbird\MockingBird-main\MockingBird-main\synthesizer\synthesizer_dataset.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\utils\tensor_new.cpp:210.)  embeds = torch.tensor(embeds)E:\mockingbird\Python\lib\site-packages\torch\nn\functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")手动修改下吧 如果不希望看到 warning 不过据说效率 反而差点。   修改成 
预处理数据集时报错，求帮助
的文本编码是否有问题 
能否支持多个声音一起训练？
就是一段音频里面有好几个不同声音 但是清晰分明  不存在重合混乱现象      
建议在人声转换的基础上增加歌声转换功能
**Summary[问题简述（一句话）]**能否在目前人声到人声转换的基础上，增加歌声转换（Singing Voice Conversion）？即给予几秒说话人语音，能把声线转换至一首纯人声的歌曲中。过分了哈 
GPU运算如何优化环境达到最高速率？
<img width="367" alt="image" src="https://user-images.githubusercontent.com/108389438/187482454-92dbfedc-4809-436e-b8f3-7d01f487de8a.png"><img width="331" alt="image" src="https://user-images.githubusercontent.com/108389438/187482549-78b0d645-b728-4517-b369-f2100f8e50e6.png">同样的环境 3090  size:66   0.59 steps/s   3060 size:16  0.65 steps/s  实在是搞不懂   而且CUDA11.6时 运算速率会随着模型增大而逐步变慢，3060一开始时1.1 steps/s  运行二十几个小时后逐步掉到0.65 steps/s   requirements.txt里有好几个支持的版本是在30系显卡有冲突或者不兼容的（也有可能是和win11不匹配），作者能否重新配置一下版本信息？https://zhuanlan.zhihu.com/p/549801042这个技术如果能结合Mockingbird可就绝了 
如何才能覆盖累积训练
开始我以为训练出来某个人的模型后   在拿那个模型去训练另外一个人  然后这东西是可以累积的  也就是说   比如我训练了A人的模型，然后在拿A模型去接着训练B，那么训练出来的这个模型就会拥有A和B的模型，可是最后我发现不是那样，只要换人练，模型就会覆盖先前的模型，说白了就是A和B白练了，因为已经被覆盖了不能使用，有没有办法不覆盖而继续累积么，按理说不是训练的声音越多，机器的学习能力越强么，可如果覆盖了那不是等于白学习了 
核对4000条训练出来不如原先的核对几百条
如题  有前辈给个指点吗   不知道为什么数据多跟数据少一样   不知道问题出在了哪里  Batch Size各种参数改都没用   
是否支持维吾尔语
是否支持维吾尔语的克隆理论上应该可以的，需要 维吾尔语 语料， 文字转音素的方法 
Trainable Parameters: 0.000M
**Summary[问题简述（一句话）]**您好，请问我在训练synthesizer 的时候，显示Trainable Parameters: 0.000M是正常的吗？看起来是没有参数在训练**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型**Screenshots[截图（如有）]**If applicable, add screenshots to help不正常，有更多截图或信息吗> 不正常，有更多截图或信息吗遇到同样的现象  遇到了同样的问题：D:\MockingBird-main>python synthesizer_train.py kilruk D:\BaiduNetdiskDownload\SV2TTS\synthesizerArguments:    run_id:          kilruk    syn_dir:         D:\BaiduNetdiskDownload\SV2TTS\synthesizer    models_dir:      synthesizer/saved_models/    save_every:      1000    backup_every:    25000    log_every:       200    force_restart:   False    hparams:Checkpoint path: synthesizer\saved_models\kilruk\kilruk.ptLoading training data from: D:\BaiduNetdiskDownload\SV2TTS\synthesizer\train.txtUsing model: TacotronUsing device: cudaInitialising Tacotron Model...\Saving the json with %s synthesizer\saved_models\kilruk\kilruk.jsonTrainable Parameters: 0.000MLoading weights at synthesizer\saved_models\kilruk\kilruk.ptTacotron weights loaded from step 0Using inputs from:        D:\BaiduNetdiskDownload\SV2TTS\synthesizer\train.txt        D:\BaiduNetdiskDownload\SV2TTS\synthesizer\mels        D:\BaiduNetdiskDownload\SV2TTS\synthesizer\embedsFound 63 samples我也有一样的问题，但synthesizer训练出来又好像是有效的……> 和后面那些人的评论一样的 
预处理时出现报错
切换到E盘后操作，cmd不允许跨盘操作我是在文件夹内打开cmd的啊> 
分享王冰冰数据集
链接：https://pan.baidu.com/s/1ScihCTR90Bnf_xdanqPyLg 提取码：77nv希望大佬们用这个数据集训练的模型也能分享出来大佬 文件已过期 能重新分享一下么> 链接：https://pan.baidu.com/s/1ScihCTR90Bnf_xdanqPyLg 提取码：77nv 希望大佬们用这个数据集训练的模型也能分享出来你能在分享一下嘛？我能做出来数据集链接：https://pan.baidu.com/s/190jnZS6egANYUppDISecaA 提取码：bjkc训练好的模型链接：链接：https://pan.baidu.com/s/1cn51AyWNivGXaQa-vJ8-4A 提取码：tj2r> 数据集链接：https://pan.baidu.com/s/190jnZS6egANYUppDISecaA 提取码：bjkc 训练好的模型链接：链接：https://pan.baidu.com/s/1cn51AyWNivGXaQa-vJ8-4A 提取码：tj2r感谢 大佬请问这个数据集是按照哪个数据集的格式来做的？请问训练的是哪个，文件路径格式。 
懒人版用不了
全都是不可选中的没下载数据集就这样，正常这个懒人版哪里下载> 没下载数据集就这样，正常数据集要下载到哪里面 
!python demo_toolbox.py -d.\samples缺少模块

python encoder_preprocess.py 运行出错
MockingBird-main\encoder\preprocess.py", line 109, in preprocess_speaker    sources_file.write("%s,%s\n" % (out_fname, in_fpath))UnicodeEncodeError: 'gbk' codec can't encode character '\ufeff' in position 5: illegal multibyte sequence以上是报错内容 请问一下有人知道怎么解决不文件名有问题 
更正错误的CPU型号
11770k😂😂😂 (离谱感谢！！！不谢 ＼(￣▽￣)／ 
capturable=False 怎么办？总结的一些办法。
①先卸载pytorch                 cmd运行  pip uninstall install torchvision==0.10.0      和    pip install torchaudio==0.9.0④去https://download.pytorch.org/whl/torch_stable.html网站，找到并下载cu111/torchvision-0.9.0%2Bcu111-cp39-cp39-win_amd64.whl此文件。⑤在下载目录运行cmd，安装指令：          pip install torch-1.9.0+cu111-cp39-cp39-win_amd64.whl感谢！热心网友yyds 
一些建议
1. 可不可以完整编译为exe，太复杂了不会搞2. 可不可以把所有文件（包括声音素材）的百度网盘链接和Google Drive链接转存为其他好一些的网盘>  Google Drive不是所有人都可以使用的（中国大陆> 百度网盘可以用自建网盘或anonfiles（无限空间，不限速，单文件大小最大20GB 
提示缺少模块
前面的pip安装都是正常的，没有报错直到最后的python demo_toolbox.py -d 
copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024])
开始的报错是weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512])之后按issue 37改synthesizer/utils/symbols.py 第11行的内容 改为：_characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz12340!\'(),-.:;? '之后报错变成copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]) 
 TypeError: CCompiler_spawn() got an unexpected keyword argument 'env'
请问最后一步报错这个 
怎么用GPU训练呢？@zhuzaileiting 你这是用cpu训练的，GPU速度大概在1.3-2 steps/s
@zhuzaileiting 你这是用cpu训练的，GPU速度大概在1.3-2 steps/s_Originally posted by @miven in 
tag 0.0.1 和最新版, 选择不同合成器模型,总会在不同的参数上出现 size mismatch, 导致语音合成失败
错误信息:size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).请问1)这个是什么原因导致的呢? 是编码器和合成器不匹配?2),这个有办法从代码层面调整么?3)这个问题怎样解决呢?这个问题有人问过，而且介绍文件里面有写， 
TypeError: synthesize_spectrograms() got an unexpected keyword argument 'style_idx'
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.toolbox跑不了**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型python3.8.10 最新mockingbird applicable, add screenshots to 出现了一个新的错误Loaded encoder "pretrained.pt" trained to step 1564501Synthesizer using device: cudaTrainable Parameters: 0.000MTraceback (most recent call last):  File "D:\Project\MockingBird\toolbox\__init__.py", line 144, in <lambda>    func = lambda: self.synthesize() or self.vocode()  File "D:\Project\MockingBird\toolbox\__init__.py", line 260, in synthesize    specs = self.synthesizer.synthesize_spectrograms(texts, embeds, style_idx=int(self.ui.style_slider.value()), min_stop_token=min_token, steps=int(self.ui.length_slider.value())*200)  File "D:\Project\MockingBird\synthesizer\inference.py", line 93, in synthesize_spectrograms    self.load()  File "D:\Project\MockingBird\synthesizer\inference.py", line 71, in load    self._model.load(self.model_fpath, self.device)  File "D:\Project\MockingBird\synthesizer\models\base.py", line 51, in load    self.load_state_dict(checkpoint["model_state"], strict=False)  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python38\lib\site-packages\torch\nn\modules\module.py", line 1604, in load_state_dict    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).        size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]).        size mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([384, 1280]).        size mismatch for decoder.rnn_input.weight: copying a param with shape torch.Size([1024, 640]) from checkpoint, the shape in current model is torch.Size([1024, 1152]).        size mismatch for decoder.stop_proj.weight: copying a param with shape torch.Size([1, 1536]) from checkpoint, the shape in current model is torch.Size([1, 2048]). 
请问训练时如果包含很多人的语音混合训练会怎么样？
如题。我有大概4万条的一个非常优质的素材库，发音标准且没有BGM和杂音，只不过里面的人都是不同的。如果我进行训练是会跑出来一个通用模型吗？可以这么操作吗？没问题的，aidatatang_200zh数据集就是很多不同的人> 如题。我有大概4万条的一个非常优质的素材库，发音标准且没有BGM和杂音，只不过里面的人都是不同的。 如果我进行训练是会跑出来一个通用模型吗？ 可以这么操作吗？这里需要把素材按不同人分到不同的文件夹，如果有标注说话人的话应该是没问题的，简单处理下就好。 
在使用Anaconda的情况下，不能打开程序，提示缺少大量module。
install -r requirements.txt那一步不能报错，文件里面的模块需要正确安装再试试这个指令解决了，是pywin32版本的问题，304无法打开，降级300后可以正常打开程序，用pip install pywin32==300解决 
导入他人模型出错
错误:Traceback (most recent call last):  File "C:\Users\Administrator\Desktop\MockingBird-0.0.1\synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "C:\Users\Administrator\Desktop\MockingBird-0.0.1\synthesizer\train.py", line 114, in train    model.load(weights_fpath, optimizer)  File "C:\Users\Administrator\Desktop\MockingBird-0.0.1\synthesizer\models\tacotron.py", line 526, in load    optimizer.load_state_dict(checkpoint["optimizer_state"])  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\optimizer.py", line 171, in load_state_dict    raise ValueError("loaded state dict contains a parameter group "ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group导入他人模型都不行,只有自己的可以用代码版本和模型没对上> 代码版本和模型没对上所以具体应该如何解决这个问题呢解决了吗 老哥 我也是这个问题，搜了一圈都是说模型版本的问题 我把作者推荐的几个模型都试了 还是不行> 解决了吗 老哥 我也是这个问题，搜了一圈都是说模型版本的问题 我把作者嘴贱的几个模型都试了 还是不行大概了解啥意思了，就是mockingbird有很多的版本，假如你一个模型一直不能用的话必须要下载其他版本的mockingbird才能用 
macbook m1: web.py启动 报错循环调用，正确的启动姿势是什么？
**Summary[问题简述（一句话）]**python web.py 启动 报错循环调用，正确的启动姿势是什么？**Env & To Reproduce[复现与环境]**macbook m1，main主分支，python3.8**Screenshots[截图（如有）]**<img width="662" alt="image" src="https://user-images.githubusercontent.com/5927124/183737266-84c9e105-e771-4046-84d3-e5815b92be3f.png">我修复了一下，可以试一下最新代码 
windows下saved_models存在同名模型会导致电脑死机
遇到一个问题：如果synthesizer/saved_models文件夹下有同名的模型，运行synthesizer_train.py，电脑会直接死机例如saved_models文件夹下存在/dsq/dsq.pt，这时运行python synthesizer_train.py dsq C:\path\to\SV2TTS\synthesizer，电脑会直接无响应，无法通过Ctrl+C或其他方式来中断程序我不知道是我电脑的问题还是其他人也有，我的电脑环境是Windows 10 + cuda11.6 + PyTorch那一行添加Pytorch的版本 
合成器训练停止后，如何用于其他的训练
之前都是使用作者或者别人打包好的训练集继续训练，但我用自己训练的数据集训练后，继续训练会出现抱错。请问是什么原因。报错如下：Traceback (most recent call last):  File "D:\mockingbird2\MockingBird-main\synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "D:\mockingbird2\MockingBird-main\synthesizer\train.py", line 215, in train    optimizer.step()  File "C:\Program Files\Python39\lib\site-packages\torch\optim\optimizer.py", line 109, in wrapper    return func(*args, **kwargs)  File "C:\Program Files\Python39\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context    return func(*args, **kwargs)  File "C:\Program Files\Python39\lib\site-packages\torch\optim\adam.py", line 157, in step    adam(params_with_grad,  File "C:\Program Files\Python39\lib\site-packages\torch\optim\adam.py", line 213, in adam    func(params,  File "C:\Program Files\Python39\lib\site-packages\torch\optim\adam.py", line 255, in _single_tensor_adam    assert not step_t.is_cuda, "If capturable=False, state_steps should not be CUDA tensors."AssertionError: If capturable=False, state_steps should not be CUDA tensors.pytorch版本不匹配导致的 
数据集里面的文本有空格

Translate update README-CN.md
Fix: Traditional Chinese to Simplified Chinese 
encoder 训练 loss不降问题
请问训练enconder 为什么loss一直都不降？跑了10k可以，loss一直保持在4.1多。然而，synthesizer的训练就很有效果。4000步就看到明显loss的下降。有高手给给意见吗。感谢enconder 的模型本来就很不一样，要跑的步数也多很多 
模型上传到huggingface
模型上传到huggingface，放在度盘下载不太方便，还有失效的风险 
我怀疑是numpy有问题，但不知如何解决。也可能不是这个问题
常见于本地python依赖冲突 
运行python demo_toolbox.py -d .\samples报错
运行python demo_toolbox.py -d install -r requirements.txt安装其他库的时候，pyworld无法安装，报错为【pip】解决ERROR: Could not build wheels for pyworld which use PEP 517 and cannot be installed directly你好。已收到你的邮件，我会在尽快处理。————————————这是来自QQ邮箱的自动回复邮件。谢谢您的来信，期待您的下次来信~我会阅读您的来信，请放心~谢谢您的来信，期待您的下次来信~我会阅读您的来信，请放心~你好。已收到你的邮件，我会在尽快处理。————————————这是来自QQ邮箱的自动回复邮件。 
GUI界面声码器没有wavernn 

long_file_cut_by_srt.py", line 46, in cut_file_by_srt     if end_time - start_time <= min_length or len(line[2].replace(" ", "")) < min_text: IndexError: list index out of range
long_file_cut_by_srt.py", line 46, in cut_file_by_srt    if end_time - start_time <= min_length or len(line[2].replace(" ", "")) < min_text:IndexError: list index out of range好像太长了无法截取    用新的又无法跳出那个页面  老是提示什么QT 无法查到 
关于语音转换Voice Conversion(PPG based)中的ppg2mel.yaml修改里面的地址指向预训练好的文件夹
**Summary[问题简述（一句话）]**原文件似乎是为linux环境创建的，没有说明如何在windows下修改地址，请教一下如何在windows下修改ppg2mel.yaml文件里的地址，有好多文件我并没有在预处理后文件夹中找到**Env & To Reproduce[复现与环境]**环境：windows11，anaconda：python3.9.12数据集文件夹：C:\test\test8\aidatatang_200zh![屏幕截图 2022-07-31 2022-07-31 2022-07-31 2022-07-31 2022-07-31 
超算云预处理pre.py出错

OSError: [WinError 1455] 页面文件太小，无法完成操作。 Error loading "D:\ProgramData\Anaconda3\lib\site-packages\torch\lib\cudnn_adv_infer64_8.dll" or one of its dependencies.
修改了 tts_schedule  还是不行 
 File "D:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\linalg\eigen\arpack\arpack.py", line 42, in <module>     from . import _arpack ImportError: DLL load failed while importing _arpack: 找不到指定的程序。
搞不懂啊 前几天还好好的 今天忽然这样  怎么改都没用   
预训练pre.py中拒绝访问怎么解决
<>去掉即可 
在作者模型上训练后再次训练报错

运行web.py报错：No module named 'mkgui'
   
调整完batch_size之后需要其他的操作吗？
调整了batch_size的大小但是占用率没有变化 
请教大佬，现在一分钟跑一个，怎么提升速度呢？（附截图）
显卡是1060 
在训练模型时attention图无法生成明显斜线是否正常

这个是什么情况？
大佬们，我按照作者的步骤安装好后，为什么打开的时候报错？报错代码：系统：Windows11  Torch选的CPU版本<img width="645" alt="image" src="https://user-images.githubusercontent.com/71160034/181170663-4f103f48-5a41-4320-9f02-77b9031e806d.png">我在我那台老电脑上还可以用，到新电脑上就出现各种问题，之前那些都解决了，唯独这个不知道该怎么办新电脑的依赖有问题。。python以前装过东西嘛？ 
奇奇怪怪的报错
Using model: TacotronUsing device: cudaInitialising Tacotron Model...\Loading the json with %s {'sample_rate': 16000, 'n_fft': 800, 'num_mels': 80, 'hop_size': 200, 'win_size': 800, 'fmin': 55, 'min_level_db': -100, 'ref_level_db': 20, 'max_abs_value': 4.0, 'preemphasis': 0.97, 'preemphasize': True, 'tts_embed_dims': 512, 'tts_encoder_dims': 256, 'tts_decoder_dims': 128, 'tts_postnet_dims': 512, 'tts_encoder_K': 5, 'tts_lstm_dims': 1024, 'tts_postnet_K': 5, 'tts_num_highways': 4, 'tts_dropout': 0.5, 'tts_cleaner_names': ['basic_cleaners'], 'tts_stop_threshold': -3.4, 'tts_schedule': [[2, 0.001, 10000, 12], [2, 0.0005, 15000, 12], [2, 0.0002, 20000, 12], [2, 0.0001, 30000, 12], [2, 5e-05, 40000, 12], [2, 1e-05, 60000, 12], [2, 5e-06, 160000, 12], [2, 3e-06, 320000, 12], [2, 1e-06, 640000, 12]], 'tts_clip_grad_norm': 1.0, 'tts_eval_interval': 500, 'tts_eval_num_samples': 1, 'tts_finetune_layers': [], 'max_mel_frames': 900, 'rescale': True, 'rescaling_max': 0.9, 'synthesis_batch_size': 16, 'signal_normalization': True, 'power': 1.5, 'griffin_lim_iters': 60, 'fmax': 7600, 'allow_clipping_in_normalization': True, 'clip_mels_length': True, 'use_lws': False, 'symmetric_mels': True, 'trim_silence': True, 'speaker_embedding_size': 256, 'silence_min_duration_split': 0.4, 'utterance_min_duration': 1.6, 'use_gst': True, 'use_ser_for_gst': True}Trainable Parameters: 0.000MLoading weights at synthesizer\saved_models\xb\xb.ptTacotron weights loaded from step 144Using inputs from:        C:\Users\hyhhy\Desktop\AIless\vc\SV2TTS\synthesizer\train.txt        C:\Users\hyhhy\Desktop\AIless\vc\SV2TTS\synthesizer\mels        C:\Users\hyhhy\Desktop\AIless\vc\SV2TTS\synthesizer\embedsFound 284 samples+----------------+------------+---------------+------------------+| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |+----------------+------------+---------------+------------------+|    9k Steps    |     12     |     0.001     |        2         |+----------------+------------+---------------+------------------+Could not load symbol cublasGetSmCountTarget from cublas64_11.dll. Error code 127Traceback (most recent call last):  File "C:\Users\hyhhy\Desktop\AIless\MockingBird\synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "C:\Users\hyhhy\Desktop\AIless\MockingBird\synthesizer\train.py", line 215, in train    optimizer.step()  File "C:\Users\hyhhy\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\optimizer.py", line 109, in wrapper    return func(*args, **kwargs)  File "C:\Users\hyhhy\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context    return func(*args, **kwargs)  File "C:\Users\hyhhy\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\adam.py", line 157, in step    adam(params_with_grad,  File "C:\Users\hyhhy\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\adam.py", line 213, in adam    func(params,  File "C:\Users\hyhhy\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\adam.py", line 255, in _single_tensor_adam    assert not step_t.is_cuda, "If capturable=False, state_steps should not be CUDA tensors."AssertionError: If capturable=False, state_steps should not be CUDA tensors.我是回到torch 1.11.0解决的，参考#631用新的安装包就行 
使用demo_toolbox进行语音合成时，生成的语音缺失头几个字
按照教程使用默认参数进行语音合成，生成的语音很逼真，但总是缺失头几个字。目标语音：欢迎使用工具箱, 现已支持中文输入！生成语音：使用工具箱, 
请教各位大佬！预加载后train文件下无变化
D:\MockingBird>python pre.py D:\MockingBird -d aidatatang_200zh -n 3Using data from:    D:\MockingBird\aidatatang_200zh\corpus\trainaidatatang_200zh:   0%|                                                                    | 0/1 [00:00<?, ?speakers/s]D:\MockingBird\synthesizer\preprocess_speaker.py:66: FutureWarning: Pass sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error  wav, _ = librosa.load(wav_fpath, hparams.sample_rate)D:\MockingBird\synthesizer\audio.py:170: FutureWarning: Pass sr=16000, n_fft=800 as keyword args. From version 0.10 passing these as positional arguments will result in an error  return librosa.filters.mel(hparams.sample_rate, hparams.n_fft, n_mels=hparams.num_mels,卡了一段时间，变成下面这样D:\MockingBird>python pre.py D:\MockingBird -d aidatatang_200zh -n 3Using data from:    D:\MockingBird\aidatatang_200zh\corpus\trainaidatatang_200zh:   0%|                                                                    | 0/1 [00:00<?, ?speakers/s]D:\MockingBird\synthesizer\preprocess_speaker.py:66: FutureWarning: Pass sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error  wav, _ = librosa.load(wav_fpath, hparams.sample_rate)D:\MockingBird\synthesizer\audio.py:170: FutureWarning: Pass sr=16000, n_fft=800 as keyword args. From version 0.10 passing these as positional arguments will result in an error  return librosa.filters.mel(hparams.sample_rate, hparams.n_fft, n_mels=hparams.num_mels,aidatatang_200zh: 100%|████████████████████████████████████████████████ aidatatang_200zh: 100%|████████████████████████████████████████████████ ███████████| 1/1 [04:19<00:00, 259.42s/speakers]The dataset consists of 452 utterances, 75598 mel frames, 15068640 audio timesteps (0.26 hours).Max input length (text chars): 90Max mel frames length: 296Max audio timesteps length: 59040Embedding:   0%|                                                                       | 0/452 [00:00<?, ?utterances/s]Loaded encoder "pretrained.pt" trained to step 1594501D:\MockingBird\encoder\audio.py:58: FutureWarning: Pass y=[-0.0061357  -0.00546735 -0.00615163 ...  0.          0.  0.        ], sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error  frames = librosa.feature.melspectrogram(Embedding:   0%|▏                                                            | 1/452 [00:20<2:37:45, 20.99s/utterances]D:\MockingBird\encoder\audio.py:58: FutureWarning: Pass y=[-7.9409342e-24  8.0116936e-17  3.2043635e-16 ...  0.0000000e+00  0.0000000e+00  0.0000000e+00], sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error  frames = librosa.feature.melspectrogram(Embedding:   0%|▎                                                            | 2/452 [00:21<1:05:30,  8.73s/utterances]D:\MockingBird\encoder\audio.py:58: FutureWarning: Pass y=[0.0000000e+00 8.0116869e-17 3.2043638e-16 ... 0.0000000e+00 0.0000000e+00 0.0000000e+00], sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error  frames = librosa.feature.melspectrogram(Embedding:   1%|▍                                                              | 3/452 [00:21<36:04,  4.82s/utterances]D:\MockingBird\encoder\audio.py:58: FutureWarning: Pass y=[0.00158023 0.00096583 0.00023703 ... 0.         0.         0.        ], sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error  frames = librosa.feature.melspectrogram(D:\MockingBird\encoder\audio.py:58: FutureWarning: Pass y=[ 0.00776944  0.00653151 -0.01059025 ...  0.          0.  0.        ], sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error 
使用中文readme中的第三个模型，同样的input音频文件和文字内容，得到的结果音频有杂音且每次效果不同
看截图，大概率是你的vocoder模型不够好，用一下hifigan在GUI工具里面，Vocoder选择g_hifigan有电音，反而wavernn_pretrained效果比g_hifigan要好点，这是啥原因呢 
安装过程出错
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型py3.9.12出错代码 **Screenshots[截图（如有）]**If applicable, add screenshots to help用py3.8也一样可以看一下这个https://github.com/babysor/MockingBird/issues/578#issuecomment-1292187363 
训练模型一段时间后重新打开训练出现File
开始训练是好的   但是当关闭训练 第二次重新训练的时候就出现File 报错  什么张量或不是张量的  想请教一下   关闭模型训练的正确方式是直接关闭cmd还是先ctrl+c 停止训练在关闭cmd。.......昨天训练了一夜，还调低了batch_size参数，甚至是换了模型从新练，各种排除法。今天再次停止重新训练，结果又如下：MockingBird-main\synthesizer\train.py", line 215, in train    optimizer.step()  File "D:\ProgramData\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 109, in wrapper    return func(*args, **kwargs)  File "D:\ProgramData\Anaconda3\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context    return func(*args, **kwargs)  File "D:\ProgramData\Anaconda3\lib\site-packages\torch\optim\adam.py", line 157, in step    adam(params_with_grad,  File "D:\ProgramData\Anaconda3\lib\site-packages\torch\optim\adam.py", line 213, in adam    func(params,  File "D:\ProgramData\Anaconda3\lib\site-packages\torch\optim\adam.py", line 255, in _single_tensor_adam    assert not step_t.is_cuda, "If capturable=False, state_steps should not be CUDA tensors."AssertionError: If capturable=False, state_steps should not be CUDA tensors.一言蔽之→：训练新模型OK，但是只要这个训练停止后，再次继续训练就一定出现以上问题，就是模型是一次性的，无法第二次打开训练 
如何安全（不损坏文件）的停止训练，笔记本没有pause键！（CTRL+Fn+Pause无法使用）
**Summary[问题简述（一句话）]**如何安全（不损坏文件）的停止训练，笔记本没有pause键！（CTRL+Fn+Pause无法使用）Pause Break是中断暂停键。在台式机的101键盘上会有这个键。当前的笔记本键盘上已经没有这个按键了，可以通过[Fn] + [P] = pause、[Fn] + [B] = break来实现相关功能。结果：CTRL+Fn+ P     能够安全停止训练，不会报错，能够继续训练。 
安装gpu又惨败，提示什么页面太大不得不强行关闭
显卡Rtx 2060  cuda_11.6各种尝试都不行 搞了三天了  开始用cpu跑 觉得太慢 想用显卡跑结果各种安装失败错误  关键是这显卡不上不下 11.6  也不知道哪里出错了  哪位好心人给个详细教程吧   拜托了 真的很喜欢这个  6679314@qq.com 邮箱訓練模型顯存不足訓練合成器時：將 synthesizer/hparams.py中的batch_size參數調小//調整前tts_schedule = [(2, 1e-3, 20_000, 12), # Progressive training schedule(2, 5e-4, 40_000, 12), # (r, lr, step, batch_size)(2, 2e-4, 80_000, 12), #(2, 1e-4, 160_000, 12), # r = reduction factor (# of mel frames(2, 3e-5, 320_000, 12), # synthesized for each decoder iteration)(2, 1e-5, 640_000, 12)], # lr = learning rate//調整後tts_schedule = [(2, 1e-3, 20_000, 8), # Progressive training schedule(2, 5e-4, 40_000, 8), # (r, lr, step, batch_size)(2, 2e-4, 80_000, 8), #(2, 1e-4, 160_000, 8), # r = reduction factor (# of mel frames(2, 3e-5, 320_000, 8), # synthesized for each decoder iteration)(2, 1e-5, 640_000, 8)], # lr = learning rate搞好了&nbsp; 谢谢&nbsp; 不过我是在cmd里面调的&nbsp;------------------&nbsp;原始邮件&nbsp;------------------发件人:                                                                                                                        "babysor/MockingBird"                                                                                    ***@***.***&gt;;发送时间:&nbsp;2022年7月24日(星期天) 晚上7:23***@***.***&gt;;***@***.******@***.***&gt;;主题:&nbsp;Re: [babysor/MockingBird] 安装gpu又惨败，提示什么页面太大不得不强行关闭 (Issue #673) 訓練模型顯存不足 訓練合成器時：將 synthesizer/hparams.py中的batch_size參數調小 //調整前 tts_schedule = [(2, 1e-3, 20_000, 12), # Progressive training schedule (2, 5e-4, 40_000, 12), # (r, lr, step, batch_size) (2, 2e-4, 80_000, 12), # (2, 1e-4, 160_000, 12), # r = reduction factor (# of mel frames (2, 3e-5, 320_000, 12), # synthesized for each decoder iteration) (2, 1e-5, 640_000, 12)], # lr = learning rate //調整後 tts_schedule = [(2, 1e-3, 20_000, 8), # Progressive training schedule (2, 5e-4, 40_000, 8), # (r, lr, step, batch_size) (2, 2e-4, 80_000, 8), # (2, 1e-4, 160_000, 8), # r = reduction factor (# of mel frames (2, 3e-5, 320_000, 8), # synthesized for each decoder iteration) (2, 1e-5, 640_000, 8)], # lr = learning rate —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you authored the thread.Message ID: ***@***.***&gt; 
训练成功过，中途退出了，重新开始训练，出现新的问题了  AssertionError：如果capturable=False，则state_步骤不应为CUDA张量。
**Summary[问题简述（一句话）]**训练成功过，中途退出了，重新开始训练，出现新的问题了    assert not step_t.is_cuda, "If capturable=False, state_steps should not be CUDA tensors."AssertionError: If capturable=False, state_steps should not be CUDA tensors.**Env & To Reproduce[复现与环境]**PYTHON 1.11.0> pytorch版本问题，将CUDA降到11.5，安装对应版本的pytorch 1.11.0我已经成功开始过训练，从75K训练到78K了，我主动停止了训练，在没有更新的情况下，再次启动训练，就弹出问题了。Arguments:    run_id:          cjgbtest    syn_dir:         E:\cjgb\SV2TTS\synthesizer    models_dir:      synthesizer/saved_models/    save_every:      1000    backup_every:    25000    log_every:       200    force_restart:   False    hparams:Checkpoint path: synthesizer\saved_models\cjgbtest\cjgbtest.ptLoading training data from: E:\cjgb\SV2TTS\synthesizer\train.txtUsing model: TacotronUsing device: cudaInitialising Tacotron Model...\Loading the json with %s {'sample_rate': 16000, 'n_fft': 800, 'num_mels': 80, 'hop_size': 200, 'win_size': 800, 'fmin': 55, 'min_level_db': -100, 'ref_level_db': 20, 'max_abs_value': 4.0, 'preemphasis': 0.97, 'preemphasize': True, 'tts_embed_dims': 512, 'tts_encoder_dims': 256, 'tts_decoder_dims': 128, 'tts_postnet_dims': 512, 'tts_encoder_K': 5, 'tts_lstm_dims': 1024, 'tts_postnet_K': 5, 'tts_num_highways': 4, 'tts_dropout': 0.5, 'tts_cleaner_names': ['basic_cleaners'], 'tts_stop_threshold': -3.4, 'tts_schedule': [[2, 0.001, 10000, 12], [2, 0.0005, 15000, 12], [2, 0.0002, 20000, 12], [2, 0.0001, 30000, 12], [2, 5e-05, 40000, 12], [2, 1e-05, 60000, 12], [2, 5e-06, 160000, 12], [2, 3e-06, 320000, 12], [2, 1e-06, 640000, 12]], 'tts_clip_grad_norm': 1.0, 'tts_eval_interval': 500, 'tts_eval_num_samples': 1, 'tts_finetune_layers': [], 'max_mel_frames': 900, 'rescale': True, 'rescaling_max': 0.9, 'synthesis_batch_size': 16, 'signal_normalization': True, 'power': 1.5, 'griffin_lim_iters': 60, 'fmax': 7600, 'allow_clipping_in_normalization': True, 'clip_mels_length': True, 'use_lws': False, 'symmetric_mels': True, 'trim_silence': True, 'speaker_embedding_size': 256, 'silence_min_duration_split': 0.4, 'utterance_min_duration': 1.6, 'use_gst': True, 'use_ser_for_gst': True}Trainable Parameters: 0.000MLoading weights at synthesizer\saved_models\cjgbtest\cjgbtest.ptTacotron weights loaded from step 78000Using inputs from:        E:\cjgb\SV2TTS\synthesizer\train.txt        E:\cjgb\SV2TTS\synthesizer\mels        E:\cjgb\SV2TTS\synthesizer\embedsFound 47 samples+----------------+------------+---------------+------------------+| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |+----------------+------------+---------------+------------------+|   82k Steps    |     6      |     5e-06     |        2         |+----------------+------------+---------------+------------------+Traceback (most recent call last):  File "E:\MockingBird-main\synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "E:\MockingBird-main\synthesizer\train.py", line 215, in train    optimizer.step()  File "G:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 109, in wrapper    return func(*args, **kwargs)  File "G:\Anaconda3\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context    return func(*args, **kwargs)  File "G:\Anaconda3\lib\site-packages\torch\optim\adam.py", line 157, in step    adam(params_with_grad,  File "G:\Anaconda3\lib\site-packages\torch\optim\adam.py", line 213, in adam    func(params,  File "G:\Anaconda3\lib\site-packages\torch\optim\adam.py", line 255, in _single_tensor_adam    assert not step_t.is_cuda, "If capturable=False, state_steps should not be CUDA tensors."AssertionError: If capturable=False, state_steps should not be CUDA tensors.论据：运行id:cjgbtestsyn\u dir:E:\cjgb\SV2TTS\synthesizermodels\u dir：合成器/保存的\u模型/每1000保存一次备份间隔：25000log_每隔：200强制重新启动：FalseH参数：检查点路径：synthesizer\saved\u models\cjgbtest\cjgbtest.pt从以下位置加载训练数据：E:\cjgb\SV2TTS\synthesizer\train.txt使用型号：Tacotron使用设备：cuda正在初始化Tacotron模型。。。\正在加载带有%s的json{sample\u rate:16000，'n\u fft:800，'num\u mels:80，'hop\u size:200，'win\u size:800，'fmin:55，'min\u level\u db'：-100，'ref\u level\u db:20，'max\u abs\u value:4.0，'preemphasis:0.97，'preemphasize:True，'tts\u embed\u dims:512，'tts\u encoder\u dims:256，'tts\u decoder\u dims:128，'tts\u postnet\u dims:512，'tts\u encoder\u K：5，“tts\u lstm\u dims”：1024，“tts\u postnet\u K”：5，“tts\u num\u highways”：4，“tts\u drop”输出：0.5，“tts\u cleaner\u names”：[“basic\u cleaners”]，“tts\u stop\u threshold”：-3.4，“tts\u schedule”：[[2，0.001，10000，12]，[2，0.0005，15000，12]，[2，0.0002，20000，12]，[2，0.0001，30000，12]，[2，5e-05，40000，12]，[2，1e-05，60000，12]，[2，5e-06，160000，12]，[2，3e-06，320000，12]，[2，1e-06640000，12]]，“tts\u clip\u grad\u norm”：1.0，“tts\u eval\u interval”：500，“tts\u eval\u num\u samples”：1，“tts\u finetune\u layers”：[]，“max\u mel\u frames”：900，“rescale”：True，“rescaling\u max”：0.9，“synthesis\u batch\u size”：16，“signal\u normalization”：True，“power”：1.5，“griffin\u lim\u iters”：60，“fmax”：7600，“allow\u clipping\u in\u normalization”：True，“clip\u mels\u length”：True，“use\u lws”：False，“symmetric\u mels”：True，“trim\u silence”：True，“speaker\u embeding\u size”：256，“silence\u min\u duration\u拆分：0.4，“outrance\u min\u duration”：1.6，“use\u gst”：True，“use\u ser\u for\u gst”：True}可训练参数：0.000M在合成器\ saved\u models\cjgbtest\cjgbtest.pt处加载权重从步骤78000加载的Tacotron重量使用以下输入：E： \cjgb\SV2TTS\synthesizer\train.txtE： \cjgb\SV2TTS\synthesizer\melsE： \cjgb\SV2TTS\synthesizer\embeddes找到47个样本+----------------+------------+---------------+------------------+|r=2的步骤|批量|学习率|输出/步骤（r）|+----------------+------------+---------------+------------------+|82k步| 6 | 5e-06 | 2|+----------------+------------+---------------+------------------+回溯（最近一次呼叫最后一次）：文件“E:\MockingBird main\synthesizer\u train.py”，第37行，在<模块>列车（**VAR（ARG））文件“E:\MockingBird main\synthesizer\train.py”，第215行，in-train优化器。步骤（）文件“G:\Anaconda3\lib\site packages\torch\optim\optimizer.py”，第109行，在包装器中return func（*args，**kwargs）文件“G:\Anaconda3\lib\site packages\torch\autograd\grad\u mode.py”，第27行，在decoration\u上下文中return func（*args，**kwargs）文件“G:\Anaconda3\lib\site packages\torch\optim\adam.py”，第157行，步骤adam（params_with_grad，adam中的文件“G:\Anaconda3\lib\site packages\torch\optim\adam.py”，第213行func（参数，文件“G:\Anaconda3\lib\site packages\torch\optim\adam.py”，第255行，在\u single\u tensor\u adam中断言not step\t。is_cuda，“如果capturable=False，则state_步长不应为cuda张量。”AssertionError：如果capturable=False，则state_步骤不应为CUDA张量。找到解决方法了首先卸载pytorchcmd    键入    pip uninstall torch         执行卸载重新装pytorch          我这里是一个旧版本：# CUDA 11.1（cmd键入以下内容）pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f 
训练到30k还是无法收敛

开始使用CPU训练 现在该如何改用GPU训练 尝试各种办法都不行
Using model: TacotronUsing device: cpu安装gpu版本的pytorch，电脑系统的cuda版本要和pytorch里面的一致python里面执行torch.cuda.is_available()后，结果为True就可以了，建议安装pytorch 1.11.0 + cuda 11.5 
训练模型时这个问题怎么办？？疑似N卡内存不够。 CUDA out of memory. Tried to allocate 122.00 MiB (GPU 0; 4.00 GiB total capacity; 3.15 GiB already allocated; 0 bytes free; 3.45 GiB reserved in total by PyTorch
**Summary[问题简述（一句话）]**训练模型时这个问题怎么办？？疑似N卡内存不够。 CUDA out of memory. Tried to allocate 122.00 MiB (GPU 0; 4.00 GiB total capacity; 3.15 GiB already allocated; 0 bytes free; 3.45 GiB reserved in total by PyTorch**Env & To Reproduce[复现与环境]**python3.9、NVIDIA GeForce GTX synthesizer/hparams.py中的batch_size參數調小//調整前tts_schedule = [(2,  1e-3,  20_000,  12),   # Progressive training schedule                (2,  5e-4,  40_000,  12),   # (r, lr, step, batch_size)                (2,  2e-4,  80_000,  12),   #                (2,  1e-4, 160_000,  12),   # r = reduction factor (# of mel frames                (2,  3e-5, 320_000,  12),   #     synthesized for each decoder iteration)                (2,  1e-5, 640_000,  12)],  # lr = learning rate//調整後tts_schedule = [(2,  1e-3,  20_000,  8),   # Progressive training schedule                (2,  5e-4,  40_000,  8),   # (r, lr, step, batch_size)                (2,  2e-4,  80_000,  8),   #                (2,  1e-4, 160_000,  8),   # r = reduction factor (# of mel frames                (2,  3e-5, 320_000,  8),   #     synthesized for each decoder iteration)                (2,  1e-5, 640_000,  8)],  # lr = learning rate> 訓練模型顯存不足 訓練合成器時：將 synthesizer/hparams.py中的batch_size參數調小> > //調整前 tts_schedule = [(2, 1e-3, 20_000, 12), # Progressive training schedule (2, 5e-4, 40_000, 12), # (r, lr, step, batch_size) (2, 2e-4, 80_000, 12), # (2, 1e-4, 160_000, 12), # r = reduction factor (# of mel frames (2, 3e-5, 320_000, 12), # synthesized for each decoder iteration) (2, 1e-5, 640_000, 12)], # lr = learning rate //調整後 tts_schedule = [(2, 1e-3, 20_000, 8), # Progressive training schedule (2, 5e-4, 40_000, 8), # (r, lr, step, batch_size) (2, 2e-4, 80_000, 8), # (2, 1e-4, 160_000, 8), # r = reduction factor (# of mel frames (2, 3e-5, 320_000, 8), # synthesized for each decoder iteration) (2, 1e-5, 640_000, 8)], # lr = learning rate感谢指导，已成功开始训练了> 我已经把batch_size调整到4了，还是出现同样的问题，唉。。。> 你是不是没装cuda?我没装之前也是这样，或者更换一下pytorch 版本 
可否提供一个api来作为tts使用
**Summary[问题简述（一句话）]**可否提供一个发送文本、设置，返回语音的api，像多数tts一样**Env & To Reproduce[复现与环境]**我已经训练了一个还行的模型，想用它作为tts来读弹幕，但是不管是web还是gui都需要手动操作，太麻烦，可否提供一个没有界面的，发送文本就返回语音的简单后台来使用。会看控制台网络么，新建一个模拟，看一下请求，自己构造即可同求 
Web服务端打开报错(提示缺少文件)
在MockingBird根目录下有这个路径跟文件，但运行python3 web.py后打开网页时候显示找不到该文件FileNotFoundError: [Errno 2] No such file or directory: '.\\mkgui\\static\\mb.png'Traceback:File "/usr/local/lib/python3.8/dist-packages/streamlit/scriptrunner/script_runner.py", line 443, in _run_script    exec(code, module.__dict__)File "/tmp/tmpi1hir5nj.py", line 14, in <module>    render_streamlit_ui()File "/root/MockingBird/mkgui/base/ui/streamlit_ui.py", line 848, in render_streamlit_ui    image = Image.open('.\\mkgui\\static\\mb.png')File "/usr/local/lib/python3.8/dist-packages/PIL/Image.py", line 3092, in open    fp = builtins.open(filename, "rb")你是在根目录运行的嘛？看下  '.\mkgui\static\mb.png' 文件在吗'.\\mkgui\\static\\mb.png' 改为'./mkgui/static/mb.png'就好了，linux下的文件系统兼容性问题 
使用自己的数据集
我是新人，请问怎么使用自己的数据集进行训练模型呢，我想用zhvoice 
hifigan的权重放到官方hifigan代码里需要做什么修改吗
我把那个24k的预训练权重放官方代码里用，也替换了官方的model.py文件，inference出来的wav没声音，请问下还需要修改哪里吗 
运行pip install -r requirements.txt 来安装剩余的必要包。 安装 C++ 14.0后仍然报错。
**Summary[问题简述（一句话）]**运行pip install -r requirements.txt 来安装剩余的必要包。 安装 C++ 14.0后仍然报错。 Building wheel for ctc-segmentation (setup.py) ... error Running setup.py clean for ctc-segmentation  Building wheel for pyworld (PEP 517) ... error**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型python3.10**Screenshots[截图（如有）]**If applicable, add screenshots to  C++ 14.0不，你要装整个的，就是那个c++开发套件> 
ModuleNotFoundError: No module named 'librosa'
ModuleNotFoundError: No module named 在requirement.txt 是有的，你安装时遇到什么错误了？直接装librosa也可以 
一次训练结束后继续训练报错
 File "C:\Users\tes223\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\optim\adam.py", line 255, in _single_tensor_adam    assert not step_t.is_cuda, "If capturable=False, state_steps should not be CUDA tensors."AssertionError: If capturable=False, state_steps should not be CUDA tensors.请问如何解决？#631 我的也是这种情况 以前不这样的  不知道为什么现在变成了这样   把torch回到1.11.0，参考#631 
Env update 添加环境需求注释
Add environmental requirement notes添加环境需求注释 
运行pip install -r requirements.txt 来安装剩余的必要包。  本步骤报错。
Collecting pytz-deprecation-shim  Using cached (15 kB)Collecting tzdata  Using cached (339 kB)Building wheels for collected packages: ctc-segmentation, pyworld  Building wheel for ctc-segmentation (setup.py) ... error  ERROR: Command errored out with exit status 1:   command: 'G:\Anaconda3\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '"'"'C:\\Users\\驿 迪\\AppData\\Local\\Temp\\pip-install-3pzcee3t\\ctc-segmentation_0c410bfa3f27404a9b4c45e8b15eb36b\\setup.py'"'"'; __file__='"'"'C:\\Users\\驿迪\\AppData\\Local\\Temp\\pip-install-3pzcee3t\\ctc-segmentation_0c410bfa3f27404a9b4c45e8b15eb36b\\setup.py'"'"';f = getattr(tokenize, '"'"'open'"'"', open)(__file__) if os.path.exists(__file__) else io.StringIO('"'"'from setuptools import setup; setup()'"'"');code = f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' bdist_wheel -d 'C:\Users\驿迪\AppData\Local\Temp\pip-wheel-ibop6woa'       cwd: C:\Users\驿迪\AppData\Local\Temp\pip-install-3pzcee3t\ctc-segmentation_0c410bfa3f27404a9b4c45e8b15eb36b\  Complete output (12 lines):  running bdist_wheel  running build  running build_py  creating build  creating build\lib.win-amd64-3.9  creating build\lib.win-amd64-3.9\ctc_segmentation  copying ctc_segmentation\ctc_segmentation.py -> build\lib.win-amd64-3.9\ctc_segmentation  copying ctc_segmentation\partitioning.py -> build\lib.win-amd64-3.9\ctc_segmentation  copying ctc_segmentation\__init__.py -> build\lib.win-amd64-3.9\ctc_segmentation  running build_ext  building 'ctc_segmentation.ctc_segmentation_dyn' extension  error: Microsoft Visual C++ 14.0 or greater is required. Get it with "Microsoft C++ Build Tools":  ----------------------------------------  ERROR: Failed building wheel for ctc-segmentation  Running setup.py clean for ctc-segmentation  Building wheel for pyworld (PEP 517) ... error  ERROR: Command errored out with exit status 1:   command: 'G:\Anaconda3\python.exe' 'G:\Anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py' build_wheel 'C:\Users\驿迪\AppData\Local\Temp\tmpmkyamjn5'       cwd: C:\Users\驿迪\AppData\Local\Temp\pip-install-3pzcee3t\pyworld_968cc4a95e404047bd2e743cd355bd23  Complete output (15 lines):  Error in sitecustomize; set PYTHONVERBOSE for traceback:  SyntaxError: (unicode error) 'utf-8' codec can't decode byte 0xe6 in position 0: invalid continuation byte (sitecustomize.py, line 21)  G:\Anaconda3\lib\site-packages\setuptools\dist.py:717: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead    warnings.warn(  running bdist_wheel  running build  running build_py  creating build  creating build\lib.win-amd64-3.9  creating build\lib.win-amd64-3.9\pyworld  copying pyworld\__init__.py -> build\lib.win-amd64-3.9\pyworld  running build_ext  skipping 'pyworld\pyworld.cpp' Cython extension (up-to-date)  building 'pyworld.pyworld' extension  error: Microsoft Visual C++ 14.0 or greater is required. Get it with "Microsoft C++ Build Tools":  ----------------------------------------  ERROR: Failed building wheel for pyworldFailed to build ctc-segmentation pyworldERROR: Could not build wheels for pyworld which use PEP 517 and cannot be installed directly需要去下载 "Microsoft C++ Build Tools":           处理了七八个问题还有   
训练后为什么会变成这样？
steps，但是效果很差很差，通过web合成出来的语言全是杂音。大佬，应该如何完善啊，在已有模型基础上继续训练么，那样音色会不会受到之前模型影响> > > > 我也是从0开始训练，用一千多字的音频切成的180个样本，然后训练了20k steps，但是效果很差很差，通过web合成出来的语言全是杂音。大佬，应该如何完善啊，在已有模型基础上继续训练么，那样音色会不会受到之前模型影响这么少的音频量，不要从0开始，基于现有模型继续训练，有奇效 
FileNotFoundError: [Errno 2] No such file or directory: 'wavs\\temp_source.wav'

Wiki 需要补全
**Summary[问题简述（一句话）]**Wiki的补全十分需要，在  这一栏里应该有版本描述。因为 torch 的版本有不同。建议部分改为 本pytorch安装部分在readme及Wiki部分都没有描述清楚。同 issue #655  因为我只有在这个版本下运行成功，欢迎大佬来指导 @babysor @moosewoler @AlexZhangji @pansila 这里有遇到什么依赖冲突嘛？我可以都更新一下到最新或者最兼容的版本@babysor 这是以OnlyCPU版本的方式安装最新版 Pytorch （以解决CUDA版本的冲突）安装最新版reuire的截图。其中 VC++ 为最新版本，GCC 执行 make 没有问题。 附安装完reque为  为 执行  为 而在  就完全没问题，只需要将 torch 改为 1.19 vision 0.10.0附 @babysor 我感觉你是python3.9的问题，我晚点试一下 
DEPENCES 依赖项不全
**What could be better?**依赖项的 requ…….txt 应该被更新，而且 pytorch 更要描述具体版本和安装方式本人配环境一个下午，勉强配出来  的版本。@babysor @oceanarium最新的会有什么问题？我记得只有python3.9会可能会有一些其他问题 
关于环境配置
**Summary[问题简述（一句话）]**所以说现在使用环境直接按照requiments.txt + webc…… 最新版 + pytorch最新版 + cu11.7可以吗？**Env & To Reproduce[复现与环境]**目前跑通用的是 re…….txt + web…… 最新 + pytorch1.9.0 + vision0.10.0 NOGPU 
能否提供数据包的格式
如果我想使用自己的数据包，我应该如何去格式化我的数据谢谢Edit 1:这边的是magicdata_dev_set的格式，我是否把我自己的数据包根据这个结构去做格式就可以了？ TRANS.txt: 应该是ok的，但估计需要改一下名字而且要加上 --skip_existing。已得到解答 
作者大神，软件载入模型时只发出了两声杂音，望回复，感谢！
用的b站视频里的百度云资源，载入模型后只有两声杂音，根据issues 不要用 pretrained，改一改参数 
这样正常么 pre.py
准备 aishell3 时 94 秒一个人。conda python 3.9 cuda 10.2 torch 1.9.0 vision 
國外合成器下載，希望可以提供其他雲端載點
**Summary[问题简述（一句话）]**無法從百度下載訓練好的合成器，可以也提供google drive連結嗎？issue区里面有一些google drive的link，可以search一下謝謝你，我已經找到了 
不能打开程序，提示缺少module。ffmpeg跟webrtcvad均正确下载
完成ffmpeg跟webrtcvad的下载，无误。在代码库路径下，运行 python demo_toolbox.py -d .\samples时提示缺少module，为PyQt5。自行下载并再次运行后，显示缺少更多module，为matplotlib，scipy，sklearn，scipy.stats._stats_py。除最后一个以外均已自行安装，最后这个不知如何下载，请求大佬解答。python，mockingbird，ffmpeg均放置在E盘，调取ffmpeg的地址正确。python版本3.9.13，ffmpeg版本4.4.1。<img width="694" alt="problem" src="https://user-images.githubusercontent.com/82459666/179083099-f962fd4d-e7de-41c9-9414-c6cf85f0f39e.PNG">建议用conda 干净的环境跑> 建议用conda 干净的环境跑转用conda的话，需要把所有的都删除重新下载吗？> > 建议用conda 干净的环境跑> > 转用conda的话，需要把所有的都删除重新下载吗？不需要的，不过会浪费一些硬盘空间> > > 建议用conda 干净的环境跑> > > > > > 转用conda的话，需要把所有的都删除重新下载吗？> > 不需要的，不过会浪费一些硬盘空间miniconda跟anconda都可以用吗？anconda文件较大，电脑硬盘装得下，但要清理> > > > 建议用conda 干净的环境跑> > > > > > > > > 转用conda的话，需要把所有的都删除重新下载吗？> > > > > > 不需要的，不过会浪费一些硬盘空间> > miniconda跟anconda都可以用吗？anconda文件较大，电脑硬盘装得下，但要清理都可以的转用miniconda后，安装时自动用miniconda替代了python并放在E盘。想要激活miniconda的environment时，在E:\Miniconda地址下输入E:\Miniconda\Scripts\activate base后依然不能激活环境。输入的内容参考了https://conda.io/activation，求教哪里出了问题。 
想问下怎么暂停训练啊
想把训练停止做别的然后空闲继续训练，但是按crtl+c之后再执行synthesizer_train就没法继续训练了默认每5000还是10000步保存一次，运行相同命令就会继续训练。算好步数就好 
训练一晚上plots文件夹还是空的
根据b站教程训练的模型，替换之前epoch跑了300多次，替换作者的模型后epoch一晚上跑了2700了，但是无论替换前后plots文件夹都是空的，见不到收敛什么的图片，求助我在colab上面训练也有这个问题，以前在本地训练是有plots的，我也想请教一下各位，是代码更新的问题还是colab的问题？可以试试这个https://github.com/babysor/MockingBird/issues/632#issuecomment-1172860266最新代码fixed了，请验证后关闭本issue 
安装requirements失败
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型python3.9**Screenshots[截图（如有）]**If applicable, add screenshots to 
size mismatch for encoder.embedding.weight exception when using miven model and NO Chinse voice but only noise
1)Use miven's model:https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ code: 20212)Got exceptionsize mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).3)NOT ABLE to have Chinese voice, but only  noise.请仔细看模型的适用代码版本，并搜索issue区解决 
fmax 8000,会对模型有什么影响吗
想做小样本学习100样本左右,微调 tacotron 的 decoder 部分<img width="433" alt="image" src="https://user-images.githubusercontent.com/32589854/178209451-0e75d0e2-f941-4030-aecf-4182328886db.png">#507 想知道fmax8000的话会对语音的相似度有什么影响吗,另外输出的这个 attention 暂时没有办法提高数据量,可以保证单个微调的单个说话人的100个样本高质量,想知道在这种情况下有什么建议呢切割重组。 
报错RuntimeError: Numpy is not available 
插入音频时出现报错RuntimeError: Numpy is not available 依赖没装好 
在gen_voice.py最后合成音频时报了这个错误，请问大家是什么情况？
在gen_voice.py最后合成音频时报了这个错误:RuntimeError: CUDA error: out of memoryCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.For debugging consider passing CUDA_LAUNCH_BLOCKING=1.是单纯的显存不够吗？还是和我在训练生成器时修改了batchsize有关？请问大家应该如何解决？训练和推理配置应该无关。 
Error pre-processing on Ubuntu
**Summary[问题简述（一句话）]**As the title**Env & To Reproduce[复现与环境]**My own dataset using aishell3 stopped here. I am not sure why this happens 
一块 2080ti ,做 finetune 的话速度大概多少 step/sec 比较合理
样本数量大概在100条左右,冻结了tacotron的前面的参数,0.3step的速度合理吗,应该大概在什么数量级呀,以及 loss大概在什么数量级的时候效果会比较好呢<img width="770" alt="image" src="https://user-images.githubusercontent.com/32589854/178133913-cf4da5a2-b10a-41bb-896a-692670ac86ba.png"><img width="547" alt="image" src="https://user-images.githubusercontent.com/32589854/178133919-4d9d0a0f-71c7-4d29-ac66-7ac0b59a4650.png">48已经很大了，loss 目前看也接近底部了，建议边跑边验证作者，我想请教一下batchsize多少比较合适？我用的64试下来效果还不如32收敛快 
训练到150k，但loss仍在0.5左右，attention图时好时坏是为什么？loss为什么减不下去？

web界面 AI拟音  提示No such file or directory:错误  求解  谢谢 
&nbsp; 好了&nbsp; &nbsp;&nbsp;------------------&nbsp;原始邮件&nbsp;------------------发件人:                                                                                                                        "babysor/MockingBird"                                                                                    ***@***.***&gt;;发送时间:&nbsp;2022年7月9日(星期六) 上午10:41***@***.***&gt;;抄送:&nbsp;"@***@***.******@***.***&gt;;主题:&nbsp;Re: [babysor/MockingBird] web界面 AI拟音  提示No such file or directory:错误  求解  谢谢  (Issue #635) 项目路径下新建文件夹wavs就行了 —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you authored the thread.Message ID: ***@***.***&gt; 
ModuleNotFoundError: No module named 'toolbox'
**Summary[问题简述（一句话）]**Traceback (most recent call last):  File "D:\Haibara-Ai\modules\MockingBird-main\demo_toolbox.py", line 2, in <module>    from toolbox import ToolboxModuleNotFoundError: No module named 'toolbox'**Env & To Reproduce[复现与环境]**python 3.9.10依赖没装好 
Merge pull request #1 from babysor/main
222 
plots and mels are removed during training
plots and mels are mistakenly removed during trainingwindows 11, pytorch 1.9.0, cuda 11.1, mockingbird downloaded on 1st of July what i understand, the code in train.py is intended to keep the most updated 20 plots/mels...but seems it removes  windows 11, pytorch 1.10.2, cuda 11.3{| Epoch: 624/1179 (73/73) | Loss: 0.3658 | 1.0 steps/s | Step: 119k | }{| Epoch: 625/1179 (73/73) | Loss: 0.3747 | 1.0 steps/s | Step: 119k | }}{| Epoch: 626/1179 (73/73) | Loss: 0.3779 | 1.0 steps/s | Step: 119k | }{| Epoch: 627/1179 (73/73) | Loss: 0.3655 | 1.0 steps/s | Step: 119k | }}{| Epoch: 628/1179 (73/73) | Loss: 0.3729 | 1.0 steps/s | Step: 119k | }}{| Epoch: 629/1179 (73/73) | Loss: 0.3694 | 1.0 steps/s | Step: 119k | }{| Epoch: 630/1179 (73/73) | Loss: 0.3718 | 1.0 steps/s | Step: 119k | }}{| Epoch: 631/1179 (10/73) | Loss: 0.3757 | 1.0 steps/s | Step: 120k | }Input at step 120000: wo3 gan3 jue2 na4 ge4 huai4 dan4 shi4 zen3 me5 lian4 cheng2 de5~______________________________________________________________________________________________{| Epoch: 631/1179 (73/73) | Loss: 0.3823 | 1.1 steps/s | Step: 120k | }{| Epoch: 632/1179 (73/73) | Loss: 0.3770 | 1.0 steps/s | Step: 120k | }{| Epoch: 633/1179 (73/73) | Loss: 0.3740 | 1.0 steps/s | Step: 120k | }{| Epoch: 634/1179 (73/73) | Loss: 0.3723 | 1.0 steps/s | Step: 120k | }{| Epoch: 635/1179 (73/73) | Loss: 0.3750 | 1.0 steps/s | Step: 120k | }{| Epoch: 636/1179 (73/73) | Loss: 0.3715 | 1.0 steps/s | Step: 120k | }{| Epoch: 637/1179 (72/73) | Loss: 0.3797 | 1.0 steps/s | Step: 120k | }Input at step 120500: zhe4 ge4 ni3 bu2 shi4 mei2 chi1 bao3~______________________________________________________________________{| Epoch: 637/1179 (73/73) | Loss: 0.3781 | 1.0 steps/s | Step: 120k | }{| Epoch: 638/1179 (73/73) | Loss: 0.3752 | 1.0 steps/s | Step: 120k | }{| Epoch: 639/1179 (73/73) | Loss: 0.3719 | 1.0 steps/s | Step: 120k | }}{| Epoch: 640/1179 (73/73) | Loss: 0.3714 | 1.0 steps/s | Step: 120k | }}{| Epoch: 641/1179 (73/73) | Loss: 0.3697 | 0.97 steps/s | Step: 120k | }{| Epoch: 642/1179 (73/73) | Loss: 0.3778 | 0.98 steps/s | Step: 120k | }{| Epoch: 643/1179 (73/73) | Loss: 0.3722 | 0.98 steps/s | Step: 120k | }{| Epoch: 644/1179 (61/73) | Loss: 0.3695 | 0.96 steps/s | Step: 121k | }Input at step 121000: ni3 yi2 ding4 shi4 ba3 wo3 de5 ben3 zhi4 kan4 qing1 chu3 le5~__________________________________________{| Epoch: 644/1179 (73/73) | Loss: 0.3712 | 0.97 steps/s | Step: 121k | }{| Epoch: 645/1179 (32/73) | Loss: 0.3713 | 0.97 steps/s | Step: 121k | }训练了十三个小时，一条预览图都没有> 请问解决了吗？我遇到了一样的情况 windows 11, pytorch 1.10.2, cuda 11.3just comment out the block of code, line 268-278, so it wont delete/remove the plots image it is just a quick fix, will not a fix of what the author intend to do (keep only the latest 20 files)> > 请问解决了吗？我遇到了一样的情况 windows 11， pytorch 1.10.2， cuda 11.3> > 只需注释掉代码块，第268-278行，因此它不会删除/删除绘图图像> > 它只是一个快速修复，不会修复作者打算做的事情（只保留最新的20个文件）好的 非常感谢Yep... I was intended to keep only 20 files per type. I will look into it this week.fixed in last trunk of code，please verify and close this issue 
capturable=False,报错
Win11 GPU：3060laptop  Python 3.9.13+----------------+------------+---------------+------------------+| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |+----------------+------------+---------------+------------------+|   101k Steps   |     16     |     3e-06     |        2         |+----------------+------------+---------------+------------------+Could not load symbol cublasGetSmCountTarget from cublas64_11.dll. Error code 127Traceback (most recent call last):  File "G:\AIvioce\MockingBird\synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "G:\AIvioce\MockingBird\synthesizer\train.py", line 216, in train    optimizer.step()  File "C:\Users\Mark\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\optimizer.py", line 109, in wrapper    return func(*args, **kwargs)  File "C:\Users\Mark\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context    return func(*args, **kwargs)  File "C:\Users\Mark\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\adam.py", line 157, in step    adam(params_with_grad,  File "C:\Users\Mark\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\adam.py", line 213, in adam    func(params,  File "C:\Users\Mark\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\adam.py", line 255, in _single_tensor_adam    assert not step_t.is_cuda, "If capturable=False, state_steps should not be CUDA tensors."AssertionError: If capturable=False, state_steps should not be CUDA tensors.G:\Vioce\MockingBird>python synthesizer_train.py tjc G:\Vioce\tjc001\SV2TTS\synthesizerArguments:    run_id:          tjc    syn_dir:         G:\Vioce\tjc001\SV2TTS\synthesizer    models_dir:      synthesizer/saved_models/    save_every:      1000    backup_every:    25000    log_every:       200    force_restart:   False    hparams:Checkpoint path: synthesizer\saved_models\tjc\tjc.ptLoading training data from: G:\Vioce\tjc001\SV2TTS\synthesizer\train.txtUsing model: TacotronUsing device: cudaInitialising Tacotron Model...\Loading the json with %s {'sample_rate': 16000, 'n_fft': 800, 'num_mels': 80, 'hop_size': 200, 'win_size': 800, 'fmin': 55, 'min_level_db': -100, 'ref_level_db': 20, 'max_abs_value': 4.0, 'preemphasis': 0.97, 'preemphasize': True, 'tts_embed_dims': 512, 'tts_encoder_dims': 256, 'tts_decoder_dims': 128, 'tts_postnet_dims': 512, 'tts_encoder_K': 5, 'tts_lstm_dims': 1024, 'tts_postnet_K': 5, 'tts_num_highways': 4, 'tts_dropout': 0.5, 'tts_cleaner_names': ['basic_cleaners'], 'tts_stop_threshold': -3.4, 'tts_schedule': [[2, 0.001, 10000, 12], [2, 0.0005, 15000, 12], [2, 0.0002, 20000, 12], [2, 0.0001, 30000, 12], [2, 5e-05, 40000, 12], [2, 1e-05, 60000, 12], [2, 5e-06, 160000, 12], [2, 3e-06, 320000, 12], [2, 1e-06, 640000, 12]], 'tts_clip_grad_norm': 1.0, 'tts_eval_interval': 500, 'tts_eval_num_samples': 1, 'tts_finetune_layers': [], 'max_mel_frames': 900, 'rescale': True, 'rescaling_max': 0.9, 'synthesis_batch_size': 16, 'signal_normalization': True, 'power': 1.5, 'griffin_lim_iters': 60, 'fmax': 7600, 'allow_clipping_in_normalization': True, 'clip_mels_length': True, 'use_lws': False, 'symmetric_mels': True, 'trim_silence': True, 'speaker_embedding_size': 256, 'silence_min_duration_split': 0.4, 'utterance_min_duration': 1.6, 'use_gst': True, 'use_ser_for_gst': True}Trainable Parameters: 32.869MLoading weights at synthesizer\saved_models\tjc\tjc.ptTacotron weights loaded from step 219000Using inputs from:        G:\Vioce\tjc001\SV2TTS\synthesizer\train.txt        G:\Vioce\tjc001\SV2TTS\synthesizer\mels        G:\Vioce\tjc001\SV2TTS\synthesizer\embedsFound 872 samples+----------------+------------+---------------+------------------+| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |+----------------+------------+---------------+------------------+|   101k Steps   |     12     |     3e-06     |        2         |+----------------+------------+---------------+------------------+Traceback (most recent call last):  File "G:\Vioce\MockingBird\synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "G:\Vioce\MockingBird\synthesizer\train.py", line 215, in train    optimizer.step()  File "C:\Users\Mark\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\optimizer.py", line 109, in wrapper    return func(*args, **kwargs)  File "C:\Users\Mark\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context    return func(*args, **kwargs)  File "C:\Users\Mark\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\adam.py", line 157, in step    adam(params_with_grad,  File "C:\Users\Mark\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\adam.py", line 213, in adam    func(params,  File "C:\Users\Mark\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\adam.py", line 255, in _single_tensor_adam    assert not step_t.is_cuda, "If capturable=False, state_steps should not be CUDA tensors."AssertionError: If capturable=False, state_steps should not be CUDA tensors.Higot in the same problem assert not step_t.is_cuda, "If capturable=False, state_steps should not be CUDA tensors."AssertionError: If capturable=False, state_steps should not be CUDA tensors.did you succeed to solve it?卸载pytorch      pip uninstall torch  然后安装pytorch CUDA 11.6可以解决Could not load symbol cublasGetSmCountTarget from cublas64_11.dll. Error code 127但是在训练时间达到五分钟后关闭训练 重启训练怎么能不报错：AssertionError: If capturable=False, state_steps should not be CUDA tensors.还没找到解决办法> Hi got in the same problem> > assert not step_t.is_cuda, "If capturable=False, state_steps should not be CUDA tensors." AssertionError: If capturable=False, state_steps should not be CUDA tensors.> > did you succeed to solve it?CPU：AMD R7 5800HGPU：RTX3060laptopWIN11按Ctrl+C 手动结束进程会损坏模型文件，导致报错 AssertionError: If capturable=False, state_steps should not be CUDA tensors.，非个例，正在寻找解决办法> 执行训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 停止训练CTRL+C或CTRL+fn+B 然后再次开始训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 报错AssertionError: If capturable=False, state_steps should not be CUDA tensors> > 这个项目不支持像Deepfacelab那样可以暂停或者多次训练的吗？ 邮件 知乎 B站均尝试过联系作者 无果 希望作者能早日看到并答疑> 执行训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 停止训练CTRL+C或CTRL+fn+B 然后再次开始训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 报错AssertionError: If capturable=False, state_steps should not be CUDA tensors> > 这个项目不支持像Deepfacelab那样可以暂停或者多次训练的吗？ 邮件 知乎 B站均尝试过联系作者 无果 希望作者能早日看到并答疑win 11, pytorch 1.9.0, cuda 11.1停止训练 using CTRL+C, and resumed without problem> > 执行训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 停止训练CTRL+C或CTRL+fn+B 然后再次开始训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 报错AssertionError: If capturable=False, state_steps should not be CUDA tensors> > 这个项目不支持像Deepfacelab那样可以暂停或者多次训练的吗？ 邮件 知乎 B站均尝试过联系作者 无果 希望作者能早日看到并答疑> > > 执行训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 停止训练CTRL+C或CTRL+fn+B 然后再次开始训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 报错AssertionError: If capturable=False, state_steps should not be CUDA tensors> > 这个项目不支持像Deepfacelab那样可以暂停或者多次训练的吗？ 邮件 知乎 B站均尝试过联系作者 无果 希望作者能早日看到并答疑> > win 11, pytorch 1.9.0, cuda 11.1> > 停止训练 using CTRL+C, and resumed without problem我的PyTorch是1.12  请问你的python是什么版本？我更换环境试一下   > > > 执行训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 停止训练CTRL+C或CTRL+fn+B 然后再次开始训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 报错AssertionError: If capturable=False, state_steps should not be CUDA tensors> > > 这个项目不支持像Deepfacelab那样可以暂停或者多次训练的吗？ 邮件 知乎 B站均尝试过联系作者 无果 希望作者能早日看到并答疑> > > > > > > 执行训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 停止训练CTRL+C或CTRL+fn+B 然后再次开始训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 报错AssertionError: If capturable=False, state_steps should not be CUDA tensors> > > 这个项目不支持像Deepfacelab那样可以暂停或者多次训练的吗？ 邮件 知乎 B站均尝试过联系作者 无果 希望作者能早日看到并答疑> > > > > > win 11, pytorch 1.9.0, cuda 11.1> > 停止训练 using CTRL+C, and resumed without problem> > 我的PyTorch是1.12 请问你的python是什么版本？ 我更换环境试一下3.7.9, i remember some people mentioned they use newer version of python. I guess you can just try do downgrade the pytorch, as the readme.md mentioned:PyTorch worked for pytorch, tested in version of 1.9.0(latest in August 2021), with GPU Tesla T4 and GTX 2060one more thing, if u end up choosing 1.9.0, suggest u use cuda 11.1 instead of 10.2, i had problem/error/crash during training but solved after changing cuda to 11.1> > > > 执行训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 停止训练CTRL+C或CTRL+fn+B 然后再次开始训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 报错AssertionError： If capturable=False， state_steps should not be CUDA tensors 这个项目不支持像Deepfacelab那样可以暂停或者多次训练的吗？ 邮件 知乎 B站均尝试过联系作者 无果 希望作者能早日看到并答疑> > > > > > > > > > 执行训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 停止训练CTRL+C或CTRL+fn+B 然后再次开始训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 报错AssertionError： If capturable=False， state_steps should not be CUDA tensors 这个项目不支持像Deepfacelab那样可以暂停或者多次训练的吗？ 邮件 知乎 B站均尝试过联系作者 无果 希望作者能早日看到并答疑> > > > > > > > > win 11， pytorch 1.9.0， cuda 11.1 停止训练 使用CTRL+C，并恢复没有问题> > > > > > 我的PyTorch是1.12 请问你的python是什么版本？ 我更换环境试一下> > 3.7.9，我记得有些人提到他们使用较新版本的python。我想你可以尝试降级pytorch，正如 readme.md 提到的：> > PyTorch适用于pytorch，在1.9.0版本（最近于2021年8月）中进行了测试，GPU Tesla T4和GTX 2060> > 还有一件事，如果你最终选择了1.9.0，建议你使用cuda 11.1而不是10.2，我在训练期间遇到了问题/错误/崩溃，但在将cuda更改为11.1后解决了非常感谢，python3.9.13  pytorch1.9  cuda11.1 Ctrl+C停止训练后确实可以继续训练  没有报错  但是step从1.1/S降低到0.8/S  我会继续尝试更换版本> > > > > 执行训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 停止训练CTRL+C或CTRL+fn+B 然后再次开始训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 报错AssertionError： If capturable=False， state_steps should not be CUDA tensors 这个项目不支持像Deepfacelab那样可以暂停或者多次训练的吗？ 邮件 知乎 B站均尝试过联系作者 无果 希望作者能早日看到并答疑> > > > > > > > > > > > > 执行训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 停止训练CTRL+C或CTRL+fn+B 然后再次开始训练python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 报错AssertionError： If capturable=False， state_steps should not be CUDA tensors 这个项目不支持像Deepfacelab那样可以暂停或者多次训练的吗？ 邮件 知乎 B站均尝试过联系作者 无果 希望作者能早日看到并答疑> > > > > > > > > > > > win 11， pytorch 1.9.0， cuda 11.1 停止训练 使用CTRL+C，并恢复没有问题> > > > > > > > > 我的PyTorch是1.12 请问你的python是什么版本？ 我更换环境试一下> > > > > > 3.7.9，我记得有些人提到他们使用较新版本的python。我想你可以尝试降级pytorch，正如 readme.md 提到的：> > PyTorch适用于pytorch，在1.9.0版本（最近于2021年8月）中进行了测试，GPU Tesla T4和GTX 2060> > 还有一件事，如果你最终选择了1.9.0，建议你使用cuda 11.1而不是10.2，我在训练期间遇到了问题/错误/崩溃，但在将cuda更改为11.1后解决了> > 非常感谢，python3.9.13 pytorch1.9 cuda11.1 Ctrl+C停止训练后确实可以继续训练 没有报错 但是step从1.1/S降低到0.8/S 我会继续尝试更换版本WIN 11    CPU：R7 5800H   GPU：3060laptopPython3.9.13  torch-1.10.2+cu113-cp39-cp39-win_amd64  无报错 正常继续训练  Steps 1/S  该问题确认解决 有人这么说的：> Hi, I am also facing the same issue when I try to load the checkpoint and resume model training on the latest pytorch (1.12).> > It seems to be related with a newly introduced parameter (capturable) for the [Adam and AdamW Currently two workarounds:> > 1. forcing capturable = True after loading the checkpoint (as suggested above)  . This seems to slow down the model training by approx. 10% (YMMV depending on the setup).> 2. Reverting pytorch back to previous versions (I have been using 1.11.0).> > I'm wondering whether enforcing  may incur unwanted side effects.我也担心 是否会带来不必要的副作用，所以我也准备回退到torch1.11. 原问题在这里：https://github.com/pytorch/pytorch/issues/80809 
Web界面预处理提示无可用的特征提取模型
**Summary[问题简述（一句话）]**Web界面的预处理功能提示无可用的特征提取模型，查看代码发现是ppg_extractor\saved_models文件夹下无可用的模型文件。是否需要执行其他脚本生成？**Env & To Reproduce[复现与环境]**win10，6月25日拉取的新版本**Screenshots[截图（如有）]**If applicable, add screenshots to 需要先下载模型请问预处理模型是这部分里提供的模型么？[2.3 Use pretrained model of 
Web版界面的功能使用提示缺少模型
**Summary[问题简述（一句话）]**Web版界面的预处理功能使用时，下拉的特征提取处理模型列表不可用**Env & To 
Merge pull request #1 from babysor/main
222 
linux下文件目录路径问题， \\识别不了
sounddevice依赖有问题，可能是python 3.9下载这个依赖版本有误，试着指定一个兼容的 
运行pre.py时出错，是虚拟内存的问题么？
运行pre.py时 报错，F:\mockingbird\MockingBird-main\MockingBird-main>python pre.py H:/data -d aidatatang_200zh -n 4Using data from:    H:\data\aidatatang_200zh\corpus\trainaidatatang_200zh:   0%|                                                    | 0/1 [00:00<?, ?speakers/s]F:\mockingbird\MockingBird-main\MockingBird-main\synthesizer\preprocess_speaker.py:66: FutureWarning: Pass sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error  wav, _ = librosa.load(wav_fpath, hparams.sample_rate)F:\mockingbird\MockingBird-main\MockingBird-main\synthesizer\audio.py:170: FutureWarning: Pass sr=16000, n_fft=800 as keyword args. From version 0.10 passing these as positional arguments will result in an error  return librosa.filters.mel(hparams.sample_rate, hparams.n_fft, n_mels=hparams.num_mels,F:\mockingbird\MockingBird-main\MockingBird-main\synthesizer\preprocess_speaker.py:66: FutureWarning: Pass sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error  wav, _ = librosa.load(wav_fpath, hparams.sample_rate)aidatatang_200zh:   0%|                                                    | 0/1 [00:43<?, ?speakers/s]multiprocessing.pool.RemoteTraceback:"""Traceback (most recent call last):  File "C:\Users\admin\AppData\Local\Programs\Python\Python39\lib\multiprocessing\pool.py", line 125, in worker    result = (True, func(*args, **kwds))  File "F:\mockingbird\MockingBird-main\MockingBird-main\synthesizer\preprocess_speaker.py", line 97, in preprocess_speaker_general    metadata.append(_process_utterance(wav, text, out_dir, sub_basename,  File "F:\mockingbird\MockingBird-main\MockingBird-main\synthesizer\preprocess_speaker.py", line 42, in _process_utterance    wav = encoder.preprocess_wav(wav, normalize=False, trim_silence=True)  File "F:\mockingbird\MockingBird-main\MockingBird-main\encoder\audio.py", line 48, in preprocess_wav    wav = trim_long_silences(wav)  File "F:\mockingbird\MockingBird-main\MockingBird-main\encoder\audio.py", line 83, in trim_long_silences    pcm_wave = struct.pack("%dh" % len(wav), *(np.round(wav * int16_max)).astype(np.int16))MemoryError"""The above exception was the direct cause of the following exception:Traceback (most recent call last):  File "F:\mockingbird\MockingBird-main\MockingBird-main\pre.py", line 74, in <module>    preprocess_dataset(**vars(args))  File "F:\mockingbird\MockingBird-main\MockingBird-main\synthesizer\preprocess.py", line 74, in preprocess_dataset    for speaker_metadata in tqdm(job, dataset, len(speaker_dirs), unit="speakers"):  File "C:\Users\admin\AppData\Local\Programs\Python\Python39\lib\site-packages\tqdm\std.py", line 1195, in __iter__    for obj in iterable:  File "C:\Users\admin\AppData\Local\Programs\Python\Python39\lib\multiprocessing\pool.py", line 870, in next    raise valueMemoryError我跟你一样的错误 解决了吗按readme处理虚拟内存 
is not supported, please vote for it in https://github.com/babysor/MockingBird/issues/10
Traceback (most recent call last):  File "MockingBird-main\pre.py", line 55, in <module>    assert args.dataset in recognized_datasets, 'is not supported, please vote for it in is not supported, please vote for it in 谢谢了------------------&nbsp;原始邮件&nbsp;------------------发件人: ***@***.***&gt;; 发送时间: 2022年6月21日(星期二) 晚上11:32收件人: ***@***.***&gt;; 抄送: ***@***.***&gt;; ***@***.***&gt;; 主题: Re: [babysor/MockingBird] is not supported, please vote for it in (Issue #625) 没正确指定数据集类型，贴一下命令？ —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you authored the thread.Message ID: ***@***.***&gt;怎么操作  请使用正确的issue提问，这里的问题没头没尾，无法知道你要操作什么 
请问这段源码是不是有问题？
**Summary[问题简述（一句话）]**A clear and concise description of what the issue & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型master分支，mkgui/app_vc.py**Screenshots[截图（如有）]**If applicable, add screenshots to help已修复，下次可以试着直接改，发起pr，成为贡献者 
MockingBird 路线图
已移动到Discussion: #622 
【MockingBird 何去何从】之路线图规划
This article illustrates the basic roadmap of MockingBird  🐦. If you want to contribute to MockingBird 🐦and you can pick one here to start. Please don't hesitate to ask question here and join our group of enthusiastic developers.本文主要阐述Mocking Bird🐦的产品路线规划，如果您有想要参与项目贡献，可以在这里找到一个未完成的方向进行。欢迎提出任何问题，并期待一起快乐开发。除了开发贡献，也欢迎提出产品建议，但请同时体谅，由于这是开源中立项目，有部分建议可能不会被最终采纳或放在高优先级。### v0.1.0 里程碑 Milestones---- [x]  新版GUI Web服务框架选型与实现 [ ]  解决GUI Web服务 稳定性与兼容问题- [ ] 重构代码，为集成CoquiTTS作为后端训练框架准备- [ ] 解决数字无法被正确发音的bug### v0.2.0 里程碑 Milestones---- [ ] 新模型支持Fast Speech.- [ ] 新模型支持Fast Pitch.- [ ] 傻瓜化下载模型，彻底解决模型兼容问题- [ ] 文档补充   - [ ] 开发入门指引   - [ ] 如何使用接口或命令行处理### 长期规划---- [ ] 新合成模型支持VITS.- [ ] 新Vocoder模型支持.- [ ] 多语言模型支持.此外，以下是不同主题的长期交流频道, 同时欢迎使用Github论坛交流：如何改参数，搞出更逼真的克隆效果  & 合法合规）https://github.com/babysor/MockingBird/issues/439 
python web.py执行抛错
3.7? 确实有点兼容问题，正在修复，再试试，已经推了修复 
python web.py运行报错，求解~~~
环境操作系统：window10Anaconda版本：4.3Python版本：3.7运行：python demo_toolbox.py -d .\samples （可以正常运行）运行：python web.py报错，错误信息如下：(pythone37) E:\CACHE_DATA\CHROME\MockingBird-main-new\MockingBird-main>python web.py  You can now view your Streamlit app in your browser.  Network URL:  External URL: synthesizer models: 2Loaded encoders models: 2Loaded vocoders models: 22022-06-16 19:48:10.184 Traceback (most recent call last):  File "E:\THIRD_PARTY\Anaconda3\envs\pythone37\lib\site-packages\streamlit\scriptrunner\script_runner.py", line 443, in _run_script    exec(code, module.__dict__)  File "C:\Users\CHARLES\AppData\Local\Temp\tmp0t72pr3r.py", line 14, in <module>    render_streamlit_ui()  File "E:\CACHE_DATA\CHROME\MockingBird-main-new\MockingBird-main\mkgui\base\ui\streamlit_ui.py", line 838, in render_streamlit_ui    opyrator = getOpyrator(mode)  File "E:\CACHE_DATA\CHROME\MockingBird-main-new\MockingBird-main\mkgui\base\ui\streamlit_ui.py", line 818, in getOpyrator    from mkgui.app import synthesize  File "E:\CACHE_DATA\CHROME\MockingBird-main-new\MockingBird-main\mkgui\app.py", line 75, in <module>    class Output(BaseModel):  File "E:\CACHE_DATA\CHROME\MockingBird-main-new\MockingBird-main\mkgui\app.py", line 76, in Output    __root__: tuple[AudioEntity, AudioEntity]TypeError: 'type' object is not subscriptable已经推了修复 
下载完成后，确保 xxx.pt 格式的文件放在代码库的 synthesizer\saved_models文件夹下，是什么意思？
<img width="1060" alt="图片" src="https://user-images.githubusercontent.com/22427032/173240380-775b3b23-3c4d-4b70-b831-aeaf233bdf79.png"><img width="625" alt="图片" src="https://user-images.githubusercontent.com/22427032/173240469-a721760f-bb30-4463-bc73-973190f64eda.png"><img width="841" alt="图片" src="https://user-images.githubusercontent.com/22427032/173240542-432beb4c-56d6-4768-83dd-45bb756dff97.png">synthesizer放到synthesizer/saved_models另两个也是分别放到对应得saved_models下 
MacBook在运行python demo_toolbox.py -d .\samples时报错
实际我是有安装PyQt5的，网上说了PyQt@5就是PyQt5。<img width="491" alt="图片" src="https://user-images.githubusercontent.com/22427032/173240261-25972477-abf0-4017-a8bc-f40b8c7b423e.png"><img width="491" alt="图片" src="https://user-images.githubusercontent.com/22427032/173240295-61d2ec48-a9a2-4fca-a336-3acf2b58df98.png">brew安装还不行，需要在python库安装，试着pip install pyqt我也有这个问题mac 不推荐用brew安装依赖 
运行python demo_toolbox.py 出现UnicodeDecodeError错误
**Summary[问题简述（一句话）]**运行python demo_toolbox.py 出现UnicodeDecodeError错误**Env & To Reproduce[复现与环境]**在成功出现工具界面后闪退。使用的python 是anaconda python 3.9编译工具是visual studio code平台是笔记本电脑控制台输出如下：PS D:\#_MockingBird-main> python demo_toolbox.pyArguments:    datasets_root:          None    vc_mode:                False    enc_models_dir:         encoder\saved_models    syn_models_dir:         synthesizer\saved_models    voc_models_dir:         vocoder\saved_models    extractor_models_dir:   ppg_extractor\saved_models    convertor_models_dir:   ppg2mel\saved_models    cpu:                    False    seed:                   None    no_mp3_support:         FalseWarning: you did not pass a root directory for datasets as argument.The recognized datasets are:        LibriSpeech/dev-clean        LibriSpeech/dev-other        LibriSpeech/test-clean        LibriSpeech/test-other        LibriSpeech/train-clean-100        LibriSpeech/train-clean-360        LibriSpeech/train-other-500        LibriTTS/dev-clean        LibriTTS/dev-other        LibriTTS/test-clean        LibriTTS/test-other        LibriTTS/train-clean-100        LibriTTS/train-clean-360        LibriTTS/train-other-500        LJSpeech-1.1        VoxCeleb1/wav        VoxCeleb1/test_wav        VoxCeleb2/dev/aac        VoxCeleb2/test/aac        VCTK-Corpus/wav48        aidatatang_200zh/corpus/dev        aidatatang_200zh/corpus/test        aishell3/test/wav        magicdata/trainFeel free to add your own. You can still use the toolbox by recording samples yourself.Traceback (most recent call last):  File "D:\#_MockingBird-main\demo_toolbox.py", line 49, in <module>    Toolbox(**vars(args))  File "D:\#_MockingBird-main\toolbox\__init__.py", line 81, in __init__    self.setup_events()  File "D:\#_MockingBird-main\toolbox\__init__.py", line 127, in setup_events    self.ui.setup_audio_devices(Synthesizer.sample_rate)  File "D:\#_MockingBird-main\toolbox\ui.py", line 149, in setup_audio_devices    for device in sd.query_devices():  File "C:\Users\jiuge\.conda\envs\MockingBird\lib\site-packages\sounddevice.py", line 559, in query_devices    return DeviceList(query_devices(i)  File "C:\Users\jiuge\.conda\envs\MockingBird\lib\site-packages\sounddevice.py", line 559, in <genexpr>    return DeviceList(query_devices(i)  File "C:\Users\jiuge\.conda\envs\MockingBird\lib\site-packages\sounddevice.py", line 573, in query_devices    name = name_bytes.decode('utf-8')**UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 6: invalid continuation byte**发现是由 sd.query_devices() 这个函数引发的UnicodeDecodeError 
hifigan的训练无法直接使用cpu，且修改代码后无法接着训练
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.hifigan的训练无法直接使用cpu，且修改代码后无法接着训练**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型最新环境、代码版本，模型：hifigan**Screenshots[截图（如有）]**If applicable, add screenshots to help将MockingBird-main\vocoder\hifigan下trian.py中41行torch.cuda.manual_seed(h.seed)改为torch..manual_seed(h.seed)；42行 device = torch.device('cuda:{:d}'.format(rank))改为device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')。之后可以以cpu训练，但每次运行相同代码无法接着上一次训练。保存的不能继续吗？用断点追一下，可能是由什么报错导致的。C:\Users\HSQzs\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\functional.py:695: UserWarning: stft will soon require the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release. (Triggered internally at  C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\SpectralOps.cpp:798.)无法解决 
Web server 无法跑起来
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型Debian 11Python 3.9 **Screenshots[截图（如有）]**If applicable, add screenshots to click<=8.0.4 试试@babysor 我已经解决了，我还自己打了一个docker镜像，可以正常使用。https://hub.docker.com/r/jearton1024/mocking-bird欢迎试用一下，提点建议，我会不断改进的。> @babysor 我已经解决了，我还自己打了一个docker镜像，可以正常使用。https://hub.docker.com/r/jearton1024/mocking-bird> > 欢迎试用一下，提点建议，我会不断改进的。可以发个新issue共享一下，这个有指引吗，可以合并贡献到readme里> @babysor 我已经解决了，我还自己打了一个docker镜像，可以正常使用。https://hub.docker.com/r/jearton1024/mocking-bird> > 欢迎试用一下，提点建议，我会不断改进的。您好，请问您是怎么解决这个问题的？@jearton 你的docker镜像没有模型吗，打开8080网页里没有合成模型> @jearton 你的docker镜像没有模型吗，打开8080网页里没有合成模型模型都在data目录下面，可以从宿主机上挂载进去，再重启容器就能看到了 
Synthesizer loss increases/diverges under training with GPU
**Summary[问题简述（一句话）]**If I use CPU to train the synthesizer, under the fine-tuning methodology, I get good results and the loss has been decreasing over time. However, when I moved the models over to an Ubuntu container, running ROCm for GPU acceleration using the AMD graphics cards, the loss actually diverges.Has anyone else experienced this, and if so, how did you solve it?**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型Ubuntu 20.04ROCm 5.1, using RX580Pytorch 1.11aidatatang_200zh**Screenshots[截图（如有）]**If applicable, add screenshots to helpWTF.... It's quite a funny scene and you should file a bug to AMD. 
生成完语音会释放内存嘛，总是内存占满
如上，好像确实内存有引用到音频，你找到位置了吗？ 
feat: Convert digit in the text to Chinese
新增数字转换成中文脚本是不是还差一些代码？> 是不是还差一些代码？text_to_chinese函数已经导入synthesizer/inference.py，具体里面怎么对每一段文件进行转换，没太搞懂，所有就没加上> > 是不是还差一些代码？> > text_to_chinese函数已经导入synthesizer/inference.py，具体里面怎么对每一段文件进行转换，没太搞懂，所有就没加上我周末测试一下， 
mac 下一直是杂音，下了几个模型都不行
**Summary[问题简述（一句话）]**mac 下一直是杂音，下了几个模型都不行**Env & To Reproduce[复现与环境]**mac , git clone的最新代码**Screenshots[截图（如有）]**我也是主要用mac，这里有报错吗> 我也是主要用mac，这里有报错吗 File "/Users/xiefei/Library/Python/3.8/lib/python/site-packages/matplotlib/artist.py", line 73, in draw_wrapper    result = draw(artist, renderer, *args, **kwargs)  File "/Users/xiefei/Library/Python/3.8/lib/python/site-packages/matplotlib/artist.py", line 50, in draw_wrapper    return draw(artist, renderer)  File "/Users/xiefei/Library/Python/3.8/lib/python/site-packages/matplotlib/figure.py", line 2823, in draw    artists = self._get_draw_artists(renderer)  File "/Users/xiefei/Library/Python/3.8/lib/python/site-packages/matplotlib/figure.py", line 238, in _get_draw_artists    ax.apply_aspect()  File "/Users/xiefei/Library/Python/3.8/lib/python/site-packages/matplotlib/axes/_base.py", line 1972, in apply_aspect    self.set_xbound(x_trf.inverted().transform([x0, x1]))  File "/Users/xiefei/Library/Python/3.8/lib/python/site-packages/matplotlib/axes/_base.py", line 3573, in set_xbound    self.set_xlim(sorted((lower, upper),  File "/Users/xiefei/Library/Python/3.8/lib/python/site-packages/matplotlib/axes/_base.py", line 3697, in set_xlim    left = self._validate_converted_limits(left, self.convert_xunits)  File "/Users/xiefei/Library/Python/3.8/lib/python/site-packages/matplotlib/axes/_base.py", line 3614, in _validate_converted_limits    raise ValueError("Axis limits cannot be NaN or Inf")ValueError: Axis limits cannot be NaN or Inf有这个报错<img width="1080" alt="image" src="https://user-images.githubusercontent.com/111316/172785347-50bb3711-a4c6-44d9-ae7b-1142de19e503.png">能帮忙看看吗？没找到具体原因#37 试试？（作者肯定也是这个回答）> #37 试试？（作者肯定也是这个回答）不管是mac，win也有这毛病，确实是#37的毛病。说人话就是半年了代码还没修改我知道有这个问题，所以代码下载后就改了**<img width="876" alt="image" src="https://user-images.githubusercontent.com/111316/172981382-d8d583b5-b206-4a50-8d3b-51689688695d.png">**但还是同样的问题，输出全是杂音如果切到0.0.1版本，报错为：<img width="1008" alt="image" src="https://user-images.githubusercontent.com/111316/172981940-14ad7fd6-5a6f-45b3-a9cc-2697fb1ee7df.png">> > 如果切到0.0.1版本，报错为： <img alt="image" width="1008" src="https://user-images.githubusercontent.com/111316/172981940-14ad7fd6-5a6f-45b3-a9cc-2697fb1ee7df.png">我后来修改了代码，跟你的结果一样。另外切换tag0.0.1请使用已知支持的模型，mian版本的如果没有做兼容将无法使用这种问题我没遇到过> Author> > 我知道有这个问题，所以代码下载后就改了 ** <img alt="image" width="876" src="https://user-images.githubusercontent.com/111316/172981382-d8d583b5-b206-4a50-8d3b-51689688695d.png"> ** 但还是同样的问题，输出全是杂音我看了你的报错，你试试在tag0.0.1里使用web工具，并且缩减输入音频到3秒> > Author> > > > > > 我知道有这个问题，所以代码下载后就改了 ** <img alt="image" width="876" src="https://user-images.githubusercontent.com/111316/172981382-d8d583b5-b206-4a50-8d3b-51689688695d.png"> ** 但还是同样的问题，输出全是杂音> > 我看了你的报错，你试试在tag0.0.1里使用web工具，并且缩减输入音频到3秒最新的代码不行,tag 0.0.1 三秒正常，后面就杂音了> 1里使用web工具，并且缩减输入音频tag0.0.1的版本有bug。。后面修复了。但这里确实没法兼容。。 
vc模式下电音（模型兼容性）以及24khifi_仅仅只有16000hz的问题，以及对要变声的输入hz的自适应和输出是否能达到41000hz的假设
encode太大而vocode仅仅只有16000hz，一听就知道是类似于采样率解释的问题（一般的win设备最高只有48000采样率）（而且还有电音）windows10 \dell G3 3500(RTX-2060)\vscode insider\3.9.12 pip only pretrained以及其他模型如rnn都能正常运作，有的甚至比24k清晰不知道多少倍，但是生成的电音实在是太严重，你还是可以听出夹杂其中优秀的目标嗓音我这里全是杂音你这里要自己调参数，random什么的多调调我的参数 random 10 （多改改）style2accurate 8![屏幕截图 2022-07-17 
这个有统一的吗
版本太多不兼容了，一直报错版本问题，建议搞个colab版本5月最后一个版本的main请使用最新版的numpy，另外确实有大量旧版库，python最好大于3.6 
python demo_toolbox.py
Arguments:    datasets_root:          None    vc_mode:                False    enc_models_dir:         encoder/saved_models    syn_models_dir:         synthesizer/saved_models    voc_models_dir:         vocoder/saved_models    extractor_models_dir:   ppg_extractor/saved_models    convertor_models_dir:   ppg2mel/saved_models    cpu:                    False    seed:                   None    no_mp3_support:         Falseqt.qpa.xcb: could not connect to display qt.qpa.plugin: Could not load the Qt platform plugin "xcb" in "" even though it was found.This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.Available platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, webgl, xcb.pyQT没有安装好，可能跟你本地python环境有冲突，建议使用虚拟环境、 
python demo_toolbox.py
Arguments:    datasets_root:          None    vc_mode:                False    enc_models_dir:         encoder/saved_models    syn_models_dir:         synthesizer/saved_models    voc_models_dir:         vocoder/saved_models    extractor_models_dir:   ppg_extractor/saved_models    convertor_models_dir:   ppg2mel/saved_models    cpu:                    False    seed:                   None    no_mp3_support:         Falseqt.qpa.xcb: could not connect to display qt.qpa.plugin: Could not load the Qt platform plugin "xcb" in "" even though it was found.This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.Available platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, webgl, xcb. 
CMD进行音频和梅尔频谱图预处理 报错
cd MockingBird && python pre.py D:\madedata -d aidatatang_200zh -n 5D:\ai music\MockingBird-main>python pre.py D:\madedata -d aidatatang_200zh -n 5Traceback (most recent call last):  File "D:\ai music\MockingBird-main\pre.py", line 57, in <module>    assert args.datasets_root.exists()AssertionError源代码：python pre.py <datasets_root> -d {dataset} -n {number}不确定“cd MockingBird &&”哪里来可检查datasets是否为aidatatang_200zh可参考https://www.bilibili.com/video/BV1dq4y137pH 
Torch.size mismatch in encoder and decoder. using pretrained model.
利用的是@miven的预训练模型报错信息如下：RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]).        size mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([384, 1280]).        size mismatch for decoder.rnn_input.weight: copying a param with shape torch.Size([1024, 640]) from checkpoint, the shape in current model is torch.Size([1024, 1152]).        size mismatch for decoder.stop_proj.weight: copying a param with shape torch.Size([1, 1536]) from checkpoint, the shape in current model is torch.Size([1, 2048])还有就是，我是小白，想问下tag 0.0.1去哪找啊...不是太懂这个tag 0.0.1是什么意思。https://user-images.githubusercontent.com/42199191/156316338-56e63a74-c8a4-4421-a18a-cfe961d2c407.png)> does not work. Same error still be therereally?you can try this: really? you can try this: seems it will work  if  uncomment "characters =...1240..." as the replacement  in the  synthesizer/utils/symbols.py   for the model from @FawenYo > and more over  the accent is decoded to Taiwan Mandarin >  same for @miven model , this model spec is blur, quality is not good sorrry，Your description is not clear. Did you encounter any mistakes？I guess you're new.This may have been put forward before. You can refer to 
No matching distribution found for numpy==1.19.3
**Summary[问题简述（一句话）]**No matching distribution found for numpy==1.19.3**Env & To Reproduce[复现与环境]**python3.10 **Screenshots[截图（如有）]**ERROR: Command errored out with exit status 1:     command: 'C:\ProgramData\Anaconda3\envs\Tensorflow\python.exe' 'C:\ProgramData\Anaconda3\envs\Tensorflow\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py' prepare_metadata_for_build_wheel 'C:\Users\ADMINI~1\AppData\Local\Temp\tmpim2rd1dg'         cwd: C:\Users\Administrator\AppData\Local\Temp\pip-install-jd3vwhom\numpy_e791d634113e45f4bd012817fb60b19f    Complete output (275 lines):    setup.py:67: RuntimeWarning: NumPy 1.19.3 may not yet support Python 3.10.降到python3.9 
NEUTRAL ENCODING 
Hi I was wondering if you had a vay to manipulate the encoding and finding the most gender neutral one!!!! 
训练synthesizer出现错误
Loading weights at synthesizer\saved_models\jia\jia.ptTacotron weights loaded from step 0Using inputs from:        Z:\制作数据集\1\SV2TTS\synthesizer\train.txt        Z:\制作数据集\1\SV2TTS\synthesizer\mels        Z:\制作数据集\1\SV2TTS\synthesizer\embedsFound 120 samples+----------------+------------+---------------+------------------+| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |+----------------+------------+---------------+------------------+|   10k Steps    |     12     |     0.001     |        2         |+----------------+------------+---------------+------------------+Traceback (most recent call last):  File "Z:\deeplearing_project\MockingBird-main\synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "Z:\deeplearing_project\MockingBird-main\synthesizer\train.py", line 199, in train    m1_hat, m2_hat, attention, stop_pred = model(texts, mels, embeds)  File "C:\Users\CHOPY\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl    return forward_call(*input, **kwargs)  File "Z:\deeplearing_project\MockingBird-main\synthesizer\models\tacotron.py", line 406, in forward    style_embed = self.gst(speaker_embedding, speaker_embedding) # for training, speaker embedding can represent both style inputs and referenced  File "C:\Users\CHOPY\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl    return forward_call(*input, **kwargs)  File "Z:\deeplearing_project\MockingBird-main\synthesizer\models\global_style_token.py", line 26, in forward    style_embed = self.stl(enc_out)  File "C:\Users\CHOPY\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl    return forward_call(*input, **kwargs)  File "Z:\deeplearing_project\MockingBird-main\synthesizer\models\global_style_token.py", line 101, in forward    style_embed = self.attention(query, keys)  File "C:\Users\CHOPY\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl    return forward_call(*input, **kwargs)  File "Z:\deeplearing_project\MockingBird-main\synthesizer\models\global_style_token.py", line 127, in forward    querys = self.W_query(query)  # [N, T_q, num_units]  File "C:\Users\CHOPY\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl    return forward_call(*input, **kwargs)  File "C:\Users\CHOPY\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\linear.py", line 103, in forward    return F.linear(input, self.weight, self.bias)  File "C:\Users\CHOPY\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\functional.py", line 1848, in linear    return torch._C._nn.linear(input, weight, bias)RuntimeError: mat1 and mat2 shapes cannot be multiplied (12x512 and 768x512)没见过的错误 ！！建议重下一个项目，可能是代码缺失也可以尝试改batch size值> 建议重下一个项目，可能是代码是的重下就好了，但是确实第一次见这个错误第一次看到这个错误，你有换python版本吗？ 
protoc 跑不起来训练，_pb2.py  in <module>到底哪个版本可以啊？
protoc 哪个版本能跑起来synthesizer_traina啊？3.19.421.0.rc21.1都不行还是一大堆_pb2.py file,in<module>traceback File "D:\MockingBird-main\synthesizer_train.py", line 2, in <module>    from synthesizer.train import train  File "D:\MockingBird-main\synthesizer\train.py", line 5, in <module>    from torch.utils.tensorboard import SummaryWriter  File "C:\Users\H410MH\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\utils\tensorboard\__init__.py", line 10, in <module>    from .writer import FileWriter, SummaryWriter  # noqa: F401  File "C:\Users\H410MH\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\utils\tensorboard\writer.py", line 9, in <module>    from tensorboard.compat.proto.event_pb2 import SessionLog  File "C:\Users\H410MH\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorboard\compat\proto\event_pb2.py", line 17, in <module>    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2  File "C:\Users\H410MH\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorboard\compat\proto\summary_pb2.py", line 17, in <module>    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2  File "C:\Users\H410MH\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorboard\compat\proto\tensor_pb2.py", line 16, in <module>    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2  File "C:\Users\H410MH\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorboard\compat\proto\resource_handle_pb2.py", line 16, in <module>    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2  File "C:\Users\H410MH\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorboard\compat\proto\tensor_shape_pb2.py", line 36, in <module>    _descriptor.FieldDescriptor(  File "C:\Users\H410MH\AppData\Local\Programs\Python\Python39\lib\site-packages\google\protobuf\descriptor.py", line 560, in __new__    _message.Message._CheckCalledFromGeneratedFile()类型TypeError: Descriptors cannot not be created directly.If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.If you cannot immediately regenerate your protos, some other possible workarounds are: 1. Downgrade the protobuf package to 3.20.x or lower.tensor环境有依赖冲突，建议搞个新的python venv> tensor环境有依赖冲突，建议搞个新的python venv解决了，谢谢大神！太感谢了，可以close issues了 
ValueError: Input signal length=0 is too small to resample from 48000->16000
╭─ayn@Ayn in ~/Alstnc/MockingBird-main via  v3.9.8 took 1m32s[🔴] × python pre.py /home/ayn/Alstnc/ayn/ -d aidatatang_200zh -n 7Using data from:    /home/ayn/Alstnc/ayn/aidatatang_200zh/corpus/trainaidatatang_200zh:   0%|                                                              | 0/1 [01:34<?, ?speakers/s]multiprocessing.pool.RemoteTraceback: """Traceback (most recent call last):  File "/home/ayn/.pyenv/versions/3.9.8/lib/python3.9/multiprocessing/pool.py", line 125, in worker    result = (True, func(*args, **kwds))  File "/home/ayn/Alstnc/MockingBird-main/synthesizer/preprocess_speaker.py", line 96, in preprocess_speaker_general    wav, text = _split_on_silences(wav_fpath, words, hparams)  File "/home/ayn/Alstnc/MockingBird-main/synthesizer/preprocess_speaker.py", line 66, in _split_on_silences    wav, _ = librosa.load(wav_fpath, sr= hparams.sample_rate)  File "/home/ayn/.pyenv/versions/3.9.8/lib/python3.9/site-packages/librosa/core/audio.py", line 175, in load    y = resample(y, sr_native, sr, res_type=res_type)  File "/home/ayn/.pyenv/versions/3.9.8/lib/python3.9/site-packages/librosa/core/audio.py", line 604, in resample    y_hat = resampy.resample(y, orig_sr, target_sr, filter=res_type, axis=-1)  File "/home/ayn/.pyenv/versions/3.9.8/lib/python3.9/site-packages/resampy/core.py", line 97, in resample    raise ValueError('Input signal length={} is too small to 'ValueError: Input signal length=0 is too small to resample from 48000->16000"""The above exception was the direct cause of the following exception:Traceback (most recent call last):  File "/home/ayn/Alstnc/MockingBird-main/pre.py", line 74, in <module>    preprocess_dataset(**vars(args))  File "/home/ayn/Alstnc/MockingBird-main/synthesizer/preprocess.py", line 74, in preprocess_dataset    for speaker_metadata in tqdm(job, dataset, len(speaker_dirs), unit="speakers"):  File "/home/ayn/.pyenv/versions/3.9.8/lib/python3.9/site-packages/tqdm/std.py", line 1195, in __iter__    for obj in iterable:  File "/home/ayn/.pyenv/versions/3.9.8/lib/python3.9/multiprocessing/pool.py", line 870, in next    raise valueValueError: Input signal length=0 is too small to resample from 48000->16000我是双系统电脑，在window环境下依然也是这个报错 ╭─ayn@Ayn in ~/Alstnc/MockingBird-main via  v3.9.8 took 1m36s[🔴] × lspci |grep -i  vga00:02.0 VGA compatible controller: Intel Corporation CometLake-H GT2 [UHD Graphics] (rev 05)01:00.0 VGA compatible controller: NVIDIA Corporation TU117M (rev a1) ╭─ayn@Ayn in ~/Alstnc/MockingBird-main via  v3.9.8 took 2s ╰─λ pyenv global system ╭─ayn@Ayn in ~/Alstnc/MockingBird-main via  v3.10.4 took 21ms ╰─λ pyenv global 3.9.8 ╭─ayn@Ayn in ~/Alstnc/MockingBird-main via  v3.9.8 took 15ms ╰─λ 显卡：集显加独显python是pyenv模拟的3.9.8cuda 10.2 
合成时想采用训练集中的数据的embed数据的均值应该怎样修改ui.py文件
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型**Screenshots[截图（如有）]**If applicable, add screenshots to help能再描述一下你的问题吗？不确定你想要的场景 
insert pauses
Is there any way in which we could insert a pause of some kind ?You can try adding some pausing symbols. 
分享一下利用colab来训练模型的一种方案
分享一个利用colab训练模型的一种方案，希望对大家有用：https://colab.research.google.com/drive/1fbkPoS8oaov83a3VfbhRk9exAcEpkixc?usp=sharing示例数据集，基于pretrained-11-7-21_75k进行训练：https://drive.google.com/file/d/1KgXUGPrXbGJJTi5T35GgazPS4EDAya4K/view?usp=sharinggood精简了一下，去掉了不必要的安装内容大佬能加个好友或者群吗？我可以付费咨询qt.qpa.xcb: could not connect to display qt.qpa.plugin: Could not load the Qt platform plugin "xcb" in "" even though it was found.This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.colab问题> qt.qpa.xcb: could not connect to display qt.qpa.plugin: Could not load the Qt platform plugin "xcb" in "" even though it was found. This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.> > colab问题Colab不支持GUI，这里要用命令行请问楼主，有没有出现这两个情况：1、显示0.9steps/s，但却6s才出一步2、无法生成plots 
小白想问一下注意力模型的图像代表什么含义？

根据作者训练进度继续训练合成器报错
size mismatch for gst.stl.attention.W_query.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).是顺着my_run8_25k.pt练的顺着pretrained-11-7-21_75k.pt就没问题> size mismatch for gst.stl.attention.W_query.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]). 是顺着my_run8_25k.pt练的 顺着pretrained-11-7-21_75k.pt就没问题> size mismatch for gst.stl.attention.W_query.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]). 是顺着my_run8_25k.pt练的 顺着pretrained-11-7-21_75k.pt就没问题请问如何根据作者训练进度继续训练呢？我想顺着pretrained-11-7-21_75k.pt，应该把这个pt文件重命名吗？然后放到哪个文件夹下呢？> 请问如何根据作者训练进度继续训练呢？我想顺着pretrained-11-7-21_75k.pt，应该把这个pt文件重命名吗？然后放到哪个文件夹下呢？就是先开始训练，等那个epoch出来了之后ctrl c终止，进入synthesizer/saved_models/<模型名称>文件夹，将<模型名称>.pt删掉，然后把pretrained-11-7-21_75.pt拷进去，改名为<模型名称>.pt，再次训练就行了（<模型名称>就是您的新模型的名字）kill -9 进程号终止可以吗？---- 回复的原邮件 ----| 发件人 | YuChen ***@***.***> || 日期 | 2022年07月09日 13:03 || 收件人 | ***@***.***> || 抄送至 | ***@***.******@***.***> || 主题 | Re: [babysor/MockingBird] 根据作者训练进度继续训练合成器报错 (Issue #592) |请问如何根据作者训练进度继续训练呢？我想顺着pretrained-11-7-21_75k.pt，应该把这个pt文件重命名吗？然后放到哪个文件夹下呢？就是先开始训练，等那个epoch出来了之后ctrl c终止，进入synthesizer/saved_models/<模型名称>文件夹，将<模型名称>.pt删掉，然后把pretrained-11-7-21_75.pt拷进去，改名为<模型名称>.pt，再次训练就行了（<模型名称>就是您的新模型的名字）—Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you commented.Message ID: ***@***.***>> kill -9 进程号终止可以吗？应该可以 
求助一下 用特定的人物训练synthesizer 越训练越糟糕
想训练一个特定人物的synthesizer， 严格根据  **MockingBird数据集制作教程-手把手教你克隆海子姐的声线** 教程做的  数据集大概90句，社区模型换了N个，这些社区模型直接用没有问题，但是synthesizer_train了以后越来越模糊 ，训练1K左右有PLOTS图，看上去收敛很正常，但使用只有纯电音，完全不会说话了，数据集、代码仔细检查了应该没问题的 不知道什么原因 求助各位大佬看看loss，还有90句训练1k应该过拟合了吧> 看看loss，还有90句训练1k应该过拟合了吧感谢大佬 弱弱问一句  拟合是什么意思> 感谢大佬 弱弱问一句 拟合是什么意思我也不太清楚（我语法还没学全呢），我也不是大佬，我回答这个问题完全是因为我发了N个这样的问题真正的大佬一个也没回 
啥报错都没有了，但就是不像，时而电流音
用的是Quick Start (Newbie) 快速开始 (新手友好版)教程代码分支是tag0.0.1 模型是配对的那个  也解决了 兼容性问题 还用过main分支 模型为中文完整教程里推荐的第一个配好后都不再有报错win10 nvcc -V 显示 cudatools 10.2nvidia-smi 显示 cuda version 11.6conda 4.9.2pytorch 1.10.1torch.cuda.is_available() 为 true用的录音都在5-8秒左右求助没有基础 显卡也差 所以没有自己训练没自己训练就是不像的 要用目标的声音训练 ，虽然我没有成功过 可以用新模型加新代码，推荐：https://github.com/babysor/MockingBird/issues/289> 可以用新模型加新代码，推荐：#289我看分享日期是去年，可以用吗 我用的是现在最新clone的main分支 
运行pip install -r requirements.txt出现错误
![K3HT2M$W HL0ZXLP 
大佬们能来个新的进群路径吗，微信，qq都可以，搞来搞去还是杂音
RT同求 二维码都过期了......同求老的交流群都满了，需要加好友才能进。辛苦网友们先在issue群沟通，为了让项目和AI持续可以给大家提供更多价值，共同学习，我在issue区根据不同主题创建长期交流频道，若留言人数超过20再考虑建立实时交流群，目前已初始化以下：如何改参数，搞出更逼真的克隆效果  & 合法合规）https://github.com/babysor/MockingBird/issues/439同求 
训练报错2，1
请问这是什么错误？root@dl-2308472413-pod-jupyter-6ff7f95f5c-xl76j:~# conda activate pytorch(pytorch) root@dl-2308472413-pod-jupyter-6ff7f95f5c-xl76j:~# cd ./MockingBird-main(pytorch) root@dl-2308472413-pod-jupyter-6ff7f95f5c-xl76j:~/MockingBird-main# python  ./synthesizer_train.py aitest ../wj12/SV2TTS/synthesizerTraceback (most recent call last):  File "/root/MockingBird-main/./synthesizer_train.py", line 2, in <module>    from synthesizer.train import train  File "/root/MockingBird-main/synthesizer/train.py", line 1, in <module>    import torchModuleNotFoundError: No module named 'torch'没安装pytorch 
encoder
Did you train your own encoder or you took the RealTime one? The pretrained encoder in this repo has been fine-tuned with Mandarin dataset based on the RealTime one. 
大概一个数据集多少次会过拟合？
我训练我老师的专用合成器，大概300句，结果练了不到1000下，就开始原地波动了，换数据集的话loss又从0.7开始，降了0.02又不动了，请问是因为过拟合了吗？该如何解决？确实快速过拟合了。增加更多数据集（有不同句子）> 确实快速过拟合了。增加更多数据集（有不同句子）请问作者，我修改过saved_every和log_every参数，与这有关吗> > 确实快速过拟合了。增加更多数据集（有不同句子）> > 请问作者，我修改过saved_every和log_every参数，与这有关吗没有关系> 没有关系谢谢 
web端只能在windows下面用吗？
 File "/home/wiseatc/zzc/hugface/lib/python3.7/site-packages/streamlit/scriptrunner/script_runner.py", line 443, in _run_script    exec(code, module.__dict__)  File "/tmp/tmpm_oj5169.py", line 14, in <module>    render_streamlit_ui()  File "/home/wiseatc/MockingBird/mkgui/base/ui/streamlit_ui.py", line 838, in render_streamlit_ui    opyrator = getOpyrator(mode)  File "/home/wiseatc/MockingBird/mkgui/base/ui/streamlit_ui.py", line 818, in getOpyrator    from mkgui.app import synthesize  File "/home/wiseatc/MockingBird/mkgui/app.py", line 1, in <module>    from asyncio.windows_events import NULL  File "/usr/local/lib/python3.7/asyncio/windows_events.py", line 3, in <module>    import _overlappedModuleNotFoundError: No module named '_overlapped'请问是什么系统？ubuntu16我的是Mac M1，好像我们的web.py只能在win下面使用> 我的是Mac M1，好像我们的web.py只能在win下面使用mac也是这个错误吗？确实这个没在多系统下测试验证过，我晚点试试我的是Mac M1 ,启动后报错 
vocoder pt
I'm training a HIFI vocoder for now and I  was wondering which is the difference between the **do_hifigan.pt** and the **g_hifigan.pt** !Also I noticed that the gound_truth argument in the vocoder_train.py script is not actually used in the train script, Am I wrong?Is do_hifigan your own trained model?@babysor Idk, i I get these two .pt when doing the training, the g_hifigan.pt is much smaller in size.Another question, I am at:**Steps : 29000, Gen Loss Total : 26.293, Mel-Spec. Error : 0.344, s/b : 1.412**is it normal that the voice is still robotic? How many steps should i normally do? Thank you@babysor Also I have tried your vocoder, the pretrained one and it is robotic too :( 29000 steps is far too early to have a good result. 
训练VOCODER出现新问题
install pywavelets> 试试pip install pywavelets 感谢大佬帮忙> > 试试pip install pywavelets> > 感谢大佬帮忙事实上，你把错误代码上bing一搜，基本上啥都有了> > > 试试pip install pywavelets> > > > > > 感谢大佬帮忙> > 事实上，你把错误代码上bing一搜，基本上啥都有了弱弱问一下BING是哪里> > > > 试试pip install pywavelets> > > > > > > > > 感谢大佬帮忙> > > > > > 事实上，你把错误代码上bing一搜，基本上啥都有了> > 弱弱问一下BING是哪里呃，就是必应，bing.com，微软的搜索引擎> bing.com感谢 
使用版本7f799d322f7599跑模型pretrained-11-7-21_75k.pt时遇到报错：raise ValueError("Axis limits cannot be NaN or Inf")
**Summary[问题简述（一句话）]**使用版本 跑模型pretrained-11-7-21_75k.pt时遇到报错： **Env & To Reproduce[复现与环境]**+ 环境    + mac电脑    + python3.9.12    + torch1.7.1+ commit log 版本: + 模型pretrained-11-7-21_75k.pt**Screenshots[截图（如有）]**<img width="1107" alt="截屏2022-05-26 上午8 07 05" src="https://user-images.githubusercontent.com/49824982/170388979-c04c54f6-64bf-4aa3-a7f9-675f17e1ca65.png">#571 解决了，谢谢！ 
请教训练vocoder报错问题
下载最新版代码试试> 下载最新版代码试试大佬，最新代码在哪里？> > 下载最新版代码试试> > > 下载最新版代码试试> > 大佬，最新代码在哪里？第一，我不是大佬，我只是小白第二，就是code里面的download as zip#577 可以参考一下> > > 下载最新版代码试试> > > > > > > 下载最新版代码试试> > > > > > 大佬，最新代码在哪里？> > 第一，我不是大佬，我只是小白 第二，就是code里面的download as zip #577 可以参考一下是下一次新的项目文件，还是不行呢Microsoft Windows [版本 10.0.19044.1706](c) Microsoft Corporation。保留所有权利。D:\shengyin\MockingBird-main>python vocoder_train.py aftest D:\shengyin\wj10 hifigan  File "D:\shengyin\MockingBird-main\vocoder_train.py", line 8    <!DOCTYPE html>    ^SyntaxError: invalid syntaxD:\shengyin\MockingBird-main>python vocoder_train.py aftest D:\shengyin\wj10 hifiganTraceback (most recent call last):  File "D:\shengyin\MockingBird-main\vocoder_train.py", line 4, in <module>    from vocoder.fregan.train import train as train_fregan  File "D:\shengyin\MockingBird-main\vocoder\fregan\train.py", line 15, in <module>    from vocoder.fregan.discriminator import ResWiseMultiPeriodDiscriminator, ResWiseMultiScaleDiscriminator  File "D:\shengyin\MockingBird-main\vocoder\fregan\discriminator.py", line 8, in <module>    from vocoder.fregan.dwt import DWT_1D  File "D:\shengyin\MockingBird-main\vocoder\fregan\dwt.py", line 10, in <module>    import pywtModuleNotFoundError: No module named 'pywt'D:\shengyin\MockingBird-main>> > > 下载最新版代码试试> > > > > > > 下载最新版代码试试> > > > > > 大佬，最新代码在哪里？> > 第一，我不是大佬，我只是小白 第二，就是code里面的download as zip #577 可以参考一下麻烦可以截图一下，下载位置吗？感谢> > > 下载最新版代码试试> > > > > > > 下载最新版代码试试> > > > > > 大佬，最新代码在哪里？> > 第一，我不是大佬，我只是小白 第二，就是code里面的download as zip #577 可以参考一下貌似代码还没有更新这个您试试再来一次pip install -r requirements.txt> pip install -r requirements.txt结果报错了ERROR: Could not build wheels for pyworld which use PEP 517 and cannot be installed directlyWARNING: You are using pip version 21.2.4; however, version 22.1.1 is available.You should consider upgrading via the 'C:\Users\Administrator\AppData\Local\Programs\Python\Python39\python.exe -m pip install --upgrade pip' command.那试试先C:\Users\Administrator\AppData\Local\Programs\Python\Python39\python.exe -m pip install --upgrade pip更新pip不应该报错> 那试试先C:\Users\Administrator\AppData\Local\Programs\Python\Python39\python.exe -m pip install --upgrade pip更新pip 不应该报错还是一样的报错呢，万分感谢。。还有没有其他的可能 
打开web端后，左侧显示模式选择，但是右侧报错，如下
ImportError: DLL load failed while importing shell: 找不到指定的程序。Traceback:File "D:\anaconda\envs\aivoice\lib\site-packages\streamlit\scriptrunner\script_runner.py", line 443, in _run_script    exec(code, module.__dict__)File "C:\Users\giant\AppData\Local\Temp\tmpp_sbx1jh.py", line 14, in <module>    render_streamlit_ui()File "D:\voice\MockingBird-main\mkgui\base\ui\streamlit_ui.py", line 838, in render_streamlit_ui    opyrator = getOpyrator(mode)File "D:\voice\MockingBird-main\mkgui\base\ui\streamlit_ui.py", line 818, in getOpyrator    from mkgui.app import synthesizeFile "D:\voice\MockingBird-main\mkgui\app.py", line 6, in <module>    from encoder import inference as encoderFile "D:\voice\MockingBird-main\encoder\inference.py", line 3, in <module>    from encoder.audio import preprocess_wav   # We want to expose this function from hereFile "D:\voice\MockingBird-main\encoder\audio.py", line 7, in <module>    import librosaFile "D:\anaconda\envs\aivoice\lib\site-packages\librosa\__init__.py", line 211, in <module>    from . import coreFile "D:\anaconda\envs\aivoice\lib\site-packages\librosa\core\__init__.py", line 5, in <module>    from .convert import *  # pylint: disable=wildcard-importFile "D:\anaconda\envs\aivoice\lib\site-packages\librosa\core\convert.py", line 7, in <module>    from . import notationFile "D:\anaconda\envs\aivoice\lib\site-packages\librosa\core\notation.py", line 8, in <module>    from ..util.exceptions import ParameterErrorFile "D:\anaconda\envs\aivoice\lib\site-packages\librosa\util\__init__.py", line 84, in <module>    from .files import *  # pylint: disable=wildcard-importFile "D:\anaconda\envs\aivoice\lib\site-packages\librosa\util\files.py", line 28, in <module>    __data_path = os.environ.get("LIBROSA_DATA_DIR", pooch.os_cache("librosa"))File "D:\anaconda\envs\aivoice\lib\site-packages\pooch\utils.py", line 99, in os_cache    return Path(appdirs.user_cache_dir(project))File "D:\anaconda\envs\aivoice\lib\site-packages\appdirs.py", line 293, in user_cache_dir    path = os.path.normpath(_get_win_folder("CSIDL_LOCAL_APPDATA"))File "D:\anaconda\envs\aivoice\lib\site-packages\appdirs.py", line 480, in _get_win_folder_with_pywin32    from win32com.shell import shellcon, shell请问有没有大佬知道如何解决问题已经解决了，pywin32版本过低导致的，安装了最新版后问题解决我也遇到了同样的问题，然而我是在使用pywin32最新版（304）遇到的。 故特此对本issue的解决方法进行补充。我是通过将pywin32降级到303版本解决了这个error， 以下是我的环境信息，仅供参考。os: win10 21h2 19044.1806python: 3.9.12absl-py                 1.1.0altair                  4.2.0aniso8601               9.0.1appdirs                 1.4.4argon2-cffi             21.3.0argon2-cffi-bindings    21.2.0asttokens               2.0.5attrs                   21.4.0audioread               2.1.9backcall                0.2.0beautifulsoup4          4.11.1bleach                  5.0.0blinker                 1.4brotlipy                0.7.0cachetools              5.2.0certifi                 2022.6.15cffi                    1.15.0charset-normalizer      2.0.4ci-sdr                  0.0.0click                   8.1.3colorama                0.4.5ConfigArgParse          1.5.3cryptography            37.0.1ctc-segmentation        1.7.1cycler                  0.11.0Cython                  0.29.30debugpy                 1.6.0decorator               5.1.1defusedxml              0.7.1dill                    0.3.5.1Distance                0.1.3einops                  0.4.1entrypoints             0.4espnet                  202205espnet-tts-frontend     0.0.3executing               0.8.3fast-bss-eval           0.1.3fastjsonschema          2.15.3filelock                3.7.1Flask                   2.1.2Flask-Cors              3.0.10flask-restx             0.5.1Flask-WTF               1.0.1fonttools               4.33.3g2p-en                  2.1.0gevent                  21.8.0gitdb                   4.0.9GitPython               3.1.27google-auth             2.8.0google-auth-oauthlib    0.4.6greenlet                1.1.2grpcio                  1.47.0h5py                    3.7.0humanfriendly           10.0idna                    3.3importlib-metadata      4.12.0inflect                 5.6.0ipykernel               6.15.0ipython                 8.4.0ipython-genutils        0.2.0ipywidgets              7.7.1itsdangerous            2.1.2jaconv                  0.3jamo                    0.4.1jedi                    0.18.1Jinja2                  3.1.2joblib                  1.1.0jsonpatch               1.32jsonpointer             2.3jsonschema              4.6.0jupyter-client          7.3.4jupyter-core            4.10.0jupyterlab-pygments     0.2.2jupyterlab-widgets      1.1.1kaldiio                 2.17.2kiwisolver              1.4.3librosa                 0.9.1llvmlite                0.38.1Markdown                3.3.7MarkupSafe              2.1.1matplotlib              3.5.2matplotlib-inline       0.1.3mistune                 0.8.4mkl-fft                 1.3.1mkl-random              1.2.2mkl-service             2.4.0multiprocess            0.70.13munkres                 1.1.4nbclient                0.6.4nbconvert               6.5.0nbformat                5.4.0nest-asyncio            1.5.5nltk                    3.7notebook                6.4.12numba                   0.55.2numpy                   1.19.3oauthlib                3.2.0packaging               21.3pandas                  1.4.3pandocfilters           1.5.0parso                   0.8.3pickleshare             0.7.5Pillow                  9.0.1pip                     21.2.4pooch                   1.6.0prometheus-client       0.14.1prompt-toolkit          3.0.29protobuf                3.19.4psutil                  5.9.1pure-eval               0.2.2pyarrow                 8.0.0pyasn1                  0.4.8pyasn1-modules          0.2.8pycparser               2.21pydeck                  0.7.1Pygments                2.12.0Pympler                 1.0.1pynndescent             0.5.7pyOpenSSL               22.0.0pyparsing               3.0.9pypinyin                0.44.0PyQt5                   5.15.7PyQt5-Qt5               5.15.2PyQt5-sip               12.11.0pyreadline3             3.4.1pyrsistent              0.18.1PySocks                 1.7.1python-dateutil         2.8.2pytorch-wpe             0.0.1pytz                    2022.1pytz-deprecation-shim   0.1.0.post0PyWavelets              1.3.0pywin32                 303pywinpty                2.0.5pyworld                 0.3.0PyYAML                  5.4.1pyzmq                   23.2.0regex                   2022.6.2requests                2.27.1requests-oauthlib       1.3.1resampy                 0.2.2rsa                     4.8scikit-learn            1.1.1scipy                   1.8.1semver                  2.13.0Send2Trash              1.8.0sentencepiece           0.1.96setuptools              61.2.0six                     1.16.0smmap                   5.0.0sounddevice             0.4.4SoundFile               0.10.3.post1soupsieve               2.3.2.post1stack-data              0.3.0streamlit               1.8.0tensorboard             2.9.1tensorboard-data-server 0.6.1tensorboard-plugin-wit  1.8.1terminado               0.15.0threadpoolctl           3.1.0tinycss2                1.1.1toml                    0.10.2toolz                   0.11.2torch                   1.11.0torch-complex           0.4.3torchaudio              0.11.0torchfile               0.1.0torchvision             0.12.0tornado                 6.1tqdm                    4.64.0traitlets               5.3.0typeguard               2.13.3typing_extensions       4.1.1tzdata                  2022.1tzlocal                 4.2umap-learn              0.5.3unicodedata2            14.0.0Unidecode               1.3.4urllib3                 1.26.9validators              0.20.0visdom                  0.1.8.9watchdog                2.1.9wcwidth                 0.2.5webencodings            0.5.1webrtcvad-wheels        2.0.10.post2websocket-client        1.3.3Werkzeug                2.1.2wheel                   0.37.1widgetsnbextension      3.6.1win-inet-pton           1.1.0wincertstore            0.2WTForms                 3.0.1zipp                    3.8.0zope.event              4.5.0zope.interface          5.4.0> 
stft_loss
I tried to use the FREGAN vocoder but I got this error!_**ModuleNotFoundError: No module named 'vocoder.fregan.stft_loss'**_And indeed there is not such script@flysmart Any idea?> @flysmart Any idea?I have submitted a new PR: Added missing files for Fre-GAN> I tried to use the FREGAN vocoder but I got this error!> > _**ModuleNotFoundError: No module named 'vocoder.fregan.stft_loss'**_> > And indeed there is not such scriptThank you very much for asking the question, please wait for the code update to resolve the issueFixed. 
loss一直迟迟下不去
c 退出再进行会好一点 
loss不降反升
我在wsl中训练，速度快了，但是loss却反升 windows中也反升 是因为我改了这几个参数吗 > 我在wsl中训练，速度快了，但是loss却反升 windows中也反升 是因为我改了这几个参数吗 > 大佬，LOSS迟迟不下降有什么好的方法吗大佬你是怎么解决的，升降反复的？> 大佬你是怎么解决的，升降反复的？是这样的：首先我不是大佬，我只是一个一心想要恶搞老师的初二无聊网课学生那我找到的方法是：从一开始就做大数据集您想想，aidatatang_200zh这个200小时的语音数据150k就过拟合了，我一200句的还不得……这只是我的猜想> > 大佬你是怎么解决的，升降反复的？> > 是这样的：> > 首先我不是大佬，我只是一个一心想要恶搞老师的初二无聊网课学生> > 那我找到的方法是：从一开始就做大数据集> > 您想想，aidatatang_200zh这个200小时的语音数据150k就过拟合了，我一200句的还不得……> > 这只是我的猜想那意思是不是说越小的数据集，越早拟合。之前作者说过，拟合只能换数据集。。我现在在换数据集继续训练，换了才开始，还不知道会不会突破之前的最低LOSS。我就是有个问题想不明白，为什么模型没有一个很好的效果，就拟合了？> 我就是有个问题想不明白，为什么模型没有一个很好的效果，就拟合了？我是这样想的：你一模一样的数据集放进去训练个几千遍，就一直只有这么一点，模型它也受不了啊，它也要一定的数据量来支持运算，一个数据集练很多遍，反而效果可能不是很好。> > 我就是有个问题想不明白，为什么模型没有一个很好的效果，就拟合了？> > 我是这样想的： 你一模一样的数据集放进去训练个几千遍，就一直只有这么一点，模型它也受不了啊，它也要一定的数据量来支持运算，一个数据集练很多遍，反而效果可能不是很好。不是吧，那我一样的数据集，每次调不同的BZ，loss下降的是不一样的呀> 不是吧，那我一样的数据集，每次调不同的BZ，loss下降的是不一样的呀qs，batch_size是非常重要的 
web端打开后左侧显示AI拟音，右侧有如下报错
ModuleNotFoundError: No module named '_overlapped'Traceback:File "/Users/christopher/opt/anaconda3/lib/python3.7/site-packages/streamlit/scriptrunner/script_runner.py", line 475, in _run_script    exec(code, module.__dict__)File "/var/folders/5t/fft4mlms1zgc8_0x30qywcmc0000gn/T/tmp85elwg7n.py", line 14, in <module>    render_streamlit_ui()File "/Users/christopher/PycharmProjects/shenlan/AI_art/AI模仿声音/MockingBird2/mkgui/base/ui/streamlit_ui.py", line 838, in render_streamlit_ui    opyrator = getOpyrator(mode)File "/Users/christopher/PycharmProjects/shenlan/AI_art/AI模仿声音/MockingBird2/mkgui/base/ui/streamlit_ui.py", line 818, in getOpyrator    from mkgui.app import synthesizeFile "/Users/christopher/PycharmProjects/shenlan/AI_art/AI模仿声音/MockingBird2/mkgui/app.py", line 1, in <module>    from asyncio.windows_events import NULLFile "/Users/christopher/opt/anaconda3/lib/python3.7/asyncio/windows_events.py", line 3, in <module>    import _overlapped 
最新版本无法训练synth
uninstall -r requirements.txt再pip install -r requirements.txt 
vocoder comparison
Is there a comparison between vocoders?People usually downloaded different pretrained and compared. But I prefer you read papers regarding their difference and apply the right one according to your requirement.OK, because I trained the models from the original repository, but I want to try to train a new vocoder, do you know which files I should modify?> OK, because I trained the models from the original repository, but I want to try to train a new vocoder, do you know which files I should modify?The folder of vocoder should be the place to go. Are you trying to do this from the original repository or this one w/ well-constructed new vocoders? @babysor with this one! But I got en error that I reported in a new ISSUE about a module in fre gan. 
ValueError: Axis limits cannot be NaN or Inf
 File "/Users/christopher/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py", line 3439, in _validate_converted_limits    raise ValueError("Axis limits cannot be NaN or Inf")ValueError: Axis limits cannot be NaN or Inf窗口太小，用鼠标拖大就好 
有无办法分批次训练合声器模型？
就是说退出的时候保存进度直接Ctrl+c的话好像前功尽弃 
训练合声器直接蓝屏了……
配置：AMD RYZEN 4600-5（6核12线程） 16G并行数：86的话就还好，10的话CPU飙上90度不支持CUDA请问最佳的并行数是多少……这指的是预处理把 
Feat:Convert digit in the text into chinese, including number, date a…
## 文本中数字转换中文代码实现参考来源- 将text_to_chinese移动到synthesizer/inference.py里更适合这里可以再提交一下修改，我合并进来吗？不然广大网友就享受不到你的优化了 
最后一步时显示“不是有效的 Win32 应用程序”，大佬们求救！！
 最后一步时出现这种情况，大佬们求解决！Install last step shows "Not a valid Win32 application", please help! That‘s the code.me too 
自己训练的模型使用不了
我想自己训练个模型，随便找了20句话左右扔进去训练。loss已经是0.2左右了，但是用的时候并不能生成输入的文本语音，而是随机生成一段训练集里面的音频。这是为什么呢？是因为我的训练集太小了不能收敛吗，我是小白，不是很懂。> 我想自己训练个模型，随便找了20句话左右扔进去训练。loss已经是0.2左右了，但是用的时候并不能生成输入的文本语音，而是随机生成一段训练集里面的音频。这是为什么呢？ 是因为我的训练集太小了不能收敛吗，我是小白，不是很懂。训练克隆特定人声音&finetune 
centos kaldiio>=2.17.0 安装失败
kaldiio 的issue区问。。 
在kaggle上训练模型，报这个错
Traceback (most recent call last):  File "../input/notebook/MockingBird-main/synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "/kaggle/input/notebook/MockingBird-main/synthesizer/train.py", line 212, in train    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), hparams.tts_clip_grad_norm)  File "/opt/conda/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py", line 43, in clip_grad_norm_    if total_norm.isnan() or total_norm.isinf():RuntimeError: CUDA error: an illegal memory access was encounteredCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.For debugging consider passing CUDA_LAUNCH_BLOCKING=1. 
启动后第一次点击 synthesize and vocode 报错
执行python demo_toolbox.py -d .\samples 启动程序后，第一次点击synthesize and vocode按钮报错，之后就不报错了，不知道什么原因？RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]).        size mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([384, 1280]).        size mismatch for decoder.rnn_input.weight: copying a param with shape torch.Size([1024, 640]) from checkpoint, the shape in current model is torch.Size([1024, 1152]).        size mismatch for decoder.stop_proj.weight: copying a param with shape torch.Size([1, 1536]) from checkpoint, the shape in current model is torch.Size([1, 2048]).搜索一下issue，报错是模型和代码没对上 
请问各位大佬，我打开工具箱后，怎么上传不了自己的音频文件啊，音频文件也改成了wav格式
求教，谢谢各位！> 求教，謝謝各位！是WAV> 谢谢，我问题的题目打错了，我确实改成的WAV格式，也上传不了> > > > 谢谢，我问题的题目打错了，我确实改成的WAV格式，也上传不了有沒有圖片或錯誤訊息不然這樣很難懂 
运行pip install -r requirements.txt 来安装剩余的必要包时出错
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.运行pip install -r requirements.txt 来安装剩余的必要包时出错**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型、**Screenshots[截图（如有）]**If applicable, add screenshots to encounter this problem tooPyworld requires to install C++ compiler, please make sure yours is correctly installed.最后运行 python compiler,请问这个问题现在有解决方案了吗？ @worlol 应该在anaconda的虚拟环境下安装剩余安装包；按照教程一步一步来没有问题的------------------&nbsp;原始邮件&nbsp;------------------发件人:                                                                                                                        "babysor/MockingBird"                                                                                    ***@***.***&gt;;发送时间:&nbsp;2022年7月14日(星期四) 下午2:05***@***.***&gt;;***@***.******@***.***&gt;;主题:&nbsp;Re: [babysor/MockingBird] 运行pip install -r requirements.txt 来安装剩余的必要包时出错 (Issue #561) 请问这个问题现在有解决方案了吗？ @worlol —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you commented.Message ID: ***@***.***&gt; 
generate audio file from python script instead of gui
通过指定需要生成语音的text文本文件，需要模仿的语音文件，自动分多个文件产生语音文件。python gen_voice.py <text_file.txt> your_wav_file.wav> 是否可以在readme增加一两句教程呢，这样会有更多用户使用，给您反馈~感激！all updated, please help to do review again, thanks. 
本地运行脚本
有没有哪位大佬写过不用toolbox的本地脚本，有的话可以拿出来分享一下吗，感觉理不清楚整体的思路！！https://github.com/babysor/MockingBird/pull/560 看这个 
Signal passing ERROR 
reports and ERROR: Pass y=[-0.0043335  -0.04104614 -0.10864258 ...  0.12042236  0.09857178  0.03143311] as keyword args. From version 0.10 passing these as positional arguments will result in an errorCan someone help me solve thisThis is just a future warning that talks us to upgrade dependency but everything still works. I wish you can help to get rid of this warning by passing the right order of parameters> This is just a future warning that talks us to upgrade dependency but everything still works. I wish you can help to get rid of this warning by passing the right order of parametershttps://github.com/babysor/MockingBird/pull/498 it's fixed by someone, you can pull and verify it. 
分享日语95k训练模型
链接：https://pan.baidu.com/s/1is8yodQ5QmgsIyLogKF24w?pwd=5yf9 
Suggestions to improve core networking and synthesis.
Take a look at how is using the new technique, instead of letter by letter/word learning adding some help to ‘dictionary lookup table is used to dissect each English word into its respective phonemes’ something similar.Also suggest adding ‘emotion of the input text is deduced using DeepMoji’what do you think? 
pre.py 改进建议
**Summary[问题简述（一句话）]**使用  时如何暂停以及开始**Env & To Reproduce[复现与环境]**各依赖环境正常使用  开始训练后，无法停止按  后报错，但计算未停止看了一下  源码，应该是  的问题，其进程实例需人为停止可以考虑使用 ：  > **Summary[问题简述（一句话）]** 使用  时如何暂停以及开始> > **Env & To Reproduce[复现与环境]** 各依赖环境正常> > 使用  开始训练后，无法停止 按  后报错，但计算未停止 看了一下  源码，应该是  的问题，其进程实例需人为停止 可以考虑使用 ： 建议很好呀，有 multiprocessing  这个lib的相关example吗？ 
为什么缺失.h文件呢，缺了两个，不知道怎么回事，求教
**Summary[问题简述（一句话）]**为什么缺失.h文件呢，缺了两个，不知道怎么回事，求教**Env & To Reproduce[复现与环境]**py3.9 PyTorch11.0 cuda 11.6 不涉及模型**Screenshots[截图（如有）]**运行pip install -r requirements.txt的报错截图![屏幕截图 2022-05-15 2022-05-15 install espnet的报错截图![屏幕截图 2022-05-15 2022-05-15 这个依赖一直安装失败,可能有依赖冲突，可以google下看看解决方法，你试着安装一个python 3.8的虚拟环境试试? 
新的Web报错
但是仍然存在在工具箱中很好用的模型在web中一塌糊涂的问题这里你试了一样的模型吗？不合理呀，我web部分的代码比较简单，就在 mkgui/app.py 里面，也做了预处理> 已解决，如下修改app.py已修复到main> > 但是仍然存在在工具箱中很好用的模型在web中一塌糊涂的问题> > 这里你试了一样的模型吗？不合理呀，我web部分的代码比较简单，就在 mkgui/app.py 里面，也做了预处理是的，一模一样的模型，一模一样的输入，但输出差很多toolbox目前传多了一些参数，我回头复现一下> toolbox目前传多了一些参数，我回头复现一下好的谢谢大佬 
有没有提供一个便于使用的函数用于合成？
自己想做一些魔改，但代码没有看懂参考根目录下mkgui的部分把On Sat, May 14, 2022 at 10:38 AM 王梓毅 ***@***.***> wrote:> 自己想做一些魔改，但代码没有看懂>> —> Reply to this email directly, view it on GitHub> <https://github.com/babysor/MockingBird/issues/551>, or unsubscribe> <https://github.com/notifications/unsubscribe-auth/ABYUKEBG63HXZKEG747X5CDVJ4G2JANCNFSM5V45XLWQ>> .> You are receiving this because you are subscribed to this thread.Message> ID: ***@***.***>>-- Cordially,Weijia Chen   - www.linkedin.com/pub/weijia-chen/84/a7a/463/ 
训练synthesizer时，如何获得已经收敛的模型？
converge，后面基本都是ok的，不用回到前面> > 这里一旦attention converge，后面基本都是ok的，不用回到前面意思是到这里关于文本转语音的部分已经训练完成了吗，接下来就是关于特定人的fine tuning和训练声码器来处理电流声吗。但是感觉输入的文本还是有部分模糊不清、不知所言的情况。对的，但是可以再训练一段时间，loss和效果比较满意的时候。 
点击 record 之后 会报  Axis limits canot be  NaN or Inf
<img width="377" alt="捕获" src="https://user-images.githubusercontent.com/12454131/167635402-92c187f6-741f-4096-9e95-6d27b7e52f6e.PNG">Building hifiganLoading 'vocoder\saved_models\pretrained\g_hifigan.pt'Complete.Removing weight norm...Traceback (most recent call last):  File "D:\AI\MockingBird-0.0.1\toolbox\__init__.py", line 123, in <lambda>    func = lambda: self.synthesize() or self.vocode()  File "D:\AI\MockingBird-0.0.1\toolbox\__init__.py", line 331, in vocode    self.ui.draw_embed(embed, name, "generated")  File "D:\AI\MockingBird-0.0.1\toolbox\ui.py", line 70, in draw_embed    embed_ax.figure.canvas.draw()  File "C:\Python39\lib\site-packages\matplotlib\backends\backend_agg.py", line 436, in draw    self.figure.draw(self.renderer)  File "C:\Python39\lib\site-packages\matplotlib\artist.py", line 73, in draw_wrapper    result = draw(artist, renderer, *args, **kwargs)  File "C:\Python39\lib\site-packages\matplotlib\artist.py", line 50, in draw_wrapper    return draw(artist, renderer)  File "C:\Python39\lib\site-packages\matplotlib\figure.py", line 2823, in draw    artists = self._get_draw_artists(renderer)  File "C:\Python39\lib\site-packages\matplotlib\figure.py", line 238, in _get_draw_artists    ax.apply_aspect()  File "C:\Python39\lib\site-packages\matplotlib\axes\_base.py", line 1972, in apply_aspect    self.set_xbound(x_trf.inverted().transform([x0, x1]))  File "C:\Python39\lib\site-packages\matplotlib\axes\_base.py", line 3573, in set_xbound    self.set_xlim(sorted((lower, upper),  File "C:\Python39\lib\site-packages\matplotlib\axes\_base.py", line 3697, in set_xlim    left = self._validate_converted_limits(left, self.convert_xunits)  File "C:\Python39\lib\site-packages\matplotlib\axes\_base.py", line 3614, in _validate_converted_limits    raise ValueError("Axis limits cannot be NaN or Inf")ValueError: Axis limits cannot be NaN or Inf窗口太小，用鼠标拉大一点同问这个问题怎么解决？用鼠标拉大一下窗口就可以了 
音频识别出来的文字允许带有句号和逗号吗，还是说需要删掉。
用了一个新的识别软件这是什么意思 
作者大大可不可以发个微信群二维码？
@babysor 已加群群在哪 
训练vocoder会自动停止
训练vocoder，在3000 训练vocoder，在3000 epoch以后每次训练几个就会自动停止，没有任何报错信息  训练size60  hifi-gan   epoch:3100就自动停了重新键入训练命令也不是继续训练而是重新训练。。。。浪费了我半天时间。。。。。<img width="359" alt="image" src="https://user-images.githubusercontent.com/108389438/187048880-a885ca45-9300-42fa-b008-c1b7afde71ac.png">> <img alt="image" width="359" src="https://user-images.githubusercontent.com/108389438/187048880-a885ca45-9300-42fa-b008-c1b7afde71ac.png">因为没有还没到保存点，这个太奇怪了，你不会是楼主把？用的什么训练集？ 
关于训练某个特定声音遇到的问题
基于社区版继续训练某个特定声音遇到的问题本人基于社区版ceshi.pt，加入一些某个人的音频文件，以aidatatang_200zh数据集格式进行训练，到244k的时候测试发现如下问题1.断句问题。比如“服务器”，会感觉服务跟器之间有明显停顿2.多音字问题。比如“重装”，“重”会读成四声3.语速快问题。用新模型进行合成之后发现，录出的语音比较快。以上三个问题，不知道能否通过预处理或者修改代码进行优化。恳请路过的各位大神不吝赐教！ 
关于性能

训练vocoder的hifigan时出错
训练vocoder的hifigan时出错，好像是显存爆了，这咋解决？我看readme中说改batch_size，但是改的好像是wavernn的batch_size，我改了wavernn下的 hparams.py，把batch_size改成6，一样提示爆了，这咋解决？ 报错图如下 
如何使声码器最终生成的音频采样率为44100，最高频率能达到20khz？
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.目前声码器生成的音频采样频率为16000，最高频率为8k，如何改变这些参数？（个人尝试将config文件中的sample_rate改为44100训练声码器模型，结果没有变化）**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型任何**Screenshots[截图（如有）]**If applicable, add screenshots to help 
有无大佬看看这是啥问题
**Summary[问题简述（一句话）]**我在实验室的机器上训练，使用的是我自己收集的，并且用aishell3数据集格式标注的数据，总是报这个错误（见截图）**Env & To Reproduce[复现与环境]**OS: CentOS 7Python: Anaconda+python in the original repo RTVC and search ValueError in issues I saw the same problem being answered before.Send your full notebook for better debug if you wish.> Send your full notebook for better debug if you wish.Many thanks if that's convenient for you. By the way, could you please give me the link of the 'original repo of RTVC' please?https://github.com/CorentinJ/Real-Time-Voice-CloningThanks @zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz-z but which issue exactly did you refer to? I found many similar issues... 
不认识的错误，有谁教教吗
Loaded encoder "pretrained.pt" trained to step 1594501Synthesizer using device: cudaTrainable Parameters: 32.869MTraceback (most recent call last):  File "C:\Users\abc\Documents\yvyinhechengqi\MockingBird-main\toolbox\__init__.py", line 259, in synthesize    specs = self.synthesizer.synthesize_spectrograms(texts, embeds, style_idx=int(self.ui.style_slider.value()), min_stop_token=min_token, steps=int(self.ui.length_slider.value())*200)  File "C:\Users\abc\Documents\yvyinhechengqi\MockingBird-main\synthesizer\inference.py", line 93, in synthesize_spectrograms    self.load()  File "C:\Users\abc\Documents\yvyinhechengqi\MockingBird-main\synthesizer\inference.py", line 71, in load    self._model.load(self.model_fpath, self.device)  File "C:\Users\abc\Documents\yvyinhechengqi\MockingBird-main\synthesizer\models\tacotron.py", line 547, in load    self.load_state_dict(checkpoint["model_state"], strict=False)  File "C:\Users\abc\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\modules\module.py", line 1497, in load_state_dict    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).        size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]).        size mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([384, 1280]).        size mismatch for decoder.rnn_input.weight: copying a param with shape torch.Size([1024, 640]) from checkpoint, the shape in current model is torch.Size([1024, 1152]).        size mismatch for decoder.stop_proj.weight: copying a param with shape torch.Size([1, 1536]) from checkpoint, the shape in current model is torch.Size([1, 2048]).就是这样，不能正常运行，会出现杂音而且输出的音频长度和我输入的字段长度无关![屏幕截图 2022-05-05 换个模型，这个模型有兼容性问题谢谢解决了，请问有训练好的hifigan_vocoder吗，我不知道怎么训练wavernn_vocoder,按照作者写的直接运行就提示没有指定vocoder的类型 
请教作者大佬，训练模型问题
不是的，你这个已经过拟合，到一定阶段，模型的适用能力下降，建议回到300k左右用其他数据集练或者优先练下vocoder大佬，拟合是看哪个值？steps吗？我看看退到哪里> 不是的，你这个已经过拟合，到一定阶段，模型的适用能力下降，建议回到300k左右用其他数据集练或者优先练下vocoder还是LOSS到不在下降，在一个值来回就拟合了？> > 不是的，你这个已经过拟合，到一定阶段，模型的适用能力下降，建议回到300k左右用其他数据集练或者优先练下vocoder> > 还是LOSS到不在下降，在一个值来回就拟合了？不是的，这里的过拟合不是个精确状态，在基本不下降的时候就有一定过拟合了，AI训练就像炼丹，要在过拟合和逼真度之间找个平衡点。如果你有超级计算机+大量数据的话，这个建议就可以忽略Make sure you have a ‘good dataset’ what defines as, there are guides. Mainly can be caused by hp-HM and the set having even the tiniest mistakes can ruin everything. Try configuring each and every variation finding the best configuration for your dataset profile. 
请问在运行python demo_toolbox.py时出现无法解码的问题
@babysor 
训练encoder和vocoder都遇到了报错
训练encoder遇到的好像是gbk和unicode编码方面的问题？UnicodeEncodeError: 'gbk' codec can't encode character '\ufeff' in position 4: illegal multibyte error: the following arguments are required: wavernn 
训练vocoder报错，请教
size> 显卡内存不够，建议修改配置中的batch size大佬，我从头训练VOCODER不会报错，停止在继续就会出现这种情况想请问训练VOCODER，怎么修改batch size？是不是和训练模型修改一样？我已经改了hparams.py 到6还是不行，改错地方了吧> 显卡内存不够，建议修改配置中的batch > 显卡内存不够，建议修改配置中的batch size> > 大佬，我从头训练VOCODER不会报错，停止在继续就会出现这种情况 想请问训练VOCODER，怎么修改batch size？是不是和训练模型修改一样？ 我已经改了hparams.py 到6还是不行，改错地方了吧因为batchsize有2个地方可以设置，优先级分别如下：1. 模型所在文件夹里的config文件2. 代码里面的hparam> > > 显卡内存不够，建议修改配置中的batch size> > > > > > 大佬，我从头训练VOCODER不会报错，停止在继续就会出现这种情况 想请问训练VOCODER，怎么修改batch size？是不是和训练模型修改一样？ 我已经改了 hparams.py 到6还是不行，改错地方了吧> > 因为batchsize有2个地方可以设置，优先级分别如下：> > 1. 模型所在文件夹里的config文件> 2. 
训练VOCODER问题
从0开始训练的话，vocoder用3080ti要训练几天几夜才有初步效果，你这个跑了多久？有2天左右吧，到1300了，出错我就重来了> > 从0开始训练的话，vocoder用3080ti要训练几天几夜才有初步效果，你这个跑了多久？> > 有2天左右吧，到1300了，出错我就重来了出错重来还得了，连续跑了多久？ 
語音轉換Voice Conversion(ppg2mel.yaml)修改地址問題
**Summary[问题简述（一句话）]**您好在訓練合成器步驟"注意在上一步先下載好ppg2mel.yaml, 修改裡面的地址指向預訓練好的文件夾： python ppg2mel_train.py --config .\ppg2mel\saved_models\ppg2mel.yaml --oneshotvc"不知道該怎麼修改地址.自己試了好久都失敗.**Screenshots[截图（如有）]**If applicable, add screenshots to 你要使用 D:\xxx\xx 或者 相对路径謝謝.我也試了.好像不行主要是.文件中應該修改第幾行?另外命令是這樣 python ppg2mel_train.py還是 python ppg2mel_train.py --config .\ppg2mel\saved_models\ppg2mel.yaml Wed, May 4, 2022 at 6:11 PM siao888 ***@***.***> wrote:> 謝謝.> 我也試了.好像不行>> 主要是.文件中應該修改第幾行?> 另外命令是> 這樣 python ppg2mel_train.py> 還是 python ppg2mel_train.py --config .\ppg2mel\saved_models\ppg2mel.yaml> --oneshotvc> 抱歉.很白癡的問題> [image: 006]> <https://user-images.githubusercontent.com/46085080/166662631-17ae3625-7890-4a73-a897-3bd419f08ff2.png>>> —> Reply to this email directly, view it on GitHub> <https://github.com/babysor/MockingBird/issues/530#issuecomment-1117142755>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/ABYUKEDO2LFIJPA67GRHSCDVIJENLANCNFSM5VBBLJKA>> .> You are receiving this because you commented.Message ID:> ***@***.***>>-- Cordially,Weijia Chen   - 好了.大致上搞定了. 現在應該是正常訓練了. 感恩. 能给我一个改好的样本截图吗？我也是改了好久没弄好。谢谢！> 谢谢您&nbsp; 我试一下&nbsp; &nbsp;------------------&nbsp;原始邮件&nbsp;------------------发件人:                                                                                                                        "babysor/MockingBird"                                                                                    ***@***.***&gt;;发送时间:&nbsp;2022年7月5日(星期二) 晚上9:59***@***.***&gt;;抄送:&nbsp;"@***@***.******@***.***&gt;;主题:&nbsp;Re: [babysor/MockingBird] 語音轉換Voice Conversion(ppg2mel.yaml)修改地址問題 (Issue #530)  @siao888謝謝我也是一個改好的樣本嗎？我改了好久沒弄好。！  不好意思.一陣子沒用了.那個文件被我刪除了. 我剛剛找到了一個.不知道對不對. 您試試看  —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you commented.Message ID: ***@***.***&gt; 
开启图形化界面时报错
D:\MockingBird-main>python demo_toolbox.pyTraceback (most recent call last):  File "demo_toolbox.py", line 2, in <module>    from toolbox import Toolbox  File "D:\MockingBird-main\toolbox\__init__.py", line 1, in <module>    from toolbox.ui import UI  File "D:\MockingBird-main\toolbox\ui.py", line 1, in <module>    from PyQt5.QtCore import Qt, QStringListModelModuleNotFoundError: No module named 'PyQt5'现在的图形化界面正在废弃中，你可以先后运行  , 进行更新代码后运行 Install PyQt5 using either pip or apt search on the web your self. 
大佬声码器预处理报错
路径拼错单词谢谢大佬了，一直在这里跟你学习。。感谢感谢 
训练时报错 PytorchStreamReader failed reading zip archive: failed finding central directory
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.训练时报错 PytorchStreamReader failed reading zip archive: failed finding central directory**Env & To applicable, add screenshots to 
在工具箱可以使用的模型在web端全是杂音
如题，救命现在的图形化界面正在废弃中，你可以先后运行  , 进行更新代码后运行 
python3 运行demo_toolbox.py的时候提示 Error: Model files not found. Please download the models
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型**Screenshots[截图（如有）]**If applicable, add screenshots to help<img width="571" alt="image" src="https://user-images.githubusercontent.com/17965372/165520089-6559daf8-eb9f-42d4-86e0-91745516f116.png"><img width="564" alt="image" src="https://user-images.githubusercontent.com/17965372/165520273-45558385-928d-4ad7-ac44-050a203c8779.png">缺少了这些模块，可以通过brew安装吗，还是要用pip安装，希望一个大佬来讲解下没有模型文件，要按readme下载> 谢啦，我去看看 
fixed typo
修复了一些笔误saved_mode -> saved_modelspython demo_toolbox.py *vc* ---> python demo_toolbox.py *-vc* 
请问一下ppg提取的预训练模型是重新训练的嘛
你好，我想问一下ppg提取的预训练模型作者是在librispeech上训练的，并不是纯中文的，如果去做中文的迁移是需要重新训练嘛，谢谢> 你好，我想问一下ppg提取的预训练模型作者是在librispeech上训练的，并不是纯中文的，如果去做中文的迁移是需要重新训练嘛，谢谢不重新训练也可以用的，但是效果一般般作者你好，请问在哪可以下在ppg2mel.yaml？> > 你好，我想问一下ppg提取的预训练模型作者是在librispeech上训练的，并不是纯中文的，如果去做中文的迁移是需要重新训练嘛，谢谢> > 不重新训练也可以用的，但是效果一般般> 作者你好，请问在哪可以下在ppg2mel.yaml？> > > > 你好，我想问一下ppg提取的预训练模型作者是在librispeech上训练的，并不是纯中文的，如果去做中文的迁移是需要重新训练嘛，谢谢> > > > > > 不重新训练也可以用的，但是效果一般般下载链接里面的模型所在同一个文件夹 
使用hifigan_24k报错 KeyError: 'model_state'
**Summary[问题简述（一句话）]**Vocoder使用hifigan_24k报错 KeyError: 'model_state'，输出为杂音。使用pretrained或g_hifigan则正常。**Env & To Reproduce[复现与环境]**模型从readme的百度网盘下载版本为最新的main分支。从百度网盘下载模型放入对应目录后，vocoder选择hifigan_24k的时候就会报错。但是点击生成仍然可以正常运行，只是出来的都是杂音。**Screenshots[截图（如有）]** 这个文件应该还伴随一个config文件，要一起下载config文件是有的：<img width="668" alt="image" src="https://user-images.githubusercontent.com/20398519/166140539-a185c0c9-3f81-4642-96b8-db89b71778f4.png">我这边可以复现，英文的话会比较好，中文基本上就是杂音，看起来是convertor的问题，需要重新训练。因为合成模型默认16k，输出的mel特征是16k 
全是杂音，已经更换了好几个预训练模型，求大佬指导
群号在哪里呀> 可以在issue里加交流群求助，这里看起来还是模型问题，建议学习一下git，不要用最老的代码或模型了求群号我也想知道交流群是哪个~~looking for vx group 
n_fft参数设置
Synthesizer和hifigan中的n_fft参数分别是800和1024，这样设置同一个波形文件处理的到的mels应该是不一样的，hifigan为什么会work呢 
纯小白请教，LOSS到034
继续作者75K训练到229LOSS到034就下不去了有什么办法吗，现在模型效果还不是很好问题在哪，应该怎么继续训练？> 继续作者75K训练到229 LOSS到034就下不去了 有什么办法吗，现在模型效果还不是很好 
请教作者大佬训练报错
报错如下这什么情况，怎么处理感谢十分我是听群里大佬说batch 报错如下 这什么情况，怎么处理 感谢十分 我是听群里大佬说batch size调大可以更好的降LOSS 我调整到32不行16可以 size主要提高训练效率，而不会直接降低loss 
encoder和合成器的训练是否有前后顺序
encoder和合成器的训练是否有前后顺序对最后的训练结果是否有影响都有影响，这个问题很难回答，其实好的encoder会给合成器的训练带来好处，但是对于刚开始接触项目的朋友来说，合成器训练的收益会比较明显， 
我的synthesizer模型调用不了，求解，菜鸟

请教作者及各位大佬，模型声音机械怎么办
纯小白，请教 1继续作者75K训练到200K，loss02左右，声音有点奇怪，最主要是效果机械，这是什么情况。要怎么办 2是训练还不到位吗？要继续训练还是咋办 3软件顶端的区域，怎么加载。就是下图这个，灰色 纯小白，请教 1继续作者75K训练到200K，loss02左右，声音有点奇怪，最主要是效果机械，这是什么情况。要怎么办 2是训练还不到位吗？要继续训练还是咋办 3软件顶端的区域，怎么加载。就是下图这个，灰色 demo_toolbox.py -d D:\shengyin\wj3还是灰色，不能加载什么情况。-D 是指哪里-d 应该指向数据集的上级。机械音很多时候也是因为vocoder不行，尝试训练 
sndfile 安装不成功
运行python web.py的时候报错 OSError: sndfile library not found使用pip install sndfile 安装相应包，报错日志如下Looking in indexes: sndfile  Downloading (4.3 kB)Requirement already satisfied: cffi>=1.0.0 in /home/hunan/miniconda3/envs/mocking/lib/python3.7/site-packages (from sndfile) (1.15.0)Requirement already satisfied: pycparser in /home/hunan/miniconda3/envs/mocking/lib/python3.7/site-packages (from cffi>=1.0.0->sndfile) (2.21)Building wheels for collected packages: sndfile  Building wheel for sndfile (setup.py) ... error  ERROR: Command errored out with exit status 1:   command: /home/hunan/miniconda3/envs/mocking/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '"'"'/tmp/pip-install-8ywejwpr/sndfile_b713bf1471364fbaa57702a47f955b06/setup.py'"'"'; __file__='"'"'/tmp/pip-install-8ywejwpr/sndfile_b713bf1471364fbaa57702a47f955b06/setup.py'"'"';f = getattr(tokenize, '"'"'open'"'"', open)(__file__) if os.path.exists(__file__) else io.StringIO('"'"'from setuptools import setup; setup()'"'"');code = f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' bdist_wheel -d /tmp/pip-wheel-kv129kjw       cwd: /tmp/pip-install-8ywejwpr/sndfile_b713bf1471364fbaa57702a47f955b06/  Complete output (23 lines):  running bdist_wheel  running build  running build_py  creating build  creating build/lib.linux-x86_64-3.7  creating build/lib.linux-x86_64-3.7/sndfile  copying sndfile/vio.py -> build/lib.linux-x86_64-3.7/sndfile  copying sndfile/formats.py -> build/lib.linux-x86_64-3.7/sndfile  copying sndfile/__init__.py -> build/lib.linux-x86_64-3.7/sndfile  copying sndfile/io.py -> build/lib.linux-x86_64-3.7/sndfile  copying sndfile/build.py -> build/lib.linux-x86_64-3.7/sndfile  running build_ext  generating cffi module 'build/temp.linux-x86_64-3.7/sndfile._sndfile.c'  creating build/temp.linux-x86_64-3.7  building 'sndfile._sndfile' extension  creating build/temp.linux-x86_64-3.7/build  creating build/temp.linux-x86_64-3.7/build/temp.linux-x86_64-3.7  gcc -pthread -B /home/hunan/miniconda3/envs/mocking/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/hunan/miniconda3/envs/mocking/include/python3.7m -c build/temp.linux-x86_64-3.7/sndfile._sndfile.c -o build/temp.linux-x86_64-3.7/build/temp.linux-x86_64-3.7/sndfile._sndfile.o  build/temp.linux-x86_64-3.7/sndfile._sndfile.c:571:10: fatal error: sndfile.h: No such file or directory   #include <sndfile.h>            ^~~~~~~~~~~  compilation terminated.  error: command '/usr/bin/gcc' failed with exit code 1  ----------------------------------------  ERROR: Failed building wheel for sndfile  Running setup.py clean for sndfileFailed to build sndfileInstalling collected packages: sndfile    Running setup.py install for sndfile ... error    ERROR: Command errored out with exit status 1:     command: /home/hunan/miniconda3/envs/mocking/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '"'"'/tmp/pip-install-8ywejwpr/sndfile_b713bf1471364fbaa57702a47f955b06/setup.py'"'"'; __file__='"'"'/tmp/pip-install-8ywejwpr/sndfile_b713bf1471364fbaa57702a47f955b06/setup.py'"'"';f = getattr(tokenize, '"'"'open'"'"', open)(__file__) if os.path.exists(__file__) else io.StringIO('"'"'from setuptools import setup; setup()'"'"');code = f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' install --record /tmp/pip-record-ezhwkiaq/install-record.txt --single-version-externally-managed --compile --install-headers /home/hunan/miniconda3/envs/mocking/include/python3.7m/sndfile         cwd: /tmp/pip-install-8ywejwpr/sndfile_b713bf1471364fbaa57702a47f955b06/    Complete output (25 lines):    running install    /home/hunan/miniconda3/envs/mocking/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.      setuptools.SetuptoolsDeprecationWarning,    running build    running build_py    creating build    creating build/lib.linux-x86_64-3.7    creating build/lib.linux-x86_64-3.7/sndfile    copying sndfile/vio.py -> build/lib.linux-x86_64-3.7/sndfile    copying sndfile/formats.py -> build/lib.linux-x86_64-3.7/sndfile    copying sndfile/__init__.py -> build/lib.linux-x86_64-3.7/sndfile    copying sndfile/io.py -> build/lib.linux-x86_64-3.7/sndfile    copying sndfile/build.py -> build/lib.linux-x86_64-3.7/sndfile    running build_ext    generating cffi module 'build/temp.linux-x86_64-3.7/sndfile._sndfile.c'    creating build/temp.linux-x86_64-3.7    building 'sndfile._sndfile' extension    creating build/temp.linux-x86_64-3.7/build    creating build/temp.linux-x86_64-3.7/build/temp.linux-x86_64-3.7    gcc -pthread -B /home/hunan/miniconda3/envs/mocking/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/hunan/miniconda3/envs/mocking/include/python3.7m -c build/temp.linux-x86_64-3.7/sndfile._sndfile.c -o build/temp.linux-x86_64-3.7/build/temp.linux-x86_64-3.7/sndfile._sndfile.o    build/temp.linux-x86_64-3.7/sndfile._sndfile.c:571:10: fatal error: sndfile.h: No such file or directory     #include <sndfile.h>              ^~~~~~~~~~~    compilation terminated.    error: command '/usr/bin/gcc' failed with exit code 1    ----------------------------------------ERROR: Command errored out with exit status 1: /home/hunan/miniconda3/envs/mocking/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '"'"'/tmp/pip-install-8ywejwpr/sndfile_b713bf1471364fbaa57702a47f955b06/setup.py'"'"'; __file__='"'"'/tmp/pip-install-8ywejwpr/sndfile_b713bf1471364fbaa57702a47f955b06/setup.py'"'"';f = getattr(tokenize, '"'"'open'"'"', open)(__file__) if os.path.exists(__file__) else io.StringIO('"'"'from setuptools import setup; setup()'"'"');code = f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' install --record /tmp/pip-record-ezhwkiaq/install-record.txt --single-version-externally-managed --compile --install-headers /home/hunan/miniconda3/envs/mocking/include/python3.7m/sndfile Check the logs for full command output.不知道你搞定没有，我的mac m1 安装好了，web.py跑步起来。这个错误我正好做了笔记，首先安装brew install libsndfile, 或者apt-get 安装，然后找到 libsndfile的头文件和lib目录，执行这个：CFLAGS="-I/opt/homebrew/Cellar/libsndfile/1.1.0/include" LDFLAGS="-L/opt/homebrew/Cellar/libsndfile/1.1.0/lib" pip install sndfile。项目启动后继续报错找不到 libsndfile.dylib，这个是因为我本地安装的目录在homebrew目录里，做个软链就好了。 
為什麼在執行的時候只打印"Python"，沒有執行
![スクリーンショット 2022-04-20 
在python3.10.1环境下numpy版本问题
这个程序要求numpy的版本低于1.21，然而python3.10不支持低于1.21版本的numpy。因此如何在python3.10环境下能正确运行程序呢？那你有的搞了，我弄了一晚上没弄好，发现是版本问题果断换了一个python版本。换3.8就行了建议3.9.8再新建一个虚拟环境用3.8吧，我就是这么解决的> 建议3.9.83.9.8可以吗> > 建议3.9.8> > 3.9.8可以吗我就是3.9.8，numpy是1.19.3 
模型无法学习8k赫兹以上的音频信息
全文搜索一下 
报错RuntimeError: Numpy is not available
(most recent call last):  File "G:\MockingBirdzip\MockingBird-main\toolbox\__init__.py", line 111, in <lambda>    func = lambda: self.load_from_browser(self.ui.browse_file())  File "G:\MockingBirdzip\MockingBird-main\toolbox\__init__.py", line 191, in load_from_browser    self.add_real_utterance(wav, name, speaker_name)  File "G:\MockingBirdzip\MockingBird-main\toolbox\__init__.py", line 215, in add_real_utterance    embed, partial_embeds, _ = encoder.embed_utterance(encoder_wav, return_partials=True)  File "G:\MockingBirdzip\MockingBird-main\encoder\inference.py", line 164, in embed_utterance    partial_embeds = embed_frames_batch(frames_batch)  File "G:\MockingBirdzip\MockingBird-main\encoder\inference.py", line 62, in embed_frames_batch    frames = torch.from_numpy(frames_batch).to(_device)RuntimeError: Numpy is not G:/MockingBirdzip/MockingBird-main/demo_toolbox.pyC:\Users\admin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\_masked\__init__.py:223: UserWarning: Failed to initialize NumPy: module compiled against API version 0xf but this version of numpy is 0xe (Triggered internally at  ..\torch\csrc\utils\tensor_numpy.cpp:68.)  example_input = torch.tensor([[-3, -2, -1], [0, 1, 2]])G:\MockingBirdzip\MockingBird-main\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.  warn("Unable to import 'webrtcvad'. This package enables noise removal and is recommended.")Arguments:    datasets_root:          None    vc_mode:                False    enc_models_dir:         encoder\saved_models    syn_models_dir:         synthesizer\saved_models    voc_models_dir:         vocoder\saved_models    extractor_models_dir:   ppg_extractor\saved_models    convertor_models_dir:   ppg2mel\saved_models    cpu:                    False    seed:                   None    no_mp3_support:         FalseWarning: you did not pass a root directory for datasets as argument.The recognized datasets are:	LibriSpeech/dev-clean	LibriSpeech/dev-other	LibriSpeech/test-clean	LibriSpeech/test-other	LibriSpeech/train-clean-100	LibriSpeech/train-clean-360	LibriSpeech/train-other-500	LibriTTS/dev-clean	LibriTTS/dev-other	LibriTTS/test-clean	LibriTTS/test-other	LibriTTS/train-clean-100	LibriTTS/train-clean-360	LibriTTS/train-other-500	LJSpeech-1.1	VoxCeleb1/wav	VoxCeleb1/test_wav	VoxCeleb2/dev/aac	VoxCeleb2/test/aac	VCTK-Corpus/wav48	aidatatang_200zh/corpus/dev	aidatatang_200zh/corpus/test	aishell3/test/wav	magicdata/trainFeel free to add your own. You can still use the toolbox by recording samples yourself.Loaded encoder "pretrained.pt" trained to step 1594501Traceback (most recent call last):  File "G:\MockingBirdzip\MockingBird-main\toolbox\__init__.py", line 111, in <lambda>    func = lambda: self.load_from_browser(self.ui.browse_file())  File "G:\MockingBirdzip\MockingBird-main\toolbox\__init__.py", line 191, in load_from_browser    self.add_real_utterance(wav, name, speaker_name)  File "G:\MockingBirdzip\MockingBird-main\toolbox\__init__.py", line 215, in add_real_utterance    embed, partial_embeds, _ = encoder.embed_utterance(encoder_wav, return_partials=True)  File "G:\MockingBirdzip\MockingBird-main\encoder\inference.py", line 164, in embed_utterance    partial_embeds = embed_frames_batch(frames_batch)  File "G:\MockingBirdzip\MockingBird-main\encoder\inference.py", line 62, in embed_frames_batch    frames = torch.from_numpy(frames_batch).to(_device)RuntimeError: Numpy is not available是刚导入样本语音时报错的可能是我用的代码不是最新的代码训练的？猜测，我的代码是大约2个月前的代码，或许新代码和旧代码的模型不通用，我等会把我的代码上传一份给你试试https://www.aliyundrive.com/s/7ab9F8d892Y我在starkoverflow和csdn也都找了一下报错，这是链接https://blog.csdn.net/weixin_40923064/article/details/119081081https://stackoverflow.com/questions/71689095/how-to-solve-the-pytorch-runtimeerror-numpy-is-not-available-without-upgrading希望能帮到你 
Add an aliyunpan download link
Baidu Yun Pan is so fxxking slow额……链接里面没有.pt文件……下了一个晚上，传到了mega云盘上https://mega.nz/folder/GfhwEJwQ#3DRny74AxJEU2-R7ITF4-g感谢。> 额……链接里面没有.pt文件……Aliyun居然也耍我，pt文件都不能分享，失算> 能想办法分享一下吗？比如改一下后缀名或者压缩等方式？可以试试改后缀，但是要重新分享链接了 
训练的时候出问题 RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format( RuntimeError: Error(s) in loading state_dict for Tacotron:
执行命令python synthesizer_train.py mandarin {自己的路径}\SV2TTS\synthesizer当_characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz1234567890!\'(),-.:;? '报错File "synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "D:\ai\MockingBird-main\MockingBird-main\synthesizer\train.py", line 122, in train    model.load(weights_fpath, device, optimizer)  File "D:\ai\MockingBird-main\MockingBird-main\synthesizer\models\tacotron.py", line 548, in load    self.load_state_dict(checkpoint["model_state"], strict=False)  File "D:\Anaconda3\envs\MB\lib\site-packages\torch\nn\modules\module.py", line 1497, in load_state_dict    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).        size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]).当_characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz1234567890!\'(),-.:;? '报错 File "synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "D:\ai\MockingBird-main\MockingBird-main\synthesizer\train.py", line 122, in train    model.load(weights_fpath, device, optimizer)  File "D:\ai\MockingBird-main\MockingBird-main\synthesizer\models\tacotron.py", line 548, in load    self.load_state_dict(checkpoint["model_state"], strict=False)  File "D:\Anaconda3\envs\MB\lib\site-packages\torch\nn\modules\module.py", line 1497, in load_state_dict    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]).        size mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([384, 1280]).改use_gst = False,    和   use_ser_for_gst = False,    报错  File "synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "D:\ai\MockingBird-main\MockingBird-main\synthesizer\train.py", line 122, in train    model.load(weights_fpath, device, optimizer)  File "D:\ai\MockingBird-main\MockingBird-main\synthesizer\models\tacotron.py", line 548, in load    self.load_state_dict(checkpoint["model_state"], strict=False)  File "D:\Anaconda3\envs\MB\lib\site-packages\torch\nn\modules\module.py", line 1497, in load_state_dict    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]).        size mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([384, 1280]).        size mismatch for decoder.rnn_input.weight: copying a param with shape torch.Size([1024, 640]) from checkpoint, the shape in current model is torch.Size([1024, 1152]).切换到tag0.0.1试试 
HIFI-GAN声码器训练出现问题
+ python3.9 + pytorch11.1数据集: 
Poor attention with a different speaker encoder
First, Thanks for the work by babysor!I noticed that the speaker encoder used in this work is ge2e, performance of which is far fall behind the SOTA. So I replaced the ge2e encoder with ECAPA-TDNN model. One difference between ge2e and ECAPA-TDNN is that the dimension of embedding is 192 in ECAPA-TDNN while 256 in ge2e. I changed the speaker embedding and batch sizeparameter in hparams.py and followed the synthesizer_train.py to train Tacotron synthesizer. The parameter I used are as follows:tts_schedule = [(2, 1e-3, 10_000, 32),(2, 5e-4, 15_000, 32),(2, 2e-4, 20_000, 32),(2, 1e-4, 30_000, 32),(2, 5e-5, 40_000, 32),(2, 1e-5, 60_000, 32),(2, 5e-6, 160_000, 32),(2, 3e-6, 320_000, 32),(2, 3e-6, 640_000, 32)]speaker_embedding_size = 192However, when i have trained the Tacotron 200k steps, i found my loss is 0.53 but the attention plot is blank. The mel output of each 500 steps is similar with the ground truth. The synthesized result with the 200k pretrained pt file is poor, but similar with the speaker used to synthesized. It is so weird.Does anyone meet the same problem? Or do i need to change other parameter when i change the dimension of speaker embedding?> First, Thanks for the work by babysor! I noticed that the speaker encoder used in this work is ge2e, performance of which is far fall behind the SOTA. So I replaced the ge2e encoder with ECAPA-TDNN model. One difference between ge2e and ECAPA-TDNN is that the dimension of embedding is 192 in ECAPA-TDNN while 256 in ge2e. I changed the speaker embedding and batch sizeparameter in hparams.py and followed the synthesizer_train.py to train Tacotron synthesizer. The parameter I used are as follows:> > tts_schedule = [(2, 1e-3, 10_000, 32), (2, 5e-4, 15_000, 32), (2, 2e-4, 20_000, 32), (2, 1e-4, 30_000, 32), (2, 5e-5, 40_000, 32), (2, 1e-5, 60_000, 32), (2, 5e-6, 160_000, 32), (2, 3e-6, 320_000, 32), (2, 3e-6, 640_000, 32)] speaker_embedding_size = 192> > However, when i have trained the Tacotron 200k steps, i found my loss is 0.53 but the attention plot is blank. The mel output of each 500 steps is similar with the ground truth. The synthesized result with the 200k pretrained pt file is poor, but similar with the speaker used to synthesized. It is so weird.> > Does anyone meet the same problem? Or do i need to change other parameter when i change the dimension of speaker embedding?Before jumping to Taco2, how's your result regarding w/ training the encoder?> > First, Thanks for the work by babysor! I noticed that the speaker encoder used in this work is ge2e, performance of which is far fall behind the SOTA. So I replaced the ge2e encoder with ECAPA-TDNN model. One difference between ge2e and ECAPA-TDNN is that the dimension of embedding is 192 in ECAPA-TDNN while 256 in ge2e. I changed the speaker embedding and batch sizeparameter in hparams.py and followed the synthesizer_train.py to train Tacotron synthesizer. The parameter I used are as follows:> > tts_schedule = [(2, 1e-3, 10_000, 32), (2, 5e-4, 15_000, 32), (2, 2e-4, 20_000, 32), (2, 1e-4, 30_000, 32), (2, 5e-5, 40_000, 32), (2, 1e-5, 60_000, 32), (2, 5e-6, 160_000, 32), (2, 3e-6, 320_000, 32), (2, 3e-6, 640_000, 32)] speaker_embedding_size = 192> > However, when i have trained the Tacotron 200k steps, i found my loss is 0.53 but the attention plot is blank. The mel output of each 500 steps is similar with the ground truth. The synthesized result with the 200k pretrained pt file is poor, but similar with the speaker used to synthesized. It is so weird.> > Does anyone meet the same problem? Or do i need to change other parameter when i change the dimension of speaker embedding?> > Before jumping to Taco2, how's your result regarding w/ training the encoder?The speaker encoder i trained was tested based on vox2 test set and achieved an EER about 0.6. 
关于能不能使用gpu训练
**问题简述**想问一下能不能通过GPU训练呢（具体就是用synthesizer_train的时候能不能使用GPU？**复现与环境**我搜了一下，将synthesizer_train头部加上了 加了那个代码才能用GPU吗是的，让你使用gpu而不是cpu 
给力
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型**Screenshots[截图（如有）]**If applicable, add screenshots to help给力灌水不文明 
M1上vocoder_train出错
**Summary[问题简述（一句话）]**AttributeError: Can't pickle local object 'run_synthesis.<locals>.<lambda>' 主要是m1系统出这个问题**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型m1，conda虚拟环境下，尝试预处理vocoder数据的时候出现上述问题。-环境配置问题基本都解决了，这个找不到本地对象不知是哪没成功。-conda下能跑合成器训练，也能打开demotoolbox。-模型是用了作者的，然后接着训练了想要模仿的声音。代码版本是最新的。**Screenshots[截图（如有）]**If applicable, add screenshots to help<img width="937" alt="截屏2022-04-09 17 33 32" src="https://user-images.githubusercontent.com/103334390/162592834-8ca71952-cc66-4884-87bf-981e89ae5a64.png">非常感谢！The version of python you're using isn't compatible with how pickle is being used in the multiprocessing module. You can turn off multiprocessing by setting num_workers=0 in the call to  
fix issue #496
pass ,  (in encoder/audio.py line 59 ) as keyword args instead of postional args to prevent warning messages from massing up console outputs while adopting librosa 0.9.1 occasionally. 
声码器一般训练多久？看什么参数？
声码器一般训练多久？看什么参数？Epoch: 95Steps : 3670, Gen Loss Total : 25.942, Mel-Spec. Error : 0.381, s/b : 1.108Steps : 3675, Gen Loss Total : 27.207, Mel-Spec. Error : 0.382, s/b : 1.116Steps : 3680, Gen Loss Total : 26.531, Mel-Spec. Error : 0.383, s/b : 1.109Steps : 3685, Gen Loss Total : 27.694, Mel-Spec. Error : 0.389, s/b : 1.118Steps : 3690, Gen Loss Total : 25.060, Mel-Spec. Error : 0.368, s/b : 1.112Steps : 3695, Gen Loss Total : 25.569, Mel-Spec. Error : 0.383, s/b : 1.111Steps : 3700, Gen Loss Total : 25.829, Mel-Spec. Error : 0.376, s/b : = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)],                      center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=False) #spec = torch.sqrt(spec.pow(2).sum(-1)+(1e-9))后spec的输出已经为[nan][nan][nan]了，导致后续在计算loss的时候就全部为nan了 
librosa reports warning as "From version 0.10 passing these as positional arguments will result in an error" that massed up the console output.
I have no idea why the version of librosa differs from .git状态： Could you please callout a pull request and I will review ASAP?> natürlichHi, I'm having the same issue, how to solve it @moosewoler 
部署好复杂，能提供docker版吗
如题 
I'm getting a DPI error message and looking forward to help!
I'm getting a DPI error message and looking forward to 
作者大佬，预处理报错
中文字符 最好是 xx1.wav 或 xx2.wav 这样子的> 字符超过限制了，不要有括号 中文字符 最好是 xx1.wav 或 xx2.wav 这样子的请问上图的no wordS是什么？我在预处理的时候没有出现这些no wordS的信息，直接解析出数据集的时长、文本长度和音频时间轴的信息之后就显示 这种属于正常现象么？> > 字符超过限制了，不要有括号 中文字符 最好是 xx1.wav 或 xx2.wav 这样子的> > 请问上图的no wordS是什么？我在预处理的时候没有出现这些no wordS的信息，直接解析出数据集的时长、文本长度和音频时间轴的信息之后就显示> > > > 这种属于正常现象么？上图的 no wordS按照我的理解是因为文本和语音不匹配，如有英文字符却识别中文。你没有出现则说明你的数据集筛选的不错，但是你的这种现象好像是训练的时候会出现的步骤吧？> > > 字符超过限制了，不要有括号 中文字符 最好是 xx1.wav 或 xx2.wav 这样子的> > > > > > 请问上图的no wordS是什么？我在预处理的时候没有出现这些no wordS的信息，直接解析出数据集的时长、文本长度和音频时间轴的信息之后就显示> > > > > > > >     > >       > >     > > > >       > >     > > > >     > >   > > 这种属于正常现象么？> > 上图的 no wordS按照我的理解是因为文本和语音不匹配，如有英文字符却识别中文。你没有出现则说明你的数据集筛选的不错，但是你的这种现象好像是训练的时候会出现的步骤吧？其实我比较疑惑，我是在执行  筛选我处理后的数据集，就会出现上图的信息(按照不匹配的说法，那么数据集应该没有大的问题)。然后再按照  训练合成器，中止并用其他网友分享的模型替换继续训练。但是从75K到136K的时候，训练出来的loss值比较大（0.3-0.48波动），还是会有电音> > > > 字符超过限制了，不要有括号 中文字符 最好是 xx1.wav 或 xx2.wav 这样子的> > > > > > > > > 请问上图的no wordS是什么？我在预处理的时候没有出现这些no wordS的信息，直接解析出数据集的时长、文本长度和音频时间轴的信息之后就显示> > > > > > > > > > > >     > > >       > > >     > > > > > >       > > >     > > > > > >     > > >   > > > 这种属于正常现象么？> > > > > > 上图的 no wordS按照我的理解是因为文本和语音不匹配，如有英文字符却识别中文。你没有出现则说明你的数据集筛选的不错，但是你的这种现象好像是训练的时候会出现的步骤吧？> > 其实我比较疑惑，我是在执行  筛选我处理后的数据集，就会出现上图的信息(按照不匹配的说法，那么数据集应该没有大的问题)。然后再按照  训练合成器，中止并用其他网友分享的模型替换继续训练。但是从75K到136K的时候，训练出来的loss值比较大（0.3-0.48波动），还是会有电音按照你的loss波动幅度大 你可以通过调整学习率来解决，或者增大你的batch size（需要足够的显存）和增加你的数据集。可以从synthesizer/hparams.py中修改参数。> > > > > 字符超过限制了，不要有括号 中文字符 最好是 xx1.wav 或 xx2.wav 这样子的> > > > > > > > > > > > 请问上图的no wordS是什么？我在预处理的时候没有出现这些no wordS的信息，直接解析出数据集的时长、文本长度和音频时间轴的信息之后就显示> > > > > > > > > > > > > > > >     > > > >       > > > >     > > > > > > > >       > > > >     > > > > > > > >     > > > >   > > > > 这种属于正常现象么？> > > > > > > > > 上图的 no wordS按照我的理解是因为文本和语音不匹配，如有英文字符却识别中文。你没有出现则说明你的数据集筛选的不错，但是你的这种现象好像是训练的时候会出现的步骤吧？> > > > > > 其实我比较疑惑，我是在执行  筛选我处理后的数据集，就会出现上图的信息(按照不匹配的说法，那么数据集应该没有大的问题)。然后再按照  训练合成器，中止并用其他网友分享的模型替换继续训练。但是从75K到136K的时候，训练出来的loss值比较大（0.3-0.48波动），还是会有电音> > 按照你的loss波动幅度大 你可以通过调整学习率来解决，或者增大你的batch size（需要足够的显存）和增加你的数据集。可以从synthesizer/hparams.py中修改参数。好的谢谢，另外这个学习率默认都是10的负多少多少次方。如果要减少波动幅度，是要把学习率往接近于0的方向调整么？> > > > > > 字符超过限制了，不要有括号 中文字符 最好是 xx1.wav 或 xx2.wav 这样子的> > > > > > > > > > > > > > > 请问上图的no wordS是什么？我在预处理的时候没有出现这些no wordS的信息，直接解析出数据集的时长、文本长度和音频时间轴的信息之后就显示> > > > > > > > > > > > > > > > > > > >     > > > > >       > > > > >     > > > > > > > > > >       > > > > >     > > > > > > > > > >     > > > > >   > > > > > 这种属于正常现象么？> > > > > > > > > > > > 上图的 no wordS按照我的理解是因为文本和语音不匹配，如有英文字符却识别中文。你没有出现则说明你的数据集筛选的不错，但是你的这种现象好像是训练的时候会出现的步骤吧？> > > > > > > > > 其实我比较疑惑，我是在执行  筛选我处理后的数据集，就会出现上图的信息(按照不匹配的说法，那么数据集应该没有大的问题)。然后再按照  训练合成器，中止并用其他网友分享的模型替换继续训练。但是从75K到136K的时候，训练出来的loss值比较大（0.3-0.48波动），还是会有电音> > > > > > 按照你的loss波动幅度大 你可以通过调整学习率来解决，或者增大你的batch size（需要足够的显存）和增加你的数据集。可以从synthesizer/hparams.py中修改参数。> > 好的谢谢，另外这个学习率默认都是10的负多少多少次方。如果要减少波动幅度，是要把学习率往接近于0的方向调整么？总之越小的学习率，梯度的下降会越慢，波动也会更小。 
导入社区模型后无法进行训练 报错RuntimeError: The size of tensor a (1024) must match the size of tensor b (3) at non-singleton dimension 3
UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")Traceback (most recent call last):  File "synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "E:\MockingBird-main\synthesizer\train.py", line 208, in train    optimizer.step()  File "C:\ProgramData\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 88, in wrapper    return func(*args, **kwargs)  File "C:\ProgramData\Anaconda3\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context    return func(*args, **kwargs)  File "C:\ProgramData\Anaconda3\lib\site-packages\torch\optim\adam.py", line 141, in step    F.adam(params_with_grad,  File "C:\ProgramData\Anaconda3\lib\site-packages\torch\optim\_functional.py", line 97, in adam    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)RuntimeError: The size of tensor a (1024) must match the size of tensor b (3) at non-singleton dimension 3RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for gst.stl.attention.W_query.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).修改后出现另一问题重新下载main后 已解决 
训练合成器时无法收敛
**问题简述**使用自己的数据集训练合成器模型的时候的时候，在预处理之后训练合成器并将合成器替换成既有model后产生的图并没有收敛。**复现与环境**参照www.bilibili.com/video/BV1dq4y137pH 进行的复现。代码版本为main branch，首先进行数据预处理之后参考视频里的首先进行合成器训练，然后用pretrained-11-7-21 替换掉当前mode pretrained-11-7-21 训练，确认一下文件名没有错误> 看步数不对，没有成功基于 pretrained-11-7-21 训练，确认一下文件名没有错误我中途强制退出了，因为一直没有呈现收敛感觉图不对。请问需要我一直train吗？文件名是使用 pretrained-11-7-21 然后改成自己的model的名字（按照视频里的操作）来的。> > 看步数不对，没有成功基于 pretrained-11-7-21 训练，确认一下文件名没有错误> > 我中途强制退出了，因为一直没有呈现收敛感觉图不对。请问需要我一直train吗？文件名是使用 pretrained-11-7-21 然后改成自己的model的名字（按照视频里的操作）来的。路径是否正确…如果一切正确 起始step会是比较大的值> > > > 看步数不对，没有成功基于 pretrained-11-7-21 训练，确认一下文件名没有错误> > > > > > 我中途强制退出了，因为一直没有呈现收敛感觉图不对。请问需要我一直train吗？文件名是使用 pretrained-11-7-21 然后改成自己的model的名字（按照视频里的操作）来的。> > 路径是否正确…如果一切正确 起始step会是比较大的值## 使用的cmd为：python synthesizer_train.py xxxx G:\\data\\SV2TTS\\synthesizer xxxx 就是main branch 文件里model的名字。是我用pretrained 替换的（就改了pretrained的名字）。> > > > > > > 看步数不对，没有成功基于 pretrained-11-7-21 训练，确认一下文件名没有错误> > > > > > > > > 我中途强制退出了，因为一直没有呈现收敛感觉图不对。请问需要我一直train吗？文件名是使用 pretrained-11-7-21 然后改成自己的model的名字（按照视频里的操作）来的。> > > > > > 路径是否正确…如果一切正确 起始step会是比较大的值> > ## 使用的cmd为：python synthesizer_train.py xxxx G:\data\SV2TTS\synthesizer> xxxx 就是main branch 文件里model的名字。是我用pretrained 替换的（就改了pretrained的名字）。有点奇怪，那你截图的文件夹路径是？> > > > > > > > > > > > 看步数不对，没有成功基于 pretrained-11-7-21 训练，确认一下文件名没有错误> > > > > > > > > > > > 我中途强制退出了，因为一直没有呈现收敛感觉图不对。请问需要我一直train吗？文件名是使用 pretrained-11-7-21 然后改成自己的model的名字（按照视频里的操作）来的。> > > > > > > > > 路径是否正确…如果一切正确 起始step会是比较大的值> > > > > > ## 使用的cmd为：python synthesizer_train.py xxxx G:\data\SV2TTS\synthesizer> > xxxx 就是main branch 文件里model的名字。是我用pretrained 替换的（就改了pretrained的名字）。> > 有点奇怪，那你截图的文件夹路径是？是main branch 文件夹里save_models，我的model 的plot里的图。> > > > > > > > > > > > 看步数不对，没有成功基于 pretrained-11-7-21 训练，确认一下文件名没有错误> > > > > > > > > > > > 我中途强制退出了，因为一直没有呈现收敛感觉图不对。请问需要我一直train吗？文件名是使用 pretrained-11-7-21 然后改成自己的model的名字（按照视频里的操作）来的。> > > > > > > > > 路径是否正确…如果一切正确 起始step会是比较大的值> > > > > > ## 使用的cmd为：python synthesizer_train.py xxxx G:\data\SV2TTS\synthesizer> > xxxx 就是main branch 文件里model的名字。是我用pretrained 替换的（就改了pretrained的名字）。> > 有点奇怪，那你截图的文件夹路径是？data  和 程序并不在一个directory里目前判断是路径问题，你确认一下训练打印时的路径里是不是你下载的预训练模型> 目前判断是路径问题，你确认一下训练打印时的路径里是不是你下载的预训练模型还有就是预处理时没有像视频教程里那样打印出no words，请问这是正常情况吗> > 目前判断是路径问题，你确认一下训练打印时的路径里是不是你下载的预训练模型> > 还有就是预处理时没有像视频教程里那样打印出no words，请问这是正常情况吗正常> > > 目前判断是路径问题，你确认一下训练打印时的路径里是不是你下载的预训练模型> > > > > > 还有就是预处理时没有像视频教程里那样打印出no words，请问这是正常情况吗> > 正常排查了一下，我训练用的是git bash的命令行，不是powershell也不是cmd，是不是这里的路径问题呢？但是我现在用powershell train发现速度非常慢。。。又重新测试了一下， 路径都正确的情况下，steps仍旧只有500解决了，我之前一直用的git bash 调用的py文件，导致了路径不对，使用cmd后就正确了。 
分享下自己训练的模型
音源是bilibili vup人间蜜药，总语音2000句，有效语音1700句链接：「mandarin.exe」https://www.aliyundrive.com/s/HKHLLwVcrrZ 提取码: 4nb7点击链接保存，或者复制本段内容，打开「阿里云盘」APP 大佬，跑出来的都是杂音，不知道什么情况我遇见过，但我没法复现。有个比较玄学的办法是打开toolbox之后先选择模型文件再选择示例语音文件，但我只遇见过极少几次。我把我的代码文件打包发你，你试试用我的代码会不会有效果？毕竟现在的代码可能更新过。当然只是猜测，如果没效果的话还是单独提个issue问作者比较好https://www.aliyundrive.com/s/7ab9F8d892Y 
找不到模型
模型已经放在特定的文件夹里面了，但是还是显示没有找到模型Error: Model files not found. Please download the models 
UnboundLocalError: local variable 'sample_rate' referenced before assignment 报错
能打开BOX，但是，不知道为什么，不管用那种模型，合成出来的声音都听不了，就是全是杂音。（中英文都不得行）在哪个代码版本？ master分支 @babysor 遇到相同问题了，Vocoder设置为g_hifigan和pretrained没问题，但是设置为Griffin-Lim出现如下报错： 修改toolbox\\\_\_init\_\_.py  297 行，增加sample_rate=Synthesizer.sample_rate重启启动运行即可修复bug！自己动手丰衣足食！> 修改toolbox\__init__.py 297 行，增加 sample_rate=Synthesizer.sample_rate 重启启动运行即可修复bug！> > 自己动手丰衣足食！感谢！有可能协助修改发起一个pull request吗可以！***@***.*** 发件人： Vega发送时间： 2022-05-13 13:42收件人： babysor/MockingBird抄送： zfb7901; Comment主题： Re: [babysor/MockingBird] UnboundLocalError: local variable 'sample_rate' referenced before assignment 报错 (Issue #488)修改toolbox_init_.py 297 行，增加 sample_rate=Synthesizer.sample_rate 重启启动运行即可修复bug！自己动手丰衣足食！感谢！有可能协助修改发起一个pull request吗—Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you commented.Message ID: ***@***.***>给开一下权限！> 给开一下权限！你可以从你fork的项目发起一个pull request到这里，或者直接推到你的项目里，我来发起。 
预处理ppg模型时出错
Globbed 891 wav files.Loaded encoder "pretrained_bak_5805000.pt" trained to step 5805001Preprocessing:   0%|                                                                         | 0/891 [00:00<?, ?wav/s]multiprocessing.pool.RemoteTraceback:"""Traceback (most recent call last):  File "C:\Users\CHOPY\AppData\Local\Programs\Python\Python39\lib\multiprocessing\pool.py", line 125, in worker    result = (True, func(*args, **kwds))  File "Z:\deeplearing_project\MockingBird-main\ppg2mel\preprocess.py", line 72, in preprocess_one    wav = resampy.resample(wav, sr, SAMPLE_RATE)  File "C:\Users\CHOPY\AppData\Local\Programs\Python\Python39\lib\site-packages\resampy\core.py", line 97, in resample    raise ValueError('Input signal length={} is too small to 'ValueError: Input signal length=2 is too small to resample from 44100->16000"""The above exception was the direct cause of the following exception:Traceback (most recent call last):  File "Z:\deeplearing_project\MockingBird-main\pre4ppg.py", line 49, in <module>    preprocess_dataset(**vars(args))  File "Z:\deeplearing_project\MockingBird-main\ppg2mel\preprocess.py", line 96, in preprocess_dataset    list(tqdm(job, "Preprocessing", len(wav_file_list), unit="wav"))  File "C:\Users\CHOPY\AppData\Local\Programs\Python\Python39\lib\site-packages\tqdm\_tqdm.py", line 1017, in __iter__    for obj in iterable:  File "C:\Users\CHOPY\AppData\Local\Programs\Python\Python39\lib\multiprocessing\pool.py", line 870, in next    raise valueValueError: Input signal length=2 is too small to resample from 44100->16000看起来好像是采样率的原因 但我又从格式工厂看了 Globbed 891 wav files. Loaded encoder "pretrained_bak_5805000.pt" trained to step 5805001 Preprocessing: 0%| | 0/891 [00:00<?, ?wav/s]multiprocessing.pool.RemoteTraceback: """ Traceback (most recent call last): File "C:\Users\CHOPY\AppData\Local\Programs\Python\Python39\lib\multiprocessing\pool.py", line 125, in worker result = (True, func(*args, **kwds)) File "Z:\deeplearing_project\MockingBird-main\ppg2mel\preprocess.py", line 72, in preprocess_one wav = resampy.resample(wav, sr, SAMPLE_RATE) File "C:\Users\CHOPY\AppData\Local\Programs\Python\Python39\lib\site-packages\resampy\core.py", line 97, in resample raise ValueError('Input signal length={} is too small to ' ValueError: Input signal length=2 is too small to resample from 44100->16000 """> > The above exception was the direct cause of the following exception:> > Traceback (most recent call last): File "Z:\deeplearing_project\MockingBird-main\pre4ppg.py", line 49, in preprocess_dataset(**vars(args)) File "Z:\deeplearing_project\MockingBird-main\ppg2mel\preprocess.py", line 96, in preprocess_dataset list(tqdm(job, "Preprocessing", len(wav_file_list), unit="wav")) File "C:\Users\CHOPY\AppData\Local\Programs\Python\Python39\lib\site-packages\tqdm_tqdm.py", line 1017, in **iter** for obj in iterable: File "C:\Users\CHOPY\AppData\Local\Programs\Python\Python39\lib\multiprocessing\pool.py", line 870, in next raise value ValueError: Input signal length=2 is too small to resample from 44100->16000> > 看起来好像是采样率的原因 但我又从格式工厂看了 是44100的采样率，必须要转换成16000才能训练吗？但是好像转换不了16000的 一样的问题> 22050试试Globbed 891 wav files.Loaded encoder "pretrained_bak_5805000.pt" trained to step 5805001Preprocessing:   0%|                                                                          | 0/891 [00:00<?, ?wav/s]multiprocessing.pool.RemoteTraceback:"""Traceback (most recent call last):  File "C:\Users\CHOPY\AppData\Local\Programs\Python\Python39\lib\multiprocessing\pool.py", line 125, in worker    result = (True, func(*args, **kwds))  File "Z:\deeplearing_project\MockingBird-main\ppg2mel\preprocess.py", line 77, in preprocess_one    _, length_f0 = _compute_f0_from_wav(output_fpath=f"{out_dir}/f0/{utt_id}.f0.npy", wav=wav)  File "Z:\deeplearing_project\MockingBird-main\ppg2mel\preprocess.py", line 41, in _compute_f0_from_wav    f0 = compute_f0(wav, SAMPLE_RATE)  File "Z:\deeplearing_project\MockingBird-main\utils\f0_utils.py", line 17, in compute_f0    f0, _ = pyworld.harvest(  File "pyworld/pyworld.pyx", line 154, in pyworld.pyworld.harvestValueError: Buffer has wrong number of dimensions (expected 1, got 2)"""The above exception was the direct cause of the following exception:Traceback (most recent call last):  File "Z:\deeplearing_project\MockingBird-main\pre4ppg.py", line 49, in <module>    preprocess_dataset(**vars(args))  File "Z:\deeplearing_project\MockingBird-main\ppg2mel\preprocess.py", line 96, in preprocess_dataset    list(tqdm(job, "Preprocessing", len(wav_file_list), unit="wav"))  File "C:\Users\CHOPY\AppData\Local\Programs\Python\Python39\lib\site-packages\tqdm\_tqdm.py", line 1017, in __iter__    for obj in iterable:  File "C:\Users\CHOPY\AppData\Local\Programs\Python\Python39\lib\multiprocessing\pool.py", line 870, in next    raise valueValueError: Buffer has wrong number of dimensions (expected 1, got 2)转换好了之后是这样的已解决我通过Audacity将要处理的音频重采样到16000Hz，然后修改了ppg2mel/preprocess.py第68行 ppg2mel/preprocess.py第68行wav, sr = soundfile.read(str(wav_path)) 仍然存在 Buffer has wrong number of dimensions 这个问题呢？> 修改了 ppg2mel/preprocess.py第68行wav, sr = soundfile.read(str(wav_path)) 仍然存在 Buffer has wrong number of dimensions 这个问题呢？再次确认你的音频是否为单声道16kHz感谢 我确实是只转换成了16KHz 没有转成单声道 问题解决了贴一段自动处理的代码 
训练不了，进行预处理时报错   OSError: [WinError 1455] 页面文件太小，无法完成操作。
python pre.py D:\adata -d aidatatang_200zh -n 6运行预处理代码时就会报错python 3.9 5800 1650和python3.9 5800 3060都是在这一步出错，一个安装了Anaconda3，一个没有但是报错都是一样的请问有人遇到过吗，有什么解决方案吗，十分感谢  已解决，解决方法如下：修改虚拟内存方法：此电脑--属性--win10在搜索并进入高级设置（win11 有高级系统设置）--系统属性-高级--性能（设置）--高级--虚拟内存（更改）更改大小--自定义，起始值和最大值一样，越大越好但是会占用磁盘空间，预留够磁盘大小 
全是杂音~
搞定了，最新版本改了symbols也不行 用ceshi的话退回一个上古版本就行了 想用最新版就用那个README里面名字特长的那个请问名字特长的是什么？synth模型？还是其他的东西？> 是synth模型pretrained-11-7-21_75k.ptmy_run8_25k.pt这两个都挺好的当然我毕竟也就是个小白，等作者正解> > > > 是synth模型 pretrained-11-7-21_75k.pt my_run8_25k.pt 这两个都挺好的 1、试试转wav 2、如果您改过synthesizer/utils/symbols.py的话，改回去 3、尝试重新下载pt感谢回复。方法一我没有尝试过。方法二试过没用。方法三也没用，已经用过好多个预训练模型了。> 1、试试转wav 2、如果您改过synthesizer/utils/symbols.py的话，改回去 3、尝试重新下载pt刚刚试了1也没有用> > 1、试试转wav 2、如果您改过synthesizer/utils/symbols.py的话，改回去 3、尝试重新下载pt> > 感谢回复。方法一我没有尝试过。方法二试过没用。方法三也没用，已经用过好多个预训练模型了。我的意思是要改回原版，而不是use this old one 
ImportError: dlopen: cannot load any more object with static TLS
**Summary[问题简述（一句话）]**在运行web.py和demo_toolbox.py时，报如题错误**Env & To Reproduce[复现与环境]**OS: CentOS 7Python: Anaconda+python 
点击合成器报错
RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([66, 512]).这个错误怎么解决，设置了use_gst = False,已经消除几个了，还有一个，怎么解决呢#37 感谢反馈，但是我硬是没找到这个文件synthesizer/utils/symbols.py![Uploading (most recent call last):  File "C:\Users\91096\Desktop\MyFile\smilatevoices\speekChinese\toolbox\__init__.py", line 262, in synthesize    specs = self.synthesizer.synthesize_spectrograms(texts, embeds, style_idx=int(self.ui.style_slider.value()), min_stop_token=min_token, steps=int(self.ui.length_slider.value())*200)  File "C:\Users\91096\Desktop\MyFile\smilatevoices\speekChinese\synthesizer\inference.py", line 93, in synthesize_spectrograms    self.load()  File "C:\Users\91096\Desktop\MyFile\smilatevoices\speekChinese\synthesizer\inference.py", line 71, in load    self._model.load(self.model_fpath, self.device)  File "C:\Users\91096\Desktop\MyFile\smilatevoices\speekChinese\synthesizer\models\tacotron.py", line 547, in load    self.load_state_dict(checkpoint["model_state"], strict=False)  File "C:\Users\91096\Envs\python37\lib\site-packages\torch\nn\modules\module.py", line 1498, in load_state_dict    self.__class__.__name__, "\n\t".join(error_msgs)))RuntimeError: Error(s) in loading state_dict for Tacotron:	size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]). 
红米k40打开demo_toolbox报错
![Screenshot_2022-03-30-13-21-43-065_com realvnc viewer my god your mobile phone is so stronger 
请教作者，训练成果问题
完成后会接近你的声音，但看起来你的语音质量不高，导致loss不怎么降低了谢谢大佬的回复，我自己做的语音都是短句，那我要增加语音量还是改一下句长呢？万分感谢> > 完成后会接近你的声音，看起来你的声音怎么不高，导致损失不降低但> > 谢谢大佬的回复，我自己做的都是短句，那我要 感谢增加语音量还是改一下句长呢？看起来你的数据集不够多 而且不够清晰 并且没有经过筛选，或者你的声音比较特别 不然训练这么多次 还是迁移学习的情况下不可能是这个loss的> > 完成后会接近你的声音，但看起来你的语音质量不高，导致loss不怎么降低了> > 谢谢大佬的回复，我自己做的语音都是短句，那我要增加语音量还是改一下句长呢？ 万分感谢多一点语音量，尽可能到小时级别Hello... 
请教作者大佬，收敛不成功，线一直在顶端
#437 
vc 模式切换 vocoder 遇到 keyError
**Summary[问题简述（一句话）]**vc 模式切换 vocoder 遇到 keyError  **Env & To Reproduce[复现与环境]**Ubuntu20.04.3,使用 cpu 版本的 pytorch（可能是cuda没装好 gpu 版本的跑不起来……），代码为最新版，在 vc 模式下切换 vocoder 为网盘里提供的 hifigan_24k 时，发生问题。  完整的终端输出见：https://pastebin.ubuntu.com/p/kDsY3JHPVj/  只有切换到这个模型的时候会出问题，有没有加载音频好像都一样  **Screenshots[截图（如有）]**![2022-03-27 17-05-22 
求助作者，无法接着别人模型训练问题
需要切换到老版本> 模型版本太老 
替换模型训练遇到的问题
就是接着这个大佬的模型继续训练，因为我自己的模型太过于小了，无法收敛。但是一更换就报错截图在下面   然后我再复制粘贴一下防止我没传上来图片E:\数据集制作\MockingBird-main\synthesizer\synthesizer_dataset.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\torch\csrc\utils\tensor_new.cpp:201.)  embeds = torch.tensor(embeds)C:\Users\11351\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")Traceback (most recent call last):  File "E:\数据集制作\MockingBird-main\synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "E:\数据集制作\MockingBird-main\synthesizer\train.py", line 208, in train    optimizer.step()  File "C:\Users\11351\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\optimizer.py", line 88, in wrapper    return func(*args, **kwargs)  File "C:\Users\11351\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\autograd\grad_mode.py", line 28, in decorate_context    return func(*args, **kwargs)  File "C:\Users\11351\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\adam.py", line 133, in step    F.adam(params_with_grad,  File "C:\Users\11351\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\_functional.py", line 86, in adam    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)RuntimeError: The size of tensor a (1024) must match the size of tensor b (3) at non-singleton dimension  我用的是这个大佬的数据集https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g 4j5d社区的模型都试过了，除了谷歌DIrve的下不了之外。其他的都试过了，也是报错。https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw 提取码：om7f  这个大佬的是这个代码RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).        size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]).        size mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([384, 1280]).        size mismatch for decoder.rnn_input.weight: copying a param with shape torch.Size([1024, 640]) from checkpoint, the shape in current model is torch.Size([1024, 1152]).        size mismatch for decoder.stop_proj.weight: copying a param with shape torch.Size([1, 1536]) from checkpoint, the shape in current model is torch.Size([1, 2048]).https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ 提取码：2021这个的是下面的   就是我的数据量要匹配这个模型才可以么RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).        size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]).        size mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([384, 1280]).        size mismatch for decoder.rnn_input.weight: copying a param with shape torch.Size([1024, 640]) from checkpoint, the shape in current model is torch.Size([1024, 1152]).        size mismatch for decoder.stop_proj.weight: copying a param with shape torch.Size([1, 1536]) from checkpoint, the shape in current model is torch.Size([1, 2048]).同求大佬，一样问题不是数据量问题，而是你的模型和代码版本不一致，你下载前要看模型是哪个代码版本，要切换过去（版本切换请学git）好的  谢谢大佬   我去学一下gui> 不是数据量问题，而是你的模型和代码版本不一致，你下载前要看模型是哪个代码版本，要切换过去（版本切换请学git）我用的tag 0.01 的版本预处理然后换了模型（用的是ceshi）发现继续训练的时候会出现valueerror， 具体为：Loading weights at synthesizer\saved_models\chenlin\chenlin.ptTraceback (most recent call last):  File "D:\Downloads\MockingBird-0.0.1\synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "D:\Downloads\MockingBird-0.0.1\synthesizer\train.py", line 114, in train    model.load(weights_fpath, optimizer)  File "D:\Downloads\MockingBird-0.0.1\synthesizer\models\tacotron.py", line 526, in load    optimizer.load_state_dict(checkpoint["optimizer_state"])  File "C:\Users\xxxxxxx\AppData\Loc  File "D:\Downloads\MockingBird-0.0.1\synthesizer\train.py", line 114, in train    model.load(weights_fpath, optimizer)  File "D:\Downloads\MockingBird-0.0.1\synthesizer\models\tacotron.py", line 526, in load    optimizer.load_state_dict(checkpoint["optimizer_state"])  File "C:\Users\xxxxxxx\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\optimizer.py", line 146, in load_state_dict    raise ValueError("loaded state dict contains a parameter group "ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's groupal\Programs\Python\Python39\lib\site-packages\torch\optim\optimizer.py", line 146, in load_state_dict    raise ValueError("loaded state dict contains a parameter group "ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group请问下好心人怎么解决，第一次使用有些地方不太了解 
求教，怎么接着别的作者继续训练
4j5d替换了，不能进行训练了。什么问题因为一直无法收敛，怎么办求教，谢谢各位大神紧接着别的作者训练是不是要训练到同K，我替换了别的作者的训练，就没办法运行训练了。求教报错大概率是代码版本问题，这个链接谁分享的？可能要去搞下代码版本 
请问克隆出来的语音有什么办法保存为wav文件吗
如题直接就保存了，你点下 export 
小白求教，这样是正常收敛中的效果吗

一些关于生成语音时一直会有电流音的猜想
现在大家不管怎么优化提纯原始的人物语音，就算训练出来后loss<2.0，生成的语音也带有电流音。会不会还是训练器本身的问题：首先就是，当你用MockingBird GUI打开本地音频，它播放时就有几率是带有电流音；另一个就是，在你生成的saved_models的wavs文件夹里，存放的都是训练时生成的音频，他们全都带有严重电流音，而且声音质量并没有随着step的增加而有任何改善。希望大神能够指导一下我等小白，怎么能生成无电流音的声音同问，不知道怎么生成无电流音的你用的vocoder是哪个？如果频谱ok的话，大概率是vocoder问题为什么考虑是vocoder问题呢？vocoder没训练好的话，电音就会很突出> vocoder没训练好的话，电音就会很突出啊，我一直就是用python pre.py D:\MockingBird-main -d aidatatang_200zh -n 7python synthesizer_train.py test D:\MockingBird-main\SV2TTS\synthesizer这俩命令呀，没有我自己搞的vocoder> 不要用hifigan这个vocoder，电音很严重> > > > 不要用hifigan这个vocoder，电音很严重我一直用的是pretrained呀，训练前准备的语音数据绝对时纯净的语音，但训练出来，就会有电音> > > > 不要用hifigan这个vocoder，电音很严重我一直用的是pretrained呀，训练前准备的语音数据绝对时纯净的语音，但训练出来，就会有电音> > > > > > > > > 不要用hifigan这个vocoder，电音很严重> > 我一直用的是pretrained呀，训练前准备的语音数据绝对时纯净的语音，但训练出来，就会有电音pretrained的效果本来就一般般。。要再训练> > > > > > > > > > > > > 不要用hifigan这个vocoder，电音很严重> > > > > > 我一直用的是pretrained呀，训练前准备的语音数据绝对时纯净的语音，但训练出来，就会有电音> > pretrained的效果本来就一般般。。要再训练在训了，在训了，但是又发现俩问题：① 想训练wavernn声码器，执行python vocoder_train.py <trainid> <datasets_root>命令会报错，提示vocoder_train.py: error: the following arguments are required: vocoder_type② 没办法，那我就去训练hifigan声码器，执行python vocoder_train.py <trainid> <datasets_root> hifigan 没有报错。但训练时控制台一直打印这种Epoch: 187 Time taken for epoch 187 is 3 secEpoch: 188 Time taken for epoch 188 is 3 sec...Epoch: 3100 Time taken for epoch 3100 is 3 sec然后就没有然后了，\vocoder\saved_models\xxx是有的，但里面只有俩日志，没有训练好的pt文件 
在cmd中输入python demo_toolbox.py后没有反应
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.在所有东西安装完成并且没有报错之后输入python demo_toolbox.py没有任何反应，普通环境和虚拟环境都没有反应，会直接跳回输入命令的地方**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型**Screenshots[截图（如有）]**If applicable, add screenshots to 
web 合成时候报错 MemoryError: Cannot allocate write+execute memory for ffi.callback(). 
**Summary[问题简述（一句话）]**使用web版本合成的时候，提示这个：MemoryError: Cannot allocate write+execute memory for ffi.callback(). You might be running on a system that prevents this. For more information, see - - [2022-03-23 16:52:59] "POST /api/synthesize HTTP/1.1" 500 426 14.242760**Env & To Reproduce[复现与环境]**mac m1用的75k steps 用3个开源数据集混合训练**Screenshots[截图（如有）]**<img width="1002" alt="image" src="https://user-images.githubusercontent.com/10892272/159668969-99fa13ff-4ad0-4bad-8a80-120b7f793482.png">我靠，m1才会遇到的问题，你可以看下报错里面的修复方案我的 M1 走通了 我用的python3.10    你试试 
Update README-CN.md
修正一个简单的翻译问题 
尝试番外遇到的问题
电脑无GPU情况下，尝试进行番外部分的实现按照4.1的部分，将下载的模型移动到了对应位置然后直接在命令行输入python demo_toolbox.py 
尝试番外篇遇到的问题
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型**Screenshots[截图（如有）]**If applicable, add screenshots to help 
奇怪的注意力模型和较低的Loss是什么情况？以及训练vocoder时失败
版本：3.7.9CUDA 版本：11.3pyTorch 版本：1.11.0GPU：1060数据集：aidatatang_200zhsorry，练坏了，要重新开始。到15k的时候还是再顶部就重试把，一开始会有一定失败率 
算我求各位的了别用那狗💩百度云了
阿里云盘 yyds我也觉得！！！！！！！1不知道G drive行不行，反正现在能上github的人多半都能上google> 不知道G drive行不行，反正现在能上github的人多半都能上googleG drive指谷歌的网盘？github裸连有时候会连不上。还是挂梯子稳。下载速度绝了一下午了一个没下载下来 
生成时出现的错误RuntimeError: Error(s) in loading state_dict for Tacotron: 	size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512])
现在该问题已经解决，具体解决方式在#209，将文件 synthesizer\hparams.py 中的: use_gst use_ser_for_gst 均设置为False后能够正常运行。非常感谢！ 
下载的模型加载dataset报错AttributeError: 'WindowsPath' object has no attribute 'tell'
下载的模型加载dataset时报错AttributeError: 'WindowsPath' object has no attribute 
在训练synthesizer为什么会无缘无故的报AssertionError
Traceback (most recent call last):  File "C:\file\speach\MockingBird\pre.py", line 57, in <module>    assert 或者，你在C盘指定得H盘导致得谢谢大哥，问题已经解决了 
运行报错缺少pyworld，安装visualstudio后pip install pyworld后报错如下，求解。
Collecting pyworld  Using cached pyworld-0.3.0.tar.gz (212 kB)  Installing build dependencies ... done  Getting requirements to build wheel ... done  Preparing metadata (pyproject.toml) ... doneRequirement already satisfied: numpy in d:\anaconda\data\lib\site-packages (from pyworld) (1.19.3)Requirement already satisfied: cython>=0.24.0 in d:\anaconda\data\lib\site-packages (from pyworld) (0.29.24)Building wheels for collected packages: pyworld  Building wheel for pyworld (pyproject.toml) ... error  error: subprocess-exited-with-error  × Building wheel for pyworld (pyproject.toml) did not run successfully.  │ exit code: 1  ╰─> [22 lines of output]      C:\Users\73465\AppData\Local\Temp\pip-build-env-_ap7ysdb\overlay\Lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead        warnings.warn(      running bdist_wheel      running build      running build_py      creating build      creating build\lib.win-amd64-3.9      creating build\lib.win-amd64-3.9\pyworld      copying pyworld\__init__.py -> build\lib.win-amd64-3.9\pyworld      running build_ext      skipping 'pyworld\pyworld.cpp' Cython extension (up-to-date)      building 'pyworld.pyworld' extension      creating build\temp.win-amd64-3.9      creating build\temp.win-amd64-3.9\Release      creating build\temp.win-amd64-3.9\Release\lib      creating build\temp.win-amd64-3.9\Release\lib\World      creating build\temp.win-amd64-3.9\Release\lib\World\src      creating build\temp.win-amd64-3.9\Release\pyworld      D:\visualstudio\visualstudio\VC\Tools\MSVC\14.29.30133\bin\HostX86\x64\cl.exe /c /nologo /O2 /W3 /GL /DNDEBUG /MD -Ilib\World\src -IC:\Users\73465\AppData\Local\Temp\pip-build-env-_ap7ysdb\overlay\Lib\site-packages\numpy\core\include -ID:\Anaconda\DATA\include -ID:\Anaconda\DATA\Include -ID:\visualstudio\visualstudio\VC\Tools\MSVC\14.29.30133\include /EHsc /Tplib\World\src\cheaptrick.cpp /Fobuild\temp.win-amd64-3.9\Release\lib\World\src\cheaptrick.obj      cheaptrick.cpp      lib\World\src\cheaptrick.cpp(10): fatal error C1083: 无法打开包括文件: “math.h”: No such file or directory      error: command 'D:\\visualstudio\\visualstudio\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX86\\x64\\cl.exe' failed with exit code 2      [end of output]  note: This error originates from a subprocess, and is likely not a problem with pip.  ERROR: Failed building wheel for pyworldFailed to build pyworldERROR: Could not build wheels for pyworld, which is required to install pyproject.toml-based projects最新代码移除这个依赖了，可以拉下来试试> 最新代码移除这个依赖了，可以拉下来试试解决了，感谢你好 请问怎么解决的啊我也遇到了这个问题 
小白提问，有大佬知道这是怎么回事吗？
{| ████████████████ 19000/19200 | Batch Size: 2 | Gen Rate: 1.5kHz | }[2022-03-17 19:46:31,246] ERROR in app: Exception on /api/synthesize [POST]Traceback (most recent call last):  File "C:\Users\29238\AppData\Local\Programs\Python\Python39\lib\site-packages\flask\app.py", line 2073, in wsgi_app    response = self.full_dispatch_request()  File "C:\Users\29238\AppData\Local\Programs\Python\Python39\lib\site-packages\flask\app.py", line 1518, in full_dispatch_request    rv = self.handle_user_exception(e)  File "C:\Users\29238\AppData\Local\Programs\Python\Python39\lib\site-packages\flask_restx\api.py", line 672, in error_router    return original_handler(e)  File "C:\Users\29238\AppData\Local\Programs\Python\Python39\lib\site-packages\flask\app.py", line 1516, in full_dispatch_request    rv = self.dispatch_request()  File "C:\Users\29238\AppData\Local\Programs\Python\Python39\lib\site-packages\flask\app.py", line 1502, in dispatch_request    return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args)  File "D:\MockingBird-main\web\__init__.py", line 118, in synthesize    write(out, sample_rate, wav.astype(np.float32))AttributeError: 'tuple' object has no attribute 'astype'127.0.0.1 - - [2022-03-17 19:46:31] "POST /api/synthesize HTTP/1.1" 500 426 15.250402看起来像一个小bug，有同学能帮忙修一下吗？#445Fixed in main branch 
分享一下自己训练的模型pipimeng
这个模型是在pretrained模型的基础上进行训练的，原来75k训练到了160k。链接: 提取码: 9dp6 好的，等我后面有时间再多找点素材再训练一会请教：剪了八百多句，做数据集，这个怎么弄。> > > > 好的，等我后面有时间再多找点素材再训练一会你是用多大的batch size 训练的> 请教：剪了八百多句，做数据集，这个怎么弄。有B站视频教学的请问一下你的数据集是怎么训练的呀，我在终端调用命令一直出现拒绝访问的情况> 这个模型是在pretrained模型的基础上进行训练的，原来75k训练到了160k。 链接: 提取码: 9dp6 复制这段内容后打开百度网盘手机App，操作更方便哦 训练集是找的B站一个vup叫皮皮梦的录播，一共剪了八百多句，效果不是特别好，不过还能将就，针对皮皮梦的音源效果会好一点。 下面是图 > 这个模型是在pretrained模型的基础上进行训练的，原来75k训练到了160k。 链接: 提取码: 9dp6 复制这段内容后打开百度网盘手机App，操作更方便哦 训练集是找的B站一个vup叫皮皮梦的录播，一共剪了八百多句，效果不是特别好，不过还能将就，针对皮皮梦的音源效果会好一点。 下面是图 > 请问你是怎么在现有模型的基础上继续训练的？就把模型文件放到自己要训练的模型文件目录，然后模型的名字也改成自己设置的就好了 
The Question About  语音转换Voice Conversion(PPG based)
In ppg2mel.yaml，there is no details  about （train_fid_list：）which is in second line. hope your replyyou need to replace it with the file in result folder of preprocessing@babysor  Thanks for your reply, I will try. 
帮忙看一下，训练到了57k,还没有收敛，是不是废了？

搭建环境，最后一步MODULE FILES NOT FOUND, 这个怎么解决？多谢
Synthesizer/Saved_models 里面已经放了下载的模型，重启也是了，还是一样的提示 
【小白提问】没有GPU的机器总是显示没有torch包
**Summary[问题简述（一句话）]没有GPU的机器总是显示没有torch包，如题，电脑没有英伟达显卡，故在安装pytorch时选择了CPU版，如图。在环境安装后，python环境下输入import pytorch不报错，但输入torch.cuda.is_available（）返回False尝试安装其余版本（如新手教程中版本，仍然如此）—其余操作都完成的情况下，在主程序目录下尝试强行python demo_toolbox.py -d demo_toolbox.py -d .\samplesTraceback (most recent call last):  File "D:\MockingBird-main\demo_toolbox.py", line 2, in <module>    from toolbox import Toolbox  File "D:\MockingBird-main\toolbox\__init__.py", line 1, in <module>    from toolbox.ui import UI  File "D:\MockingBird-main\toolbox\ui.py", line 7, in <module>    from encoder.inference import plot_embedding_as_heatmap  File "D:\MockingBird-main\encoder\inference.py", line 2, in <module>    from encoder.model import SpeakerEncoder  File "D:\MockingBird-main\encoder\model.py", line 5, in <module>    from torch.nn.utils import clip_grad_norm_ModuleNotFoundError: No module named 'torch'没有cuda得情况下也要确保torch已经安装好谢谢！已经解决了，重新装了一遍就不报错了... 
求助，预处理失败，请帮忙看一下
D:\Aria2\Sound_File_Processing-master\Sound_File_Processing-master>python pre.py D:\Aria2\aidatatang_200zh -d aidatatang_200zh -n 10python: can't open file 'D:\Aria2\Sound_File_Processing-master\Sound_File_Processing-master\pre.py': [Errno 2] No such file or directory 
web端无法正常合成
**Summary[问题简述（一句话）]**web端无法正常合成**Env & To Reproduce[复现与环境]**Linux **** SMP Tue Jun 1 16:14:33 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux版本：commit 3fe0690cc6c3b88a77c6036ac09a1fbbedeac3f9模型：ceshi.pf**错误信息**[2022-03-14 15:07:39,920] ERROR in app: Exception on /api/synthesize [POST]Traceback (most recent call last):  File "/usr/local/lib/python3.9/site-packages/flask/app.py", line 2073, in wsgi_app    response = self.full_dispatch_request()  File "/usr/local/lib/python3.9/site-packages/flask/app.py", line 1518, in full_dispatch_request    rv = self.handle_user_exception(e)  File "/usr/local/lib/python3.9/site-packages/flask_restx/api.py", line 672, in error_router    return original_handler(e)  File "/usr/local/lib/python3.9/site-packages/flask/app.py", line 1516, in full_dispatch_request    rv = self.dispatch_request()  File "/usr/local/lib/python3.9/site-packages/flask/app.py", line 1502, in dispatch_request    return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args)  File "/root/RemoteWorking/web/__init__.py", line 108, in synthesize    specs = current_synt.synthesize_spectrograms(texts, embeds)  File "/root/RemoteWorking/synthesizer/inference.py", line 93, in synthesize_spectrograms    self.load()  File "/root/RemoteWorking/synthesizer/inference.py", line 71, in load    self._model.load(self.model_fpath, self.device)  File "/root/RemoteWorking/synthesizer/models/tacotron.py", line 547, in load    self.load_state_dict(checkpoint["model_state"], strict=False)  File "/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1497, in load_state_dict    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).        size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]).        size mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([384, 1280]).        size mismatch for decoder.rnn_input.weight: copying a param with shape torch.Size([1024, 640]) from checkpoint, the shape in current model is torch.Size([1024, 1152]).        size mismatch for decoder.stop_proj.weight: copying a param with shape torch.Size([1, 1536]) from checkpoint, the shape in current model is torch.Size([1, 2048]).切换到 tag0.0.1分支。切换后，时而正常，时而异常，错误如下：RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).#37 
大家好，请问在打开web后点击录制出现【Uncaught Error】“recStart”未定义...是存在什么问题，应该怎么解决呢？ 
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型**Screenshots[截图（如有）]*![This is an applicable, add screenshots to help重复问题 
大家好，请问在打开web后点击录制出现【Uncaught Error】“recStart”未定义...是存在什么问题，应该怎么解决呢？
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型**Screenshots[截图（如有）]**If applicable, add screenshots to help 
'pip' 不是内部或外部命令，也不是可运行的程序 或批处理文件。
**Summary[问题简述（一句话）]**A clear and concise description of what the issue is.pip install -r requirements.txt 导入不了**Env & To Reproduce[复现与环境]**描述你用的环境、代码版本、模型C:\Users\1\Desktop\al\MockingBird-main>pip install -r requirements.txt'pip' 不是内部或外部命令，也不是可运行的程序或批处理文件。**Screenshots[截图（如有）]**If applicable, add screenshots to 下载后执行setup.py如果没装Python先装PythonPIP都不知道，玩这个，还是很困难的。 
在运行工具箱合成声音时出现了这个提示是怎么回事呀
Traceback (most recent call last):  File "D:\ProgramData\Anaconda3\envs\Pytorch\lib\site-packages\flask\app.py", line 2073, in wsgi_app    response = self.full_dispatch_request()  File "D:\ProgramData\Anaconda3\envs\Pytorch\lib\site-packages\flask\app.py", line 1518, in full_dispatch_request    rv = self.handle_user_exception(e)  File "D:\ProgramData\Anaconda3\envs\Pytorch\lib\site-packages\flask_restx\api.py", line 672, in error_router    return original_handler(e)  File "D:\ProgramData\Anaconda3\envs\Pytorch\lib\site-packages\flask\app.py", line 1516, in full_dispatch_request    rv = self.dispatch_request()  File "D:\ProgramData\Anaconda3\envs\Pytorch\lib\site-packages\flask\app.py", line 1502, in dispatch_request    return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args)  File "D:\PythonProject\Pytorch\MockingBird_main\web\__init__.py", line 108, in synthesize    specs = current_synt.synthesize_spectrograms(texts, embeds)  File "D:\PythonProject\Pytorch\MockingBird_main\synthesizer\inference.py", line 128, in synthesize_spectrograms    _, mels, alignments = self._model.generate(chars, speaker_embeddings, style_idx=style_idx, min_stop_token=min_stop_token, steps=steps)  File "D:\PythonProject\Pytorch\MockingBird_main\synthesizer\models\tacotron.py", line 448, in generate    encoder_seq = self.encoder(x, speaker_embedding)  File "D:\ProgramData\Anaconda3\envs\Pytorch\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl    return forward_call(*input, **kwargs)  File "D:\PythonProject\Pytorch\MockingBird_main\synthesizer\models\tacotron.py", line 41, in forward    x = self.cbhg(x)  File "D:\ProgramData\Anaconda3\envs\Pytorch\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl    return forward_call(*input, **kwargs)  File "D:\PythonProject\Pytorch\MockingBird_main\synthesizer\models\tacotron.py", line 138, in forward    c = conv(x) # Convolution  File "D:\ProgramData\Anaconda3\envs\Pytorch\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl    return forward_call(*input, **kwargs)  File "D:\PythonProject\Pytorch\MockingBird_main\synthesizer\models\tacotron.py", line 84, in forward    x = self.conv(x)  File "D:\ProgramData\Anaconda3\envs\Pytorch\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl    return forward_call(*input, **kwargs)  File "D:\ProgramData\Anaconda3\envs\Pytorch\lib\site-packages\torch\nn\modules\conv.py", line 301, in forward    return self._conv_forward(input, self.weight, self.bias)  File "D:\ProgramData\Anaconda3\envs\Pytorch\lib\site-packages\torch\nn\modules\conv.py", line 298, in _conv_forward    self.padding, self.dilation, self.groups)RuntimeError: Unable to find a valid cuDNN algorithm to run convolution直接用的大佬的模型pytorch安装正常了吗？显卡是不是不支持cuda应该是没有问题，之前自己训练模型的时候也用的cuda训练成功了，这次不知道为什么------------------&nbsp;原始邮件&nbsp;------------------发件人: ***@***.***&gt;; 发送时间: 2022年3月12日(星期六) 晚上7:12收件人: ***@***.***&gt;; 抄送: "Yuzhu ***@***.***&gt;; ***@***.***&gt;; 主题: Re: [babysor/MockingBird] 在运行工具箱合成声音时出现了这个提示是怎么回事呀 (Issue #450) pytorch安装正常了吗？显卡是不是不支持cuda —Reply to this email directly, view it on GitHub, or unsubscribe.Triage notifications on the go with GitHub Mobile for iOS or Android. You are receiving this because you authored the thread.Message ID: ***@***.***&gt;看报错，是pytorch出了问题，每次必现吗？不是欸，使用了您发布的老版本就没有问题，我运行新版本会报这个错误------------------&nbsp;原始邮件&nbsp;------------------发件人: ***@***.***&gt;; 发送时间: 2022年3月12日(星期六) 晚上7:29收件人: ***@***.***&gt;; 抄送: "Yuzhu ***@***.***&gt;; ***@***.***&gt;; 主题: Re: [babysor/MockingBird] 在运行工具箱合成声音时出现了这个提示是怎么回事呀 (Issue #450) 看报错，是pytorch出了问题，每次必现吗？ —Reply to this email directly, view it on GitHub, or unsubscribe.Triage notifications on the go with GitHub Mobile for iOS or Android. You are receiving this because you authored the thread.Message ID: ***@***.***&gt; 
训练一段时间后报错
经常训练到1200步左右的时候就报错：Traceback (most recent call last):  File "synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "E:\MockingBird-main\synthesizer\train.py", line 180, in train    for i, (texts, mels, embeds, idx) in enumerate(data_loader, 1):  File "E:\python3.7\lib\site-packages\torch\utils\data\dataloader.py", line 352, in __iter__    return self._get_iterator()  File "E:\python3.7\lib\site-packages\torch\utils\data\dataloader.py", line 294, in _get_iterator    return _MultiProcessingDataLoaderIter(self)  File "E:\python3.7\lib\site-packages\torch\utils\data\dataloader.py", line 801, in __init__    w.start()  File "E:\python3.7\lib\multiprocessing\process.py", line 112, in start    self._popen = self._Popen(self)  File "E:\python3.7\lib\multiprocessing\context.py", line 223, in _Popen    return _default_context.get_context().Process._Popen(process_obj)  File "E:\python3.7\lib\multiprocessing\context.py", line 322, in _Popen    return Popen(process_obj)  File "E:\python3.7\lib\multiprocessing\popen_spawn_win32.py", line 89, in __init__    reduction.dump(process_obj, to_child)  File "E:\python3.7\lib\multiprocessing\reduction.py", line 60, in dump    ForkingPickler(file, protocol).dump(obj)  File "E:\python3.7\lib\multiprocessing\synchronize.py", line 104, in __getstate__    h = context.get_spawning_popen().duplicate_for_child(sl.handle)  File "E:\python3.7\lib\multiprocessing\popen_spawn_win32.py", line 95, in duplicate_for_child    return reduction.duplicate(handle, self.sentinel)  File "E:\python3.7\lib\multiprocessing\reduction.py", line 77, in duplicate    0, inheritable, _winapi.DUPLICATE_SAME_ACCESS)PermissionError: [WinError 5] 拒绝访问。PS E:\MockingBird-main> Traceback (most recent call last):  File "<string>", line 1, in <module>  File "E:\python3.7\lib\multiprocessing\spawn.py", line 105, in spawn_main    exitcode = _main(fd)  File "E:\python3.7\lib\multiprocessing\spawn.py", line 115, in _main    self = reduction.pickle.load(from_parent)EOFError: Ran out of input每次都复现吗？看看调小一下batchsize> 每次都复现吗？看看调小一下batchsize是的，每次都复现，偶尔400多步的时候就会出现；3090  batchsize设置96，现在设置成12测试96有点玄乎，你先试试> 96有点玄乎，你先试试{| Epoch: 234/13750 (4/4) | Loss: 0.2344 | 0.43 steps/s | Step: 105k | }{| Epoch: 235/13750 (4/4) | Loss: 0.2343 | 0.43 steps/s | Step: 105k | }{| Epoch: 236/13750 (4/4) | Loss: 0.2342 | 0.43 steps/s | Step: 105k | }{| Epoch: 237/13750 (4/4) | Loss: 0.2342 | 0.43 steps/s | Step: 105k | }Traceback (most recent call last):  File "synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "C:\MachineLearning_Data\MockingBird-main\synthesizer\train.py", line 209, in train    loss.backward()  File "d:\app\Anaconda3\envs\faceswap\lib\site-packages\torch\_tensor.py", line 307, in backward    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)  File "d:\app\Anaconda3\envs\faceswap\lib\site-packages\torch\autograd\__init__.py", line 156, in backward    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flagRuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling 我也是训练一段时间后报错，bs是96，调成32也一样，显存22g> > 96有点玄乎，你先试试> > {| Epoch: 234/13750 (4/4) | Loss: 0.2344 | 0.43 steps/s | Step: 105k | } {| Epoch: 235/13750 (4/4) | Loss: 0.2343 | 0.43 steps/s | Step: 105k | } {| Epoch: 236/13750 (4/4) | Loss: 0.2342 | 0.43 steps/s | Step: 105k | } {| Epoch: 237/13750 (4/4) | Loss: 0.2342 | 0.43 steps/s | Step: 105k | } Traceback (most recent call last): File "synthesizer_train.py", line 37, in train(**vars(args)) File "C:\MachineLearning_Data\MockingBird-main\synthesizer\train.py", line 209, in train loss.backward() File "d:\app\Anaconda3\envs\faceswap\lib\site-packages\torch_tensor.py", line 307, in backward torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs) File "d:\app\Anaconda3\envs\faceswap\lib\site-packages\torch\autograd__init__.py", line 156, in backward allow_unreachable=True, accumulate_grad=True) # allow_unreachable flag RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling > > 我也是训练一段时间后报错，bs是96，调成32也一样，显存22g你这个报错我之前也有出现过，后来发现是因为预处理和数据集不对造成的，你可以检查一下，预处理和数据集，或者重新预处理一下，看看还会不会报错，我现在不报这个错误了，开始报主帖上的错误> > > 96有点玄乎，你先试试> > > > > > {| Epoch: 234/13750 (4/4) | Loss: 0.2344 | 0.43 steps/s | Step: 105k | } {| Epoch: 235/13750 (4/4) | Loss: 0.2343 | 0.43 steps/s | Step: 105k | } {| Epoch: 236/13750 (4/4) | Loss: 0.2342 | 0.43 steps/s | Step: 105k | } {| Epoch: 237/13750 (4/4) | Loss: 0.2342 | 0.43 steps/s | Step: 105k | } Traceback (most recent call last): File "synthesizer_train.py", line 37, in train(**vars(args)) File "C:\MachineLearning_Data\MockingBird-main\synthesizer\train.py", line 209, in train loss.backward() File "d:\app\Anaconda3\envs\faceswap\lib\site-packages\torch_tensor.py", line 307, in backward torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs) File "d:\app\Anaconda3\envs\faceswap\lib\site-packages\torch\autograd__init__.py", line 156, in backward allow_unreachable=True, accumulate_grad=True) # allow_unreachable flag RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling > > 我也是训练一段时间后报错，bs是96，调成32也一样，显存22g> > 你这个报错我之前也有出现过，后来发现是因为预处理和数据集不对造成的，你可以检查一下，预处理和数据集，或者重新预处理一下，看看还会不会报错，我现在不报这个错误了，开始报主帖上的错误我尼玛刚说完就报了一个和你一模一样的错误，你有毒吧而且还会感染的那种-.-> > > > 96有点玄乎，你先试试> > > > > > > > > {| Epoch: 234/13750 (4/4) | Loss: 0.2344 | 0.43 steps/s | Step: 105k | } {| Epoch: 235/13750 (4/4) | Loss: 0.2343 | 0.43 steps/s | Step: 105k | } {| Epoch: 236/13750 (4/4) | Loss: 0.2342 | 0.43 steps/s | Step: 105k | } {| Epoch: 237/13750 (4/4) | Loss: 0.2342 | 0.43 steps/s | Step: 105k | } Traceback (most recent call last): File "synthesizer_train.py", line 37, in train(**vars(args)) File "C:\MachineLearning_Data\MockingBird-main\synthesizer\train.py", line 209, in train loss.backward() File "d:\app\Anaconda3\envs\faceswap\lib\site-packages\torch_tensor.py", line 307, in backward torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs) File "d:\app\Anaconda3\envs\faceswap\lib\site-packages\torch\autograd__init__.py", line 156, in backward allow_unreachable=True, accumulate_grad=True) # allow_unreachable flag RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling > > > 我也是训练一段时间后报错，bs是96，调成32也一样，显存22g> > > > > > 你这个报错我之前也有出现过，后来发现是因为预处理和数据集不对造成的，你可以检查一下，预处理和数据集，或者重新预处理一下，看看还会不会报错，我现在不报这个错误了，开始报主帖上的错误> > 我尼玛刚说完就报了一个和你一模一样的错误，你有毒吧而且还会感染的那种-.-前天去了天河城，昨天核酸检测，今天阴性问题不大 
再次训练，报AttributeError: 'HParams' object has no attribute 'dumpJson'错

无法启动web
Loaded synthesizer models: 0Loaded encoder "pretrained.pt" trained to step 1594501Building Wave-RNNTrainable Parameters: 4.481MLoading model weights at vocoder\saved_models\pretrained\pretrained.ptTraceback (most recent call last):  File "C:\Users\98212\MockingBird\web.py", line 6, in <module>    app = webApp()  File "C:\Users\98212\MockingBird\web\__init__.py", line 35, in webApp    gan_vocoder.load_model(Path("vocoder/saved_models/pretrained/g_hifigan.pt"))TypeError: load_model() missing 1 required positional argument: 'config_fpath'下载最新版本试试https://github.com/babysor/MockingBird/archive/refs/heads/main.zip最新版本也有同樣的問題，還是不能啟動web而且一直出現ModuleNotFoundError, 請大佬救救！Loaded synthesizer models: 1Traceback (most recent call last):  File "D:\MockingBird-main\MockingBird-main\web.py", line 1, in <module>    from web import webApp  File "D:\MockingBird-main\MockingBird-main\web\__init__.py", line 3, in <module>    from gevent import pywsgi as wsgiModuleNotFoundError: No module named 'gevent'好像还是一样(base) PS C:\Users\98212> cd .\MockingBird-main\(base) PS C:\Users\98212\MockingBird-main> python web.pyLoaded synthesizer models: 0Loaded encoder "pretrained.pt" trained to step 1594501Building Wave-RNNTrainable Parameters: 4.481MLoading model weights at vocoder\saved_models\pretrained\pretrained.ptTraceback (most recent call last):  File "C:\Users\98212\MockingBird-main\web.py", line 6, in <module>    app = webApp()  File "C:\Users\98212\MockingBird-main\web\__init__.py", line 35, in webApp    gan_vocoder.load_model(Path("vocoder/saved_models/pretrained/g_hifigan.pt"))TypeError: load_model() missing 1 required positional argument: 'config_fpath'> 好像还是一样 (base) PS C:\Users\98212> cd .\MockingBird-main (base) PS C:\Users\98212\MockingBird-main> python web.py Loaded synthesizer models: 0 Loaded encoder "pretrained.pt" trained to step 1594501 Building Wave-RNN Trainable Parameters: 4.481M Loading model weights at vocoder\saved_models\pretrained\pretrained.pt Traceback (most recent call last): File "C:\Users\98212\MockingBird-main\web.py", line 6, in app = webApp() File "C:\Users\98212\MockingBird-main\web__init__.py", line 35, in webApp gan_vocoder.load_model(Path("vocoder/saved_models/pretrained/g_hifigan.pt")) TypeError: load_model() missing 1 required positional argument: 'config_fpath'把vocoder\hifigan\inference.py第22行从 改成 我更正代码一下，感谢楼上 
使用WaveRNN报错
报错如下> Traceback (most recent call last):  File "D:\Apps\Anaconda3\lib\site-packages\flask\app.py", line 2447, in wsgi_app    response = self.full_dispatch_request()  File "D:\Apps\Anaconda3\lib\site-packages\flask\app.py", line 1952, in full_dispatch_request    rv = self.handle_user_exception(e)  File "D:\Apps\Anaconda3\lib\site-packages\flask_restx\api.py", line 672, in error_router    return original_handler(e)  File "D:\Apps\Anaconda3\lib\site-packages\flask\app.py", line 1821, in handle_user_exception    reraise(exc_type, exc_value, tb)  File "D:\Apps\Anaconda3\lib\site-packages\flask\_compat.py", line 39, in reraise    raise value  File "D:\Apps\Anaconda3\lib\site-packages\flask\app.py", line 1950, in full_dispatch_request    rv = self.dispatch_request()  File "D:\Apps\Anaconda3\lib\site-packages\flask\app.py", line 1936, in dispatch_request    return  File "D:\Users\Jerry\Documents\Jerry\MockingBird-main\web\__init__.py", line 118, in synthesize    write(out, sample_rate, wav.astype(np.float32))AttributeError: 'tuple' object has no attribute 'astype'127.0.0.1 - - [2022-03-09 14:13:44] "POST /api/synthesize HTTP/1.1" 500 426 14.539417@JerryZRF 你解决了吗？看起来像一个小bug，有同学能帮忙修一下吗？ 
求助执行requirements.txt时报No module named 'pyworld'是什么问题？？
已是最新代码E:\MockingBird\MockingBird>python demo_toolbox.pyTraceback (most recent call last):  File "E:\MockingBird\MockingBird\demo_toolbox.py", line 2, in <module>    from toolbox import Toolbox  File "E:\MockingBird\MockingBird\toolbox\__init__.py", line 9, in <module>    from utils.f0_utils import compute_f0, f02lf0, compute_mean_std, get_converted_lf0uv  File "E:\MockingBird\MockingBird\utils\f0_utils.py", line 3, in <module>    import pyworldModuleNotFoundError: No module named 'pyworld'BUG 待修复 pip安装pyworld不就可以了pip pyworld安装失败是的，使用  单独去安装也是失败，可能是我电脑环境的问题，这时我尝试去装  的历史版本竟然成功了https://pypi.org/project/pyworld/#history 我试试------------------&nbsp;原始邮件&nbsp;------------------发件人:                                                                                                                        "babysor/MockingBird"                                                                                    ***@***.***&gt;;发送时间:&nbsp;2022年3月10日(星期四) 晚上6:10***@***.***&gt;;***@***.******@***.***&gt;;主题:&nbsp;Re: [babysor/MockingBird] 求助执行requirements.txt时报No module named 'pyworld'是什么问题？？ (Issue #444) 是的，使用 pip install pyworld 单独去安装也是失败，可能是我电脑环境的问题，这时我尝试的全装  pyworld 的历史版本竟然成功了 https://pypi.org/project/pyworld/#history pip install pyworld==0.2.12 —Reply to this email directly, view it on GitHub, or unsubscribe.Triage notifications on the go with GitHub Mobile for iOS or Android. You are receiving this because you are subscribed to this thread.Message ID: ***@***.***&gt;> 0.2.12> > 0.2.12好的> > 0.2.12还是安装失败，错误为ERROR: Command errored out with exit status 1:   command: 'D:\Anaconda\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '"'"'C:\\Users\\树林耶\\AppData\\Local\\Temp\\pip-install-vlnx96ap\\pyworld_ba3f67a25ede499eb3eb0fdbc186f129\\setup.py'"'"'; __file__='"'"'C:\\Users\\树林耶\\AppData\\Local\\Temp\\pip-install-vlnx96ap\\pyworld_ba3f67a25ede499eb3eb0fdbc186f129\\setup.py'"'"';f = getattr(tokenize, '"'"'open'"'"', open)(__file__) if os.path.exists(__file__) else io.StringIO('"'"'from setuptools import setup; setup()'"'"');code = f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' bdist_wheel -d 'C:\Users\树林耶\AppData\Local\Temp\pip-wheel-duqz_bk4'       cwd: C:\Users\树林耶\AppData\Local\Temp\pip-install-vlnx96ap\pyworld_ba3f67a25ede499eb3eb0fdbc186f129\  Complete output (13 lines):  D:\Anaconda\lib\site-packages\setuptools\dist.py:717: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead    warnings.warn(  running bdist_wheel  running build  running build_py  creating build  creating build\lib.win-amd64-3.9  creating build\lib.win-amd64-3.9\pyworld  copying pyworld\__init__.py -> build\lib.win-amd64-3.9\pyworld  running build_ext  skipping 'pyworld\pyworld.cpp' Cython extension (up-to-date)  building 'pyworld.pyworld' extension  error: Microsoft Visual C++ 14.0 or greater is required. Get it with "Microsoft C++ Build Tools":  ----------------------------------------  ERROR: Failed building wheel for pyworld试试这个 安装c++编译工具后再pip install pyworld我安装完cppbuildtools后可以pip了我也好了> 试试这个 [https://blog.csdn.net/vans05/article/details/116994443 安装c++编译工具后再pip install pyworld差点给我C盘炸了，还是ERROR: Could not build wheels for pyworld, which is required to install pyproject.toml-based projectsOSError: Failed to open file b'C:\\Users\\\xe6\xa0\x91\xe6\x9e\x97\xe8\x80\xb6\\AppData\\Local\\Temp\\scipy-xa_hg05d'这个错误需要怎么修改非常感谢，已经解决 
训练synthesizer的时候中途出现了 warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")，请问是什么问题？
在网上查找的时候发现了这个问题会降低训练速度 ，请问这对速度有影响吗？我使用的是980ti，显存6g，满载速度是0.55 steps/s另外，此问题并不影响训练，只是中途出现过几次，所以来问一下，训练模型与进度一切正常大致查找了一下，是需要把nn.functional.tanh替换为torcht.tanh对吗？请问是需要替换哪些代码呢？代码库搜一下，好像最新的代码没有这个问题了吧？有的话方便修一下吗？我是大约两个星期前的代码，我去试一下更新，感谢 
问一下，这5个参数各起到啥作用，感觉调来调去有点作用但不是很明显
同问同问同问有答案了没？ 
输出结果只有杂音
#209重复问题 #218 
【长期】跨语言支持
### 已有讨论#142 #197### Q & “你好” 变成 “ni2 hao3”，只需要找到一个tts前端处理一下日语为phenomenon> 作者却苦于近期精力限制只能势单力薄处理一些小的bug，也看到issue区有不少爱好与开发者想要学习或二次改造更好满足自己需求，不过比较零碎难以展开。为了让项目和AI持续可以给大家提供更多价值，共同学习，我在issue区根据不同主题创建长期交流频道，若留言人数超过20也将建立对应交流群。> - 如何改参数，搞出更逼真的克隆效果 435> - 如何改模型，搞出更好效果 436> - 训练克隆特定人声音&finetune 437> - 学术/论文讨论/训练分析 438> - 跨语言支持  440> - 工程化/新场景讨论（绝不做恶 & 合法合规） 439 感觉这样做出来会很强大，还自带翻译功能，会更复杂关于日语方面，我已经找到了对应的tts前端，配合自己写的java脚本对日语处理为phenomenon，可以将一个wav文件夹下所有的语音和文本生成对应的alignment格式。一个是将日语的所有汉字等转换为片假名（类似于中文的拼音），另一个是输入对应的语音和txt文本，输出对应的每个拼音的时间。最后用java脚本生成alignment.txt文件。tts前端：链接: 提取码: 6f5s 关于日语方面，我已经找到了对应的tts前端，配合自己写的java脚本对日语处理为phenomenon，可以将一个wav文件夹下所有的语音和文本生成对应的alignment格式。一个是将日语的所有汉字等转换为片假名（类似于中文的拼音），另一个是输入对应的语音和txt文本，输出对应的每个拼音的时间。最后用java脚本生成alignment.txt文件。 tts前端：链接: 提取码: 6f5s 复制这段内容后打开百度网盘手机App，操作更方便哦 包括两个文件japankana和segmentation-kit2 第一个japankana使用比较简单，打开后输入就行了，可以将包括汉字的日语转换为片假名 第二个操作方法请参考里面的readme.txt> > 我已经开始尝试训练了50k步，但日语效果并不是很好。基本只有前面的两个词左右能够识别输出，剩下的都是语音和输入文本对应不上。目前数据集是单独一个人的日语语音10个小时左右，可能是因为数据集太小的原因所以效果不好。**如果有日语的较多较好音频和对应的文字文本训练集或是有相关训练经验，欢迎一起探讨学习。**赞链接：https://pan.baidu.com/s/1eBX12_eJR8TvED15DdfD0g?pwd=1111 提取码：1111我不知道gui如何上传音频文件 所以使用百度网盘上传这是我训练的效果，我相信相同的步骤可以应用于其他语言第一步 修改MockingBird-main\synthesizer\utils\symbols.py 所以加入了所有的片假名第二步 修改一个日语数据集，使其符合aidatatang_200zh, magicdata, aishell3, 我只训练了20k步 更长时间的训练应该能获得更好的效果）> 链接：https://pan.baidu.com/s/1eBX12_eJR8TvED15DdfD0g?pwd=1111 提取码：1111 我不知道gui如何上传音频文件 所以使用百度网盘上传 这是我训练的效果，我相信相同的步骤可以应用于其他语言 第一步 修改MockingBird-main\synthesizer\utils\symbols.py 中的参数 这里我是想训练日语tts 所以加入了所有的片假名 第二步 修改一个日语数据集，使其符合aidatatang_200zh, magicdata, aishell3, data_aishell的格式 第三步用数据集训练合成器（我认为只训练合成器就可以达到较好的效果，如果训练声码器和编码器效果应该会更好？） （时间原因 我只训练了20k步 更长时间的训练应该能获得更好的效果）我们用的是同一个数据集，都是英伟达的common voice。你直接用片假名训练的，我则是把片假名再进一步转换为罗马音进行训练。现在是90k step，效果只能说一般，部分文字还是识别不出来。可能是数据集大小还不够> > 链接：https://pan.baidu.com/s/1eBX12_eJR8TvED15DdfD0g?pwd=1111码提取：1111 我不知道如何上传音频文件，使用百度网盘上传的步骤相同句号修改其他语言 tt 0 aidata0zh， magicdata、aishell3、data_aishell > 我们用的是，都是英伟的普通话。你直接用片假名训练的，我把同一个片假名再进一步转换为罗马达音进行。现在是90k步，效果只能说一般，部分文字还是识别不出来。我对日语并不熟悉，但或许片假/平假对tts来说更易拟合？ 我对目前训练的效果还是满意的 毕竟日语的大型数据集太难找了我接下来会用jsut 和 jvs混合训练试一试> > > 我不知道如何上传音频文件，使用百度网盘上传的步骤相同句号修改其他语言 tt 0 aidata0zh， magicdata、aishell3、data_aishell > > > > > 我们用的是，都是英伟的普通话。你直接用片假名训练的，我把同一个片假名再进一步转换为罗马达音进行。现在是90k步，效果只能说一般，部分文字还是识别不出来。> > 我对日语并不熟悉，但或许片假/平假对tts来说更易拟合？ 我对目前训练的效果还是满意的 日语的大型数据集太难找了那我今天根据片假名再重新开始训练好了。日语的数据集还是挺多的比如LaboroTVSpeech和Corpus of Spontaneous Japanese等，都有几百个小时。但前者申请需要国内大学的老师或者日本当地大学的学生、后者一个数据集2000rmb，对于个人兴趣负担还是太大了。方便的话可以分享一下你训练的模型吗，我已经把我训练的上传到issue了。我训练到90k的时候会分享的，感觉loss还有下降的空间> 所以我没有划分单词时间链接：https://pan.baidu.com/s/1_qn8nL7AKbAcBrmH8TcptQ?pwd=1111 提取码：1111> 神经网络似乎可以自己对单词的时间进行划分 所以我没有划分单词时间 链接：https://pan.baidu.com/s/1_qn8nL7AKbAcBrmH8TcptQ?pwd=1111 提取码：1111感谢 
【长期】工程化/新场景讨论（绝不做恶 & 合法合规）
### 作者下一步计划1. 淘汰QT5，选型新的技术框架2. 可视化每个layer推理结果### 跨平台支持安卓📱Android成功运行本项目 #358 M1芯片下安装包报错的暂时性解决方案 #65 能否部署到树莓派运行 #431容器化 作者却苦于近期精力限制只能势单力薄处理一些小的bug，也看到issue区有不少爱好与开发者想要学习或二次改造更好满足自己需求，不过比较零碎难以展开。为了让项目和AI持续可以给大家提供更多价值，共同学习，我在issue区根据不同主题创建长期交流频道，若留言人数超过20也将建立对应交流群。> - 如何改参数，搞出更逼真的克隆效果 435> - 如何改模型，搞出更好效果 436> - 训练克隆特定人声音&finetune 437> - 学术/论文讨论/训练分析 438> - 跨语言支持  440> - 工程化/新场景讨论（绝不做恶 & 合法合规） 439 我需要群> 我需要群有什么场景需求吗？能否将模型转换成onnx模型呢？方便部署> 能否将模型转换成onnx模型呢？方便部署看了一下是个open standard，是个不错的建议，adoption高吗？需要一个群 
【长期】学术/论文讨论/训练分析
> 作者却苦于近期精力限制只能势单力薄处理一些小的bug，也看到issue区有不少爱好与开发者想要学习或二次改造更好满足自己需求，不过比较零碎难以展开。为了让项目和AI持续可以给大家提供更多价值，共同学习，我在issue区根据不同主题创建长期交流频道，若留言人数超过20也将建立对应交流群。> - 如何改参数，搞出更逼真的克隆效果 435> - 如何改模型，搞出更好效果 436> - 训练克隆特定人声音&finetune 437> - 学术/论文讨论/训练分析 438> - 跨语言支持  440> - 工程化/新场景讨论（绝不做恶 & 合法合规） 439 留下第一条！请问克隆出来的语音有什么办法可以保存为wav文件吗> 请问克隆出来的语音有什么办法可以保存为wav文件吗这个有点不太扣题，我回了你的issue了> 我也想知道 需要用其他软件再录下来吗 感觉好麻烦> > > > 我也想知道 需要用其他软件再录下来吗 感觉好麻烦软件界面就有直接导出保存的> > > > 我也想知道 需要用其他软件再录下来吗 感觉好麻烦Export按钮 
【长期】训练克隆特定人声音&finetune
#380> 作者却苦于近期精力限制只能势单力薄处理一些小的bug，也看到issue区有不少爱好与开发者想要学习或二次改造更好满足自己需求，不过比较零碎难以展开。为了让项目和AI持续可以给大家提供更多价值，共同学习，我在issue区根据不同主题创建长期交流频道，若留言人数超过20也将建立对应交流群。> - 如何改参数，搞出更逼真的克隆效果 435> - 如何改模型，搞出更好效果 436> - 训练克隆特定人声音&finetune 437> - 学术/论文讨论/训练分析 438> - 跨语言支持  440> - 工程化/新场景讨论（绝不做恶 & 合法合规） 439 fine-tune的话大概需要多少数据才能使音色比较相似最好是小时级别得完全小白尝试，我想训练比如原神里安柏的声音，但发现训练出来都不像，可能因为声优配音通常都不是平调，会带有很多感情语气吧，不知道收集所有配音做个数据集后再训练会不会好一点？、像 完全小白尝试，我想训练比如原神里安柏的声音，但发现训练出来都不像，可能因为声优配音通常都不是平调，会带有很多感情语气吧，不知道收集所有配音做个数据集后再训练会不会好一点？、像 #460 那样进行特化训练？，有模型分享更集中一点的平台吗？自己大概是训练不出来了，碰碰运气搞一搞 目前，除了使用默认数据集Dataset、更改模型Synthesizer和Browse加入某个音频以外，其他栏目的作用和影响并不是很清楚看起来你都还没训练把？是的，我想我得先换个电脑，只不过想事先了解一下可行性，或许还缺了亿点点学习时间我想，配音数据过少的情况下，我或许可以把合成出来的某些句子，依靠自己的听力判断是否相像，挑出其中相像的拿来训练，循环往复，是不是就越来越接近了> 我想，配音数据过少的情况下，我或许可以把合成出来的某些句子，依靠自己的听力判断是否相像，挑出其中相像的拿来训练，循环往复，是不是就越来越接近了效率会比较低 可以看下一楼的视频教程> > 我想，配音数据过少的情况下，我或许可以把合成出来的某些句子，依靠自己的听力判断是否相像，挑出其中相像的拿来训练，循环往复，是不是就越来越接近了> > 效率会比较低 > > 我想，配音数据过少的情况下，我或许可以把合成出来的某些句子，依靠自己的听力判断是否相像，挑出其中相像的拿来训练，循环往复，是不是就越来越接近了> > > > > > 效率会比较低 可以看下一楼的视频教程> > 按视频做了，请问这几张图是否代表第二张图效果最好？那我要如何把模型回退到这一步?到群里获得解答了，默认每100k会生成一个新的pt，若要缩短间隔，需要自己改代码配置您好，请问hifi-gan的电音问题可以通过微调解决吗预置的vocoder是在预测mels上训练的吗我看代码应该是在ground truth mels指导下生成的mels上训练的，而不是纯预测的mels上训练的---------------------------------------------------------------------------------------------我用目标语音微调自己的synthesizer后，hifigan的电音竟然也消失了目前我发现关于文本的标注中的词之间的停顿都没有加入模型进行训练，是这部分不好处理吗。因为现在克隆出的人声停顿还不太好。> 目前我发现关于文本的标注中的词之间的停顿都没有加入模型进行训练，是这部分不好处理吗。因为现在克隆出的人声停顿还不太好。理论上可以加入的，预处理部分改下代码应该就可以了，有木有兴趣验证一下？可以帮忙找机器跑。另外注意symbols得覆盖停顿标志> > 目前我发现关于文本的标注中的词之间的停顿都没有加入模型进行训练，是这部分不好处理吗。因为现在克隆出的人声停顿还不太好。> > 理论上可以加入的，预处理部分改下代码应该就可以了，有木有兴趣验证一下？可以帮忙找机器跑。另外注意symbols得覆盖停顿标志预处理怎么加入对停顿的处理呢，我目前用%进行停顿标注，在symbols中覆盖了%的标注进行微调。关于克隆特定人声音的问题1、是用主页提供的模型接着训练，还是自己从零训练好些？2、自己准备数据集，大概需要多丰富的数据效果才能好？ 
【长期】如何改模型，搞出更好效果
### 合成器Synthesizer将 synthesizer部分换为 tacotron2 详见 作者却苦于近期精力限制只能势单力薄处理一些小的bug，也看到issue区有不少爱好与开发者想要学习或二次改造更好满足自己需求，不过比较零碎难以展开。为了让项目和AI持续可以给大家提供更多价值，共同学习，我在issue区根据不同主题创建长期交流频道，若留言人数超过20也将建立对应交流群。> - 如何改参数，搞出更逼真的克隆效果 435> - 如何改模型，搞出更好效果 436> - 训练克隆特定人声音&finetune 437> - 学术/论文讨论/训练分析 438> - 跨语言支持  440> - 工程化/新场景讨论（绝不做恶 & 合法合规） 439 声码器可以参考一下：❤❤❤SingGAN: Generative Adversarial Network For High-Fidelity Singing Voice Generation标题：SingGan：高保真歌声生成的生成性对抗性网络（声码器中加入F0输入，字节也这样做）链接：https://arxiv.org/abs/2110.07468演示：https://singgan.github.io/作者：Feiyang Chen,Rongjie Huang,Chenye Cui,Yi Ren,Jinglin Liu,Zhou Zhao,Nicholas Yuan,Baoxing Huai机构：Zhejiang University, Huawei Cloud备注：vocoder, generative adversarial network, singing voice synthesis摘要：由于超长的连续发音、高采样率和强的表现力，高保真歌唱语音合成对神经声码器来说是一项挑战。现有的用于文本到语音的神经声码器不能直接应用于歌唱语音合成，因为它们会导致生成的频谱图出现小故障，并且高频重建效果不佳。为了解决歌唱建模的困难，本文提出了一种具有生成对抗网络的歌唱声码器SingGAN。具体来说，1）SingGAN使用源激发来缓解谱图中的小故障问题；（字节跳动也这么弄）2）SingGAN采用多频带鉴别器，引入频域损耗和子带特征匹配损耗来监督高频重构。据我们所知，SingGAN是第一个设计用于高保真多扬声器歌唱语音合成的声码器。实验结果表明，与以前的方法相比，SingGAN合成的人声质量要高得多（0.41MOS增益）。进一步的实验表明，结合FastSpeech~2作为声学模型，SingGAN在歌唱语音合成管道中实现了很高的鲁棒性，并且在语音合成中表现良好。❤❤❤Multi-Singer: Fast Multi-Singer Singing Voice Vocoder With A Large-Scale Corpus标题：Multi-Singer：基于大规模语料的多发音人歌声声码器作者：Rongjie Huang, Feiyang Chen, Yi Ren, Jinglin Liu, Chenye Cui, Zhou Zhao代码：https://github.com/Rongjiehuang/Multi-Singer演示：https://multi-singer.github.io/#data摘要：高保真度多歌手歌唱语音合成由于歌唱语音数据不足、歌手泛化能力有限、计算量大等问题，对神经声码器来说是一个挑战。现有的开放语料库由于规模和质量的不足，无法满足高保真声乐合成的要求。以前的声码器在多歌手建模方面有困难，并且在进行看不见的歌手歌唱的声音生成时出现了明显的退化。为了加快社区对歌唱嗓音的研究，我们发布了一个大规模的、多歌手的中文歌唱嗓音数据集OpenSinger。为了解决隐形歌唱者建模的困难，我们提出了一种基于生成对抗网络的快速多歌唱者声码器Multi-Singer。具体来说，1)Multi-Singer使用Mulit Band genertor来加速训练和推理过程。2) Multi-Singer采用singer条件判别器和条件对抗训练目标，从声学特征(即mell -谱图)中获取并重建歌唱者身份。（字节跳动也是这样做的，必备模块）3)为了监督在频域频谱包络中歌唱者身份的重建，我们提出了一种辅助的歌唱者感知损失；联合训练方法是一种有效的多歌唱者语音建模方法。（声纹联合训练）实验结果验证了OpenSinger算法的有效性，表明Multi-Singer算法在速度和质量上都比以前的算法得到了提高。进一步的实验证明，Multi-Singer结合FastSpeech 2作为声学模型，在多singer歌唱语音合成流水线中具有较强的鲁棒性。通用声码必备技术：1，F0转换为激励，解决持续发音的断音；2，判别器加入speaker embedding;3，声纹损失约束；虽然我不太了解深度学习，但是我看到一些相关的文章。这个训练的optimizer本来是Adam，不知道换成NAdam或者AdamW或者Adamax有没有什么帮助 
【长期】如何改参数，搞出更逼真的克隆效果
待补充描述欢迎留言讨论> 作者却苦于近期精力限制只能势单力薄处理一些小的bug，也看到issue区有不少爱好与开发者想要学习或二次改造更好满足自己需求，不过比较零碎难以展开。为了让项目和AI持续可以给大家提供更多价值，共同学习，我在issue区根据不同主题创建长期交流频道，若留言人数超过20也将建立对应交流群。> - 如何改参数，搞出更逼真的克隆效果 435> - 如何改模型，搞出更好效果 436> - 训练克隆特定人声音&finetune 437> - 学术/论文讨论/训练分析 438> - 跨语言支持  440> - 工程化/新场景讨论（绝不做恶 & 合法合规） 439 留言，支持 
训练过程中报错
> {| Epoch: 1/1 (400/52340) | Loss: 0.4498 | 0.55 steps/s | Step: 27k | }Traceback (most recent call last):  File "D:\Users\Jerry\Documents\Jerry\MockingBird-main\synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "D:\Users\Jerry\Documents\Jerry\MockingBird-main\synthesizer\train.py", line 255, in train    eval_model(attention=np_now(attention[sample_idx][:, :attention_len]),  File "D:\Users\Jerry\Documents\Jerry\MockingBird-main\synthesizer\train.py", line 287, in eval_model    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)  File "D:\Users\Jerry\Documents\Jerry\MockingBird-main\synthesizer\audio.py", line 91, in inv_mel_spectrogram    S = _mel_to_linear(_db_to_amp(D + hparams.ref_level_db), hparams)  # Convert back to linear  File "D:\Users\Jerry\Documents\Jerry\MockingBird-main\synthesizer\audio.py", line 165, in _mel_to_linear    _inv_mel_basis = np.linalg.pinv(_build_mel_basis(hparams))  File "D:\Users\Jerry\Documents\Jerry\MockingBird-main\synthesizer\audio.py", line 169, in _build_mel_basis    assert hparams.fmax <= hparams.sample_rate // 2AssertionError我修改过mandarin.json，但又改回了原状，不知道是不是这个问题~~~json{"sample_rate": 14000, "n_fft": 800, "num_mels": 80, "hop_size": 200, "win_size": 800, "fmin": 55, "min_level_db": -100, "ref_level_db": 20, "max_abs_value": 4.0, "preemphasis": 0.97, "preemphasize": true, "tts_embed_dims": 512, "tts_encoder_dims": 256, "tts_decoder_dims": 128, "tts_postnet_dims": 512, "tts_encoder_K": 5, "tts_lstm_dims": 1024, "tts_postnet_K": 5, "tts_num_highways": 4, "tts_dropout": 0.5, "tts_cleaner_names": ["basic_cleaners"], "tts_stop_threshold": -3.4, "tts_schedule": [[2, 0.001, 10000, 14], [2, 0.0005, 15000, 14], [2, 0.0002, 20000, 14], [2, 0.0001, 30000, 14], [2, 5e-05, 40000, 14], [2, 1e-05, 60000, 14], [2, 5e-06, 140000, 14], [2, 3e-06, 320000, 14], [2, 1e-06, 640000, 14]], "tts_clip_grad_norm": 1.0, "tts_eval_interval": 500, "tts_eval_num_samples": 1, "tts_finetune_layers": [], "max_mel_frames": 900, "rescale": true, "rescaling_max": 0.9, "synthesis_batch_size": 16, "signal_normalization": true, "power": 1.5, "griffin_lim_iters": 60, "fmax": 7600, "allow_clipping_in_normalization": true, "clip_mels_length": true, "use_lws": false, "symmetric_mels": true, "trim_silence": true, "speaker_embedding_size": 256, "silence_min_duration_split": 0.4, "utterance_min_duration": 1.6, "use_gst": true, "use_ser_for_gst": true}~~~fmax，sample rate不能乱改把。。 
关于plot文件夹下自动生成的注意力图的含义求解
想知道plot文件夹下自动生成的注意力图里的横坐标、纵坐标分别是代表什么参数？为什么出现一条漂亮的斜线就是“出现了注意力机制”？如果两幅图都出现了注意力机制线条，那么能否以及如何比较他们的好坏？目前查阅了一堆关于注意力机制的内容，都没有看到和这幅图相关的知识点，求解！ 
使用新版本遇到问题
我在使用hifigan这一vocoder时候会出现以下错误，听不到合成的声音Read ['欢迎使用工具箱', '现已支持中文输入']Synthesizing ['huan1 ying2 shi3 yong4 gong1 ju4 xiang1', 'xian4 yi3 zhi1 chi2 zhong1 wen2 shu1 ru4']| Generating 1/1Done.Traceback (most recent call last):  File "Z:\deeplearing_project\MockingBird-main\toolbox\__init__.py", line 144, in <lambda>    func = lambda: self.synthesize() or self.vocode()  File "Z:\deeplearing_project\MockingBird-main\toolbox\__init__.py", line 284, in vocode    self.init_vocoder()  File "Z:\deeplearing_project\MockingBird-main\toolbox\__init__.py", line 456, in init_vocoder    model_config_fpath = model_config_fpaths[0]IndexError: list index out of range但使用pretrained则不会有该情况出现，可以听到合成后的声音已修复：config默认路径错误，你拉下最新代码 git pull origin/main 试试> 已修复：config默认路径错误，你拉下最新代码 git pull origin/main 试试Read ['欢迎使用工具箱', '现已支持中文输入']Synthesizing ['huan1 ying2 shi3 yong4 gong1 ju4 xiang1', 'xian4 yi3 zhi1 chi2 zhong1 wen2 shu1 ru4']| Generating 1/1Done.Building hifiganTraceback (most recent call last):  File "Z:\deeplearing_project\MockingBird-main\toolbox\__init__.py", line 144, in <lambda>    func = lambda: self.synthesize() or self.vocode()  File "Z:\deeplearing_project\MockingBird-main\toolbox\__init__.py", line 284, in vocode    self.init_vocoder()  File "Z:\deeplearing_project\MockingBird-main\toolbox\__init__.py", line 465, in init_vocoder    vocoder.load_model(model_fpath, model_config_fpath)  File "Z:\deeplearing_project\MockingBird-main\vocoder\hifigan\inference.py", line 28, in load_model    with open(config_fpath) as f:TypeError: expected str, bytes or os.PathLike object, not NoneType重新下载了整个zip，hifigan还是会出现错误Read ['欢迎使用工具箱', '现已支持中文输入']Synthesizing ['huan1 ying2 shi3 yong4 gong1 ju4 xiang1', 'xian4 yi3 zhi1 chi2 zhong1 wen2 shu1 ru4']| Generating 1/1Done.Traceback (most recent call last):  File "Z:\deeplearing_project\MockingBird-main\toolbox\__init__.py", line 144, in <lambda>    func = lambda: self.synthesize() or self.vocode()  File "Z:\deeplearing_project\MockingBird-main\toolbox\__init__.py", line 305, in vocode    breaks = [np.zeros(int(0.15 * sample_rate))] * len(breaks)UnboundLocalError: local variable 'sample_rate' referenced before assignment这是使用Griffin-Lim的错误> > 已修复：config默认路径错误，你拉下最新代码 git pull origin/main 试试> > Read ['欢迎使用工具箱', '现已支持中文输入'] Synthesizing ['huan1 ying2 shi3 yong4 gong1 ju4 xiang1', 'xian4 yi3 zhi1 chi2 zhong1 wen2 shu1 ru4']> > | Generating 1/1> > Done.> > Building hifigan Traceback (most recent call last): File "Z:\deeplearing_project\MockingBird-main\toolbox__init__.py", line 144, in func = lambda: self.synthesize() or self.vocode() File "Z:\deeplearing_project\MockingBird-main\toolbox__init__.py", line 284, in vocode self.init_vocoder() File "Z:\deeplearing_project\MockingBird-main\toolbox__init__.py", line 465, in init_vocoder vocoder.load_model(model_fpath, model_config_fpath) File "Z:\deeplearing_project\MockingBird-main\vocoder\hifigan\inference.py", line 28, in load_model with open(config_fpath) as f: TypeError: expected str, bytes or os.PathLike object, not NoneType> > 重新下载了整个zip，hifigan还是会出现错误> > Read ['欢迎使用工具箱', '现已支持中文输入'] Synthesizing ['huan1 ying2 shi3 yong4 gong1 ju4 xiang1', 'xian4 yi3 zhi1 chi2 zhong1 wen2 shu1 ru4']> > | Generating 1/1> > Done.> > Traceback (most recent call last): File "Z:\deeplearing_project\MockingBird-main\toolbox__init__.py", line 144, in func = lambda: self.synthesize() or self.vocode() File "Z:\deeplearing_project\MockingBird-main\toolbox__init__.py", line 305, in vocode breaks = [np.zeros(int(0.15 * sample_rate))] * len(breaks) UnboundLocalError: local variable 'sample_rate' referenced before assignment> > 这是使用Griffin-Lim的错误把vocoder\hifigan\config_16k.json 放到 vocoder\saved models\pretrained 里面十四hi> > > 已修复：config默认路径错误，你拉下最新代码 git pull origin/main 试试> > > > > > Read ['欢迎使用工具箱', '现已支持中文输入'] Synthesizing ['huan1 ying2 shi3 yong4 gong1 ju4 xiang1', 'xian4 yi3 zhi1 chi2 zhong1 wen2 shu1 ru4']> > | Generating 1/1> > Done.> > Building hifigan Traceback (most recent call last): File "Z:\deeplearing_project\MockingBird-main\toolbox__init__.py", line 144, in func = lambda: self.synthesize() or self.vocode() File "Z:\deeplearing_project\MockingBird-main\toolbox__init__.py", line 284, in vocode self.init_vocoder() File "Z:\deeplearing_project\MockingBird-main\toolbox__init__.py", line 465, in init_vocoder vocoder.load_model(model_fpath, model_config_fpath) File "Z:\deeplearing_project\MockingBird-main\vocoder\hifigan\inference.py", line 28, in load_model with open(config_fpath) as f: TypeError: expected str, bytes or os.PathLike object, not NoneType> > 重新下载了整个zip，hifigan还是会出现错误> > Read ['欢迎使用工具箱', '现已支持中文输入'] Synthesizing ['huan1 ying2 shi3 yong4 gong1 ju4 xiang1', 'xian4 yi3 zhi1 chi2 zhong1 wen2 shu1 ru4']> > | Generating 1/1> > Done.> > Traceback (most recent call last): File "Z:\deeplearing_project\MockingBird-main\toolbox__init__.py", line 144, in func = lambda: self.synthesize() or self.vocode() File "Z:\deeplearing_project\MockingBird-main\toolbox__init__.py", line 305, in vocode breaks = [np.zeros(int(0.15 * sample_rate))] * len(breaks) UnboundLocalError: local variable 'sample_rate' referenced before assignment> > 这是使用Griffin-Lim的错误> > 把vocoder\hifigan\config_16k.json 放到 vocoder\saved models\pretrained 里面十四hi成功了，hifigan可以成功合成，但是使用Griffin-Lim还是会出现以下错误：Read ['欢迎使用工具箱', '现已支持中文输入']Synthesizing ['huan1 ying2 shi3 yong4 gong1 ju4 xiang1', 'xian4 yi3 zhi1 chi2 zhong1 wen2 shu1 ru4']| Generating 1/1Done.Traceback (most recent call last):  File "Z:\deeplearing_project\MockingBird-main\toolbox\__init__.py", line 144, in <lambda>    func = lambda: self.synthesize() or self.vocode()  File "Z:\deeplearing_project\MockingBird-main\toolbox\__init__.py", line 305, in vocode    breaks = [np.zeros(int(0.15 * sample_rate))] * len(breaks)UnboundLocalError: local variable 'sample_rate' referenced before assignment请问为什么我更换了新版本的MockingBird-main以后就打不开MockingBird-main了，输入运行命令后会报这个错误C:\jr\MockingBird-main>python demo_toolbox.pyTraceback (most recent call last):  File "C:\jr\MockingBird-main\demo_toolbox.py", line 2, in <module>    from toolbox import Toolbox  File "C:\jr\MockingBird-main\toolbox\__init__.py", line 6, in <module>    import ppg_extractor as extractor  File "C:\jr\MockingBird-main\ppg_extractor\__init__.py", line 6, in <module>    from .frontend import DefaultFrontend  File "C:\jr\MockingBird-main\ppg_extractor\frontend.py", line 5, in <module>    from torch_complex.tensor import ComplexTensorModuleNotFoundError: No module named 'torch_complex'> 请问为什么我更换了新版本的MockingBird-main以后就打不开MockingBird-main了，输入运行命令后会报这个错误 C:\jr\MockingBird-main>python demo_toolbox.py Traceback (most recent call last): File "C:\jr\MockingBird-main\demo_toolbox.py", line 2, in from toolbox import Toolbox File "C:\jr\MockingBird-main\toolbox__init__.py", line 6, in import ppg_extractor as extractor File "C:\jr\MockingBird-main\ppg_extractor__init__.py", line 6, in from .frontend import DefaultFrontend File "C:\jr\MockingBird-main\ppg_extractor\frontend.py", line 5, in from torch_complex.tensor import ComplexTensor ModuleNotFoundError: No module named 'torch_complex'还是落后一两个commit，再拉一下最新>更换了，但是还是不行 C:\jr\MockingBird-main>python demo_toolbox.pyTraceback (most recent call last):  File "C:\jr\MockingBird-main\demo_toolbox.py", line 2, in <module>    from toolbox import Toolbox  File "C:\jr\MockingBird-main\toolbox\__init__.py", line 9, in <module>    from utils.f0_utils import compute_f0, f02lf0, compute_mean_std, get_converted_lf0uv  File "C:\jr\MockingBird-main\utils\f0_utils.py", line 3, in <module>    import pyworldModuleNotFoundError: No module named 'pyworld'> 你的环境还没装好吧是在安装时pyworld报错了，一直装不上pyworld> 是在安装时pyworld报错了，一直装不上pyworldpip install pyworld同上，然后执行： pip install pyworld返回：Could not build wheels for pyworld, which is required to install pyproject.toml-based C++ 14.我去搞一下> > 看了，我可能缺了visual C++ 14.我去搞一下下一个visual studio 链接：https://visualstudio.microsoft.com/zh-hans/  安装时选择c++ 的拓展安装完成即可解决了，确实是以上问题 
能否部署到树莓派运行
我尝试了一下在树莓派安装requirements.txt里面的包，但是部分无法安装，请问这个项目可以部署到树莓派运行吗？树莓派的性能不太行吧我自己的配置是7700+1066+7200转机械硬盘预处理aidatatang_200zh数据集要3h左右，8进程训练速度只有0.47~0.56steps/s左右，batch_size是14，如果上16就爆显存了训练过程种占用6.5G左右内存> 好的，谢谢啦~只是推理的话应该ok把， 
无法启动web
最新的commit输入 报错> Loaded synthesizer models: 1Loaded encoder "pretrained.pt" trained to step 1594501Building Wave-RNNTrainable Parameters: 4.481MLoading model weights at vocoder\saved_models\pretrained\pretrained.ptBuilding hifiganTraceback (most recent call last):  File "D:\Users\Jerry\Documents\Jerry\MockingBird-main\web.py", line 6, in <module>    app = webApp()  File "D:\Users\Jerry\Documents\Jerry\MockingBird-main\web\__init__.py", line 35, in webApp    gan_vocoder.load_model(Path("vocoder/saved_models/pretrained/g_hifigan.pt"))  File "D:\Users\Jerry\Documents\Jerry\MockingBird-main\vocoder\hifigan\inference.py", line 28, in load_model    with open(config_fpath) as f:FileNotFoundError: [Errno 2] No such file or directory: './vocoder/saved_models/24k/config.json'我也是一样的情况已修复：config默认路径错误，你拉下最新代码 git pull origin/main 试试> 已修复：config默认路径错误，你拉下最新代码 git pull origin/main 试试是不是还有bug 少了一个_文件名是config_16k_.json已经改了，感谢 
训练自己的模型中断后，想继续训练上一次的模型，该在终端运行什么指令？
我想达到的继续训练效果是指loss和步数都能接着上次的训练结果，比如我这次训练到25K步，loss=0.45，下次训练一开始就能从步数25k，loss=0.45开始 ，那么我该如何输入终端指令？我试着run_id都不变，文件命名也不做任何改动，直接运行python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer  多谢指点！> 我之前在win上试是重来的... 看了你的回答，我又在linux服务器上试了，确实是接着的 多谢指点！我这个就是windows，你在执行 时可以加上参数-s 是指每训练多少steps就保存一下，默认是1000-b 是指每训练多少steps就备份一下，默认是25000更多参数打开文件看 
执行python demo_toolbox.py报错
执行python demo_toolbox.py报错，PS C:\MockingBird-main (1)\MockingBird-main> python demo_toolbox.pyTraceback (most recent call last):  File "C:\MockingBird-main (1)\MockingBird-main\demo_toolbox.py", line 2, in <module>    from toolbox import Toolbox  File "C:\MockingBird-main (1)\MockingBird-main\toolbox\__init__.py", line 6, in <module>    import ppg_extractor as extractor  File "C:\MockingBird-main (1)\MockingBird-main\ppg_extractor\__init__.py", line 8, in <module>    from .encoder.conformer_encoder import ConformerEncoder  File "C:\MockingBird-main (1)\MockingBird-main\ppg_extractor\encoder\conformer_encoder.py", line 28, in <module>    from .subsampling import Conv2dNoSubsampling, Conv2dSubsampling  File "C:\MockingBird-main (1)\MockingBird-main\ppg_extractor\encoder\subsampling.py", line 11, in <module>    from espnet.nets.pytorch_backend.transformer.embedding import PositionalEncodingModuleNotFoundError: No module named 'espnet'之后我执行pip install espnet也出现了错误，找了很久都没有找到解决方法，希望可以得到一些帮助。有一种可能，运行pip install espnet时是不是出现了“安装pyworld的环节”失败的情况？如果是这样的话，似乎需要先下载visual_studio再重试pip install espnet我用的是anaconda，在下载了visual_studio之后仍然报错，于是重装了anaconda之后再pip install pyworld或者pip install espnet就成功了（费了好大功夫）已修复：依赖没有剥离干净，你拉下最新代码 git pull origin/main 试试> 已修复：依赖没有剥离干净，你拉下最新代码 git pull origin/main 试试已经解决了，感谢visual_studio 安装了以后，一定要重启一下电脑，我的就是这么解决的 o(╥﹏╥)o 
一个数据集生成器
我做了一个利用aliyun tts批量生成数据集的软件，大家可以尝试使用现成的tts制作更多纯净的语音数据集来反哺自己的模型，暂时只有CLI，没有GUI：https://github.com/AyahaShirane/Transcript2vioce希望大家支持，也希望各位大佬帮我继续补全，谢谢这里的说话人会集中在几个人吗？> 这里的说话人会集中在几个人吗？将voice参数设置为random可以随机生成28个发音人的内容，再算上语调和语速上的改变，基本上够用> > 这里的说话人会集中在几个人吗？> > 将voice参数设置为random可以随机生成28个发音人的内容，再算上语调和语速上的改变，基本上够用比较适合把vocoder部分训练好一点，其他的会影响模型泛化能力把 
经常发生crash PaAlsaStreamComponent_BeginPolling
python3: src/hostapi/alsa/pa_linux_alsa.c:3641: PaAlsaStreamComponent_BeginPolling: Assertion 
在encoder预处理中
对aidatatang数据集encoder预处理的时候命令行运行的是python encoder_preprocess.py F:\dataC:\Users\Administrator\MockingBird-main\encoder\audio.py:58: FutureWarning: Pass y=[0.0000000e+00 0.0000000e+00 2.1362305e-04 ... 1.0070801e-03 9.1552734e-05 2.7465820e-04], sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error  frames = 
AttributeError: 'UI' object has no attribute 'current_extractor_fpath'
在运行demo_toolbox.py后，选用g-hifigan模型合成声音后会显示AttributeError: 'UI' object has no attribute 'current_extractor_fpath'已推送修复 
新的ppg模式相关问题
请问最新的ppg两个模型可以在哪里下载？已补充到readme> 已补充到自述文件好像没有下载链接。。要自己寻找吗自述文件里也没有啊Commit里没有readme的update---原始邮件---发件人: ***@***.***&gt;发送时间: 2022年3月8日(周二) 上午10:29收件人: ***@***.***&gt;;抄送: ***@***.******@***.***&gt;;主题: Re: [babysor/MockingBird] 请问最新的ppg两个模型可以在哪里下载？ (Issue #423)  已补充到自述文件  好像没有下载链接。。要自己寻找吗 —Reply to this email directly, view it on GitHub, or unsubscribe.Triage notifications on the go with GitHub Mobile for iOS or Android. You are receiving this because you authored the thread.Message ID: ***@***.***&gt;oh shit，好像一直没有成功，我这里先贴一下：链接:  密码: bt9m--来自百度网盘超级会员V4的分享如果方便的话能换其他的下载源吗，因为百度云限速挺麻烦的---原始邮件---发件人: ***@***.***&gt;发送时间: 2022年3月8日(周二) 晚上7:43收件人: ***@***.***&gt;;抄送: ***@***.******@***.***&gt;;主题: Re: [babysor/MockingBird] 请问最新的ppg两个模型可以在哪里下载？ (Issue #423) oh shit，好像一直没有成功，我这里先贴一下： 链接:  密码: bt9m --来自百度网盘超级会员V4的分享 —Reply to this email directly, view it on GitHub, or unsubscribe.Triage notifications on the go with GitHub Mobile for iOS or Android. You are receiving this because you authored the thread.Message ID: ***@***.***&gt;> oh shit，好像一直没有成功，我这里先贴一下： 链接: 密码: bt9m --来自百度网盘超级会员V4的分享这个合成出来感觉都是电流声，这个ppg模型要自己用数据集去训练效果才会好吗> > oh shit，好像一直没有成功，我这里先贴一下： 链接: 密码: bt9m --来自百度网盘超级会员V4的分享> > 这个合成出来感觉都是电流声，这个ppg模型要自己用数据集去训练效果才会好吗不用，大概率是你没选对模型，除了encoder基本上都要替换成新的> > > oh shit，好像一直没有成功，我这里先贴一下： 链接: 密码: bt9m --来自百度网盘超级会员V4的分享> > > > > > 这个合成出来感觉都是电流声，这个ppg模型要自己用数据集去训练效果才会好吗> > 不用，大概率是你没选对模型，除了encoder基本上都要替换成新的这样子吗，有没有demo参照一下呢readme中文版里面的截图就是一个可行的> readme中文版里面的截图就是一个可行的请问如果要提高特定人语音的相似度，训练哪一个模型是最好的，Convertor 或者Encoder还是其他的目前看 convertor 是瓶颈，但我继续用aidatatang数据训练效果收益一般> 目前看 convertor 是瓶颈，但我继续用aidatatang数据训练效果收益一般这个图谱显示了全是电流<img width="512" alt="微信图片_20220314180818" src="https://user-images.githubusercontent.com/75252160/158150949-a07370cf-cb02-4890-a1dc-15537066d6f1.png">> 目前看 convertor 你做了什么手动转码？用格式工厂自己批量转了---原始邮件---发件人: ***@***.***&gt;发送时间: 2022年3月14日(周一) 晚上11:45收件人: ***@***.***&gt;;抄送: ***@***.******@***.***&gt;;主题: Re: [babysor/MockingBird] 新的ppg模式相关问题 (Issue #423) 你做了什么手动转码？ —Reply to this email directly, view it on GitHub, or unsubscribe.Triage notifications on the go with GitHub Mobile for iOS or Android. You are receiving this because you authored the thread.Message ID: ***@***.***&gt;> 用格式工厂自己批量转了> ---原始邮件--- 发件人: ***@***.***&gt; 发送时间: 2022年3月14日(周一) 晚上11:45 收件人: ***@***.***&gt;; 抄送: ***@***.******@***.***&gt;; 主题: Re: [babysor/MockingBird] 新的ppg模式相关问题 (Issue #423) 你做了什么手动转码？ — Reply to this email directly, view it on GitHub, or unsubscribe. Triage notifications on the go with GitHub Mobile for iOS or Android. You are receiving this because you authored the thread.Message ID: ***@***.***&gt;感觉你触发了不曾见过的bug，先不转，你试着preprocess一下我说的这些bug就是预处理的时候遇到的，连正式训练都进不去---原始邮件---发件人: ***@***.***&gt;发送时间: 2022年3月15日(周二) 凌晨0:05收件人: ***@***.***&gt;;抄送: ***@***.******@***.***&gt;;主题: Re: [babysor/MockingBird] 新的ppg模式相关问题 (Issue #423)  用格式工厂自己批量转了 … ---原始邮件--- 发件人: @.&gt; 发送时间: 2022年3月14日(周一) 晚上11:45 收件人: @.&gt;; 抄送: @.@.&gt;; 主题: Re: [babysor/MockingBird] 新的ppg模式相关问题 (Issue #423) 你做了什么手动转码？ — Reply to this email directly, view it on GitHub, or unsubscribe. Triage notifications on the go with GitHub Mobile for iOS or Android. You are receiving this because you authored the thread.Message ID: @.***&gt;  感觉你触发了不曾见过的bug，先不转，你试着preprocess一下 —Reply to this email directly, view it on GitHub, or unsubscribe.Triage notifications on the go with GitHub Mobile for iOS or Android. You are receiving this because you authored the thread.Message ID: ***@***.***&gt;> > oh shit，好像一直没有成功，我这里先贴一下： 链接: 密码: bt9m --来自百度网盘超级会员V4的分享> > 这个合成出来感觉都是电流声，这个ppg模型要自己用数据集去训练效果才会好吗> > oh shit，好像一直没有成功，我这里先贴一下： 链接: 密码: bt9m --来自百度网盘超级会员V4的分享> > 这个合成出来感觉都是电流声，这个ppg模型要自己用数据集去训练效果才会好吗应该把作者提供的声码器模型hifigan_24k.pt重命名为g_hifigan_24k.pt~ 再选择~  电流音解决~但感觉效果不咋样捏> > > oh shit，好像一直没有成功，我这里先贴一下： 链接: 密码: bt9m --来自百度网盘超级会员V4的分享> > > > > > 这个合成出来感觉都是电流声，这个ppg模型要自己用数据集去训练效果才会好吗> > > > oh shit，好像一直没有成功，我这里先贴一下： 链接: 密码: bt9m --来自百度网盘超级会员V4的分享> > > > > > 这个合成出来感觉都是电流声，这个ppg模型要自己用数据集去训练效果才会好吗> > 应该把作者提供的声码器模型hifigan_24k.pt重命名为g_hifigan_24k.pt~ 再选择~ 电流音解决~但感觉效果不咋样捏确实 而且再播放合成好的声音还会变化成低音> 目前看 convertor 是瓶颈，但我继续用aidatatang数据训练效果收益一般所以为啥训练合成器的时候能去掉时长较短的音频，训练ppg2mel的时候没有这一步？我感觉也不怎么样，怀疑数据集问题，还是相信paper本身的> 我感觉也不怎么样，怀疑数据集问题，还是相信paper本身的有了解过他们的技术吗 看起来已经能和变声器一样实时了，不知道是不是真的https://www.respeecher.com/voice-cloning-video-demos?hsLang=en大佬有杂音的解决办法了吗？我用英文源码给的30400步的那个模型直接换声出来是没有杂音的，就是声音不够像，但是只要用中文训练过就会有杂音的问题，感觉可能是数据处理的问题？> 大佬有杂音的解决办法了吗？我用英文源码给的30400步的那个模型直接换声出来是没有杂音的，就是声音不够像，但是只要用中文训练过就会有杂音的问题，感觉可能是数据处理的问题？数据集的原因，这个模型对数据集要求高一些> > 大佬有杂音的解决办法了吗？我用英文源码给的30400步的那个模型直接换声出来是没有杂音的，就是声音不够像，但是只要用中文训练过就会有杂音的问题，感觉可能是数据处理的问题？> > 数据集的原因，这个模型对数据集要求高一些请问具体是哪方面要求呢？有没有适合训练的中文数据集推荐呢？> > > 大佬有杂音的解决办法了吗？我用英文源码给的30400步的那个模型直接换声出来是没有杂音的，就是声音不够像，但是只要用中文训练过就会有杂音的问题，感觉可能是数据处理的问题？> > > > > > 数据集的原因，这个模型对数据集要求高一些> > 请问具体是哪方面要求呢？有没有适合训练的中文数据集推荐呢？开源的很多都不行，最好是有商用的> > > > 大佬有杂音的解决办法了吗？我用英文源码给的30400步的那个模型直接换声出来是没有杂音的，就是声音不够像，但是只要用中文训练过就会有杂音的问题，感觉可能是数据处理的问题？> > > > > > > > > 数据集的原因，这个模型对数据集要求高一些> > > > > > 请问具体是哪方面要求呢？有没有适合训练的中文数据集推荐呢？> > 开源的很多都不行，最好是有商用的是采样率的问题吗？还有其他方面会有影响吗？我好按照具体方向去找相应数据集求一个最新的 两个ppg模型 百度网盘链接> oh shit，好像一直没有成功，我这里先贴一下： 链接: 密码: bt9m --来自百度网盘超级会员V4的分享求最新的 两个ppg模型 百度网盘链接 
求助！！！在下载剩余的包 pip install -r requirements.txt 报错了 大佬知道咋解决吗
以下是报错代码：Building wheels for collected packages: ctc-segmentation, pyworld  Building wheel for ctc-segmentation (setup.py) ... error  error: subprocess-exited-with-error  × python setup.py bdist_wheel did not run successfully.  │ exit code: 1  ╰─> [12 lines of output]      running bdist_wheel      running build      running build_py      creating build      creating build\lib.win-amd64-3.7      creating build\lib.win-amd64-3.7\ctc_segmentation      copying ctc_segmentation\ctc_segmentation.py -> build\lib.win-amd64-3.7\ctc_segmentation      copying ctc_segmentation\partitioning.py -> build\lib.win-amd64-3.7\ctc_segmentation      copying ctc_segmentation\__init__.py -> build\lib.win-amd64-3.7\ctc_segmentation      running build_ext      building 'ctc_segmentation.ctc_segmentation_dyn' extension      error: Microsoft Visual C++ 14.0 is required. Get it with "Build Tools for Visual Studio":      [end of output]  note: This error originates from a subprocess, and is likely not a problem with pip.  ERROR: Failed building wheel for ctc-segmentation  Running setup.py clean for ctc-segmentation  Building wheel for pyworld (pyproject.toml) ... error  error: subprocess-exited-with-error  × Building wheel for pyworld (pyproject.toml) did not run successfully.  │ exit code: 1  ╰─> [13 lines of output]      running bdist_wheel      running build      running build_py      creating build      creating build\lib.win-amd64-3.7      creating build\lib.win-amd64-3.7\pyworld      copying pyworld\__init__.py -> build\lib.win-amd64-3.7\pyworld      running build_ext      skipping 'pyworld\pyworld.cpp' Cython extension (up-to-date)      building 'pyworld.pyworld' extension      C:\Users\ADMINI~1\AppData\Local\Temp\pip-build-env-_y7fbfzj\overlay\Lib\site-packages\setuptools\dist.py:741: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead        % (opt, underscore_opt)      error: Microsoft Visual C++ 14.0 or greater is required. Get it with "Microsoft C++ Build Tools":      [end of output]  note: This error originates from a subprocess, and is likely not a problem with pip.  ERROR: Failed building wheel for pyworldFailed to build ctc-segmentation pyworldERROR: Could not build wheels for pyworld, which is required to install pyproject.toml-based projects额 pyworld居然需要visual c++依赖 你可以按提示先安装一下。看来有必要把vc 部分给分离出来问下是怎样把vc分离出来呀。。我这边下载了visual c++是在mocking bird 文件夹里面安装吗？拉一下最新的，如果你暂时用不到voice conversion功能，再次运行 pip install 就好了谢谢！问题解决了！运行 pip install -v -e .之后报错：error: subprocess-exited-with-error× python setup.py develop did not run successfully.note: This error originates from a subprocess, and is likely not a problem with pip. 大佬可以分享一下解决过程吗？怎么解决的没人说一下么？？> 谢谢！问题解决了！楼主辛苦填一下坑呗我来解答。刚才在一台新windows电脑上安装运行环境，也遇到这个问题。这个错误是由于没有vc++编译环境造成的。其实解决方法也简单：从windows官网下载最新版的“visual studio installer”， 安装 vc++ 就行。> 拉一下最新的，如果你暂时用不到voice conversion功能，再次运行 pip install 就好了最新的内容是哪个呀大佬 
synthesizer跑到到1500步就出错，有高手知道是怎么回事吗
下面是日志：{| Epoch: 143/910 (10/11) | Loss: 0.3434 | 1.1 steps/s | Step: 1k | }{| Epoch: 143/910 (11/11) | Loss: 0.3442 | 1.1 steps/s | Step: 1k | }{| Epoch: 144/910 (11/11) | Loss: 0.3435 | 1.1 steps/s | Step: 1k | }{| Epoch: 145/910 (2/11) | Loss: 0.3433 | 1.1 steps/s | Step: 1k | }Traceback (most recent call last):  File "D:\MockingBird-main\synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "D:\MockingBird-main\synthesizer\train.py", line 201, in train    loss.backward()  File "D:\anaconda\envs\pytorch\lib\site-packages\torch\_tensor.py", line 307, in backward    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)  File "D:\anaconda\envs\pytorch\lib\site-packages\torch\autograd\__init__.py", line 154, in backward    Variable._execution_engine.run_backward(RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling 目前发现这个问题没有好得解决方法，跟驱动稳定性有关，可以把batchsize调小一点看可能是否会改善。> 目前发现这个问题没有好得解决方法，跟驱动稳定性有关，可以把batchsize调小一点看可能是否会改善。谢谢，我昨晚从12改成8果然不报错了。但是又出现一个问题，昨天速度还是1.3steps/s, 【{| Epoch: 143/30000 (10/11) | Loss: 0.5434 | 1.3 steps/s | Step: 55k | }】今天再打开怎么只有0.67steps/s了【{| Epoch: 34/16500 (4/4) | Loss: 0.5018 | 0.67 steps/s | Step: 94k | }】> > 目前发现这个问题没有好得解决方法，跟驱动稳定性有关，可以把batchsize调小一点看可能是否会改善。> > 谢谢，我昨晚从12改成8果然不报错了。 但是又出现一个问题，昨天速度还是1.3steps/s, 【{| Epoch: 143/30000 (10/11) | Loss: 0.5434 | 1.3 steps/s | Step: 55k | }】> > 今天再打开怎么只有0.67steps/s了 【{| Epoch: 34/16500 (4/4) | Loss: 0.5018 | 0.67 steps/s | Step: 94k | }】是的，建议阶梯调大，而不是调小> 0.67 steps/s总是 0.67 steps/s，这个速度怎么调大呀？> > 0.67 steps/s> > 总是 0.67 steps/s，这个速度怎么调大呀？这里有效速度约是这个数字*batchsize. 完全看gpu算力 没法手动调大发现是个bug，正在修复，你可以先把saved model下的json删掉， 
在导入音频文件时报错
导入MP3音频时，报错 RuntimeError: Error opening 'C:\\Users\\Administrator\\Desktop\\1.mp3': File contains data in an unknown 
对于wiki教程的百度网盘链接的建议
drive得好的，多谢 
话说收敛曲线怎么样才算合格？
在练的过程中，至少要等到Plots 里面的attention图出现收敛（Convergence），才能正常发出人声，如下图出现一条明显斜线，否则就是一团奇怪噪音，对于loss数字就多寡随意，丰俭由人了。这是原话  话说你这个图实在哪里看的呀？我训练了24h了saved_models还是空的> > 在练的过程中，至少要等到Plots 里面的attention图出现收敛（Convergence），才能正常发出人声，如下图出现一条明显斜线，否则就是一团奇怪噪音，对于loss数字就多寡随意，丰俭由人了。> > 这是原话 话说你这个图实在哪里看的呀？我训练了24h了saved_models还是空的plots文件夹你是不是没正式训练？正常来说预处理后训练一开始就会创建相应的模型文件夹> > > 在练的过程中，至少要等到Plots 里面的attention图出现收敛（Convergence），才能正常发出人声，如下图出现一条明显斜线，否则就是一团奇怪噪音，对于loss数字就多寡随意，丰俭由人了。> > > > > > 这是原话 话说你这个图实在哪里看的呀？我训练了24h了saved_models还是空的> > plots文件夹 你是不是没正式训练？ 在训练呀{| Epoch: 2/2 (4519/10207) | Loss: 0.4708 | 0.66 steps/s | Step: 54k | }那我还真不知道了 问问原作者吧---原始邮件---发件人: ***@***.***&gt;发送时间: 2022年3月2日(周三) 下午2:36收件人: ***@***.***&gt;;抄送: ***@***.******@***.***&gt;;主题: Re: [babysor/MockingBird] 话说收敛曲线怎么样才算合格？ (Issue #413)    在练的过程中，至少要等到Plots 里面的attention图出现收敛（Convergence），才能正常发出人声，如下图出现一条明显斜线，否则就是一团奇怪噪音，对于loss数字就多寡随意，丰俭由人了。  这是原话 话说你这个图实在哪里看的呀？我训练了24h了saved_models还是空的  plots文件夹 你是不是没正式训练？ 正常来说预处理后训练一开始就会创建相应的模型文件夹   在训练呀 {| Epoch: 2/2 (4519/10207) | Loss: 0.4708 | 0.66 steps/s | Step: 54k | } —Reply to this email directly, view it on GitHub, or unsubscribe.Triage notifications on the go with GitHub Mobile for iOS or Android. You are receiving this because you authored the thread.Message ID: ***@***.***&gt;> 我发现了，是我看错目录了。。。是程序源码所在的MockingBird-main\synthesizer\saved_models\mandarin\plots文件夹是吧抱歉收敛已经没问题了，就看你训练数据质量和batchsize了，等loss基本不降得时候停下来试试这条斜线应该是一条对角线，还是一条45度的斜线，这里的注意力是谁和谁之间的注意力查了一下，这里的注意力计算的是源状态（文字编码状态）与目标状态（mel特征隐状态）之间的注意力。很多图后半部分注意力分散代表了什么？ 
训练到39k还没有saved_models正常吗
用的是aidatatang_200zh{| Epoch: 1/1 (9228/10207) | Loss: 0.4928 | 0.69 steps/s | Step: 39k | }saved_models文件夹也没有新建，自己新建了一个，用了别人训练好的，但是效果不好。就继续训练。训练了一晚上文件夹还是空的SV2TTS文件夹25.5 GB (27,449,503,629 字节)aidatatang_200zh文件夹32.5 GB (34,988,358,844 字节)你这32.5GB的aidatatang_200zh数据集是原本18G解压出来的吗？> 你这32.5GB的aidatatang_200zh数据集是原本18G解压出来的吗？是的，但是我解压了aidatatang_200zh\corpus\train里的压缩包后没有删除压缩包，所以可能大一点好的，感谢！------------------&nbsp;原始邮件&nbsp;------------------发件人:                                                                                                                        "babysor/MockingBird"                                                                                    ***@***.***&gt;;发送时间:&nbsp;2022年3月2日(星期三) 上午9:43***@***.***&gt;;***@***.******@***.***&gt;;主题:&nbsp;Re: [babysor/MockingBird] 训练到39k还没有saved_models正常吗 (Issue #412)  你这32.5GB的aidatatang_200zh数据集是原本18G解压出来的吗？  是的，但是我解压了aidatatang_200zh\corpus\train里的压缩包后没有删除压缩包，所以可能大一点 —Reply to this email directly, view it on GitHub, or unsubscribe.Triage notifications on the go with GitHub Mobile for iOS or Android. You are receiving this because you commented.Message ID: ***@***.***&gt;这么厉害？32.5GB的aidatatang_200zh 。 我的数据集自己做的才1个G你仔细看文件日期是会变动的会保存的> 你仔细看文件日期是会变动的会保存的我看资源监视器里python.exe一直是写0字节/秒，倒是一直在读。占用GPU70%，显存5G多抱歉看错了目录是MockingBird-main\synthesizer\saved_models\mandarin\plots 
训练多长时间能够完成？（解决）
我没有使用作者的模型进行训练，跑了两天，现在的进度是{| Epoch: 949/1124 (36/89) | Loss: 0.1887 | 0.91 steps/s | Step: 144k | }符合了官方教程的标准，但是打开toolbox之后声音完全不清楚，波频图如下，[url=https://postimg.cc/9R44NRmP][img]https://i.postimg.cc/9R44NRmP/Snipaste-2022-03-01-06-33-17.png[/img][/url][url=https://postimg.cc/Lh1Yt102][img]https://i.postimg.cc/Lh1Yt102/Snipaste-2022-03-01-06-33-29.png[/img][/url]请问还要有多长时间能结束？或者是不是我的操作出现问题才导致的声音模糊？ 已解决，根据之前的issue，用上了官方的模型就开跑了顺便说一下，如果用不了，应该是你的MockingBird-main不是最新版本，将code重新下一个，把saved_models重新复制进去就能用了。 
如何把一个py文件转换成ui文件？方便添加自己的功能并且再次修改界面ui
求助作者及各位大佬！不太明白你的意思，能讲一下需求吗就是如何修改UI界面？是直接改ui.py里面的代码吗？------------------&nbsp;原始邮件&nbsp;------------------发件人:                                                                                                                        "babysor/MockingBird"                                                                                    ***@***.***&gt;;发送时间:&nbsp;2022年3月2日(星期三) 凌晨0:02***@***.***&gt;;***@***.******@***.***&gt;;主题:&nbsp;Re: [babysor/MockingBird] 如何把一个py文件转换成ui文件？方便添加自己的功能并且再次修改界面ui (Issue #410) 不太明白你的意思，能讲一下需求吗 —Reply to this email directly, view it on GitHub, or unsubscribe.Triage notifications on the go with GitHub Mobile for iOS or Android. You are receiving this because you authored the thread.Message ID: ***@***.***&gt;只是界面的话，直接改toolbox，学一下qt5 
【小白提问】AttributeError: module 'distutils' has no attribute 'version' 怎么解？？这个问题困扰了一下午都没能解决
D:\MockingBird-main>python synthesizer_train.py man D:\制作数据集\SV2TTS\synthesizerTraceback (most recent call last):  File "synthesizer_train.py", line 2, in <module>    from synthesizer.train import train  File "D:\MockingBird-main\synthesizer\train.py", line 5, in <module>    from torch.utils.tensorboard import SummaryWriter  File "C:\Users\VisionMNext\AppData\Roaming\Python\Python36\site-packages\torch\utils\tensorboard\__init__.py", line 4, in <module>    LooseVersion = distutils.version.LooseVersionAttributeError: module 'distutils' has no attribute 'version'经过本人努力，发现 在 C:\Users\VisionMNext\AppData\Roaming\Python\Python36\site-packages\torch\utils\tensorboard __init__.py 文件中加入 from distutils.version import LooseVersion。锁定位置就OK了原理是什么有人能教一下吗你们训练了多久才出图，我用了半个小时> 你们训练了多久才出图，我用了半个小时比较快了> > 你们训练了多久才出图，我用了半个小时> > 比较快了还好，我现在20分钟一张图，训练了8个小时从0.8降到0.62> 你们训练了多久才出图，我用了半个小时我练了24小时还没出图{| Epoch: 2/2 (4130/10207) | Loss: 0.4730 | 0.66 steps/s | Step: 54k | }看错文件夹了吧？如果是得话麻烦关闭一下issue 
Mac m1 启动GUI闪退（sounddevice.PortAudioError: Error querying host API -9979）
python3 demo_toolbox.py-------------------------------------------------------Arguments:    datasets_root:    None    enc_models_dir:   encoder/saved_models    syn_models_dir:   synthesizer/saved_models    voc_models_dir:   vocoder/saved_models    cpu:              False    seed:             None    no_mp3_support:   FalseWarning: you did not pass a root directory for datasets as argument.The recognized datasets are:	LibriSpeech/dev-clean	LibriSpeech/dev-other	LibriSpeech/test-clean	LibriSpeech/test-other	LibriSpeech/train-clean-100	LibriSpeech/train-clean-360	LibriSpeech/train-other-500	LibriTTS/dev-clean	LibriTTS/dev-other	LibriTTS/test-clean	LibriTTS/test-other	LibriTTS/train-clean-100	LibriTTS/train-clean-360	LibriTTS/train-other-500	LJSpeech-1.1	VoxCeleb1/wav	VoxCeleb1/test_wav	VoxCeleb2/dev/aac	VoxCeleb2/test/aac	VCTK-Corpus/wav48	aidatatang_200zh/corpus/dev	aidatatang_200zh/corpus/test	aishell3/test/wav	magicdata/trainFeel free to add your own. You can still use the toolbox by recording samples yourself.||PaMacCore (AUHAL)|| AUHAL component not found.||PaMacCore (AUHAL)|| OpenStream @ 16000 returned: -9999: Unanticipated host error||PaMacCore (AUHAL)|| AUHAL component not found.||PaMacCore (AUHAL)|| OpenStream @ 16000 returned: -9999: Unanticipated host errorTraceback (most recent call last):  File "/Users/***/work/demo/MockingBird/toolbox/ui.py", line 159, in setup_audio_devices    sd.check_output_settings(device=device["name"], samplerate=sample_rate)  File "/Users/***/miniconda3/lib/python3.9/site-packages/sounddevice.py", line 691, in check_output_settings    _check(_lib.Pa_IsFormatSupported(_ffi.NULL, parameters, samplerate))  File "/Users/***/miniconda3/lib/python3.9/site-packages/sounddevice.py", line 2736, in _check    raise PortAudioError(errormsg, err, hosterror_info)sounddevice.PortAudioError: <unprintable PortAudioError object>During handling of the above exception, another exception occurred:Traceback (most recent call last):  File "/Users/***/work/demo/MockingBird/demo_toolbox.py", line 43, in <module>    Toolbox(**vars(args))  File "/Users/***/work/demo/MockingBird/toolbox/__init__.py", line 76, in __init__    self.setup_events()  File "/Users/***/work/demo/MockingBird/toolbox/__init__.py", line 113, in setup_events    self.ui.setup_audio_devices(Synthesizer.sample_rate)  File "/Users/***/work/demo/MockingBird/toolbox/ui.py", line 164, in setup_audio_devices    warn("Unsupported output device %s for the sample rate: %d \nError: %s" % (device["name"], sample_rate, str(e)))  File "/Users/***/miniconda3/lib/python3.9/site-packages/sounddevice.py", line 2220, in __str__    hostname = query_hostapis(host_api)['name']  File "/Users/***/miniconda3/lib/python3.9/site-packages/sounddevice.py", line 640, in query_hostapis    raise PortAudioError('Error querying host API {}'.format(index))sounddevice.PortAudioError: Error querying host API -9979请问有解决办法吗 
我能在训练途中重新进行预处理吗

求助， No module named 'vocoder.wavernn'
Mac M1，所有依赖包安装完成后运行demo，提示报错，大家有没有思路；Traceback (most recent call last):  File "/Users/***/work/demo/MockingBird/demo_toolbox.py", line 2, in <module>    from toolbox import Toolbox  File "/Users/***/work/demo/MockingBird/toolbox/__init__.py", line 5, in <module>    from vocoder.wavernn import inference as rnn_vocoderModuleNotFoundError: No module named 'vocoder.wavernn'试着在 vocoder.wavernn 目录里面加一个 __init__.py 的空文件后试试> 试着在 vocoder.wavernn 目录里面加一个 **init**.py 的空文件后试试还是照旧 困扰了一天  文件名字用这个解决了vocoder目录下文件不齐 
web连续换行造成的多了个None
小问题，gui好像没有这个问题，自己用的时候直接调用的函数发现的这个情况。中间会有段空声音，难道是两个换号断句更加明显 
预处理失败，求助
C:\Users\Administrator\Downloads\MockingBird-main\MockingBird-main>python pre.py C:\Users\Administrator\Downloads -d aidatatang_200zh -n 6Using data from:    C:\Users\Administrator\Downloads\aidatatang_200zh\corpus\trainTraceback (most recent call last):  File "C:\Users\Administrator\Downloads\MockingBird-main\MockingBird-main\pre.py", line 74, in <module>    preprocess_dataset(**vars(args))  File "C:\Users\Administrator\Downloads\MockingBird-main\MockingBird-main\synthesizer\preprocess.py", line 45, in preprocess_dataset    assert all(input_dir.exists() for input_dir in input_dirs)AssertionError数据集文件路径不正确，确认一下是否有 C:\Users\Administrator\Downloads\aidatatang_200zh\corpus\train感谢，我的aidatatang_200zh文件夹套了两层我没发现顺带一提，如果只是要训练出来一个特定声音的模型而不是通用模型 ，大概需要多长时间的语音或者几条短句？我大概是2000条短句5个小时，但是训练出来的模型根本无法收敛，是一条直线。我是继续去找数据集还是接着pretrained-11-7-21_75k.pt训练比较好？接着pretrained-11-7-21_75k.pt训练感谢> 感谢请关闭 issue.@Xlbnas 
关于YourTTS
#436 
我用aidatatang_200zh数据训练了150k 发现模拟出来的声音全是电流
我用aidatatang_200zh数据训练了150k 发现模拟出来的声音全都是电流 声音模糊不清是从头训练的？在20k的时候观察一下有没有明显attention alignment线想问下aidatatang_200zh数据集解压后的文件大小是多少？> 是从头训练的？在20k的时候观察一下有没有明显attention alignment线请问使用18G的aidatatang从头训练，和用自己处理后的数据集生成syn模型后再用readme提供模型替换继续训练，两者有什么区别么> > 是从头训练的？在20k的时候观察一下有没有明显attention alignment线> > 请问使用18G的aidatatang从头训练，和用自己处理后的数据集生成syn模型后再用readme提供模型替换继续训练，两者有什么区别么自己重新开始训练的话，有一定失败率，一定要在5k-15k之间观察attention图是否形成可见的直线 
AttributeError: 'HParams' object has no attribute 'dumpJson'突然出现的问题
本来好好地训练到了100k，但是中间暂停了一下试了一下模型，然后重新开始训练报错了Traceback (most recent call last):  File "D:\Mocking Bird\MockingBird-main\synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "D:\Mocking Bird\MockingBird-main\synthesizer\train.py", line 85, in train    hparams.dumpJson(weights_fpath.parent.joinpath(run_id).with_suffix(".json"))AttributeError: 'HParams' object has no attribute 'dumpJson'没学过python所以看不懂 ：( 求教5555已解决，我把两天前 train.py 的添加更新删了…………                # Try to scan config file        model_config_fpaths = list(weights_fpath.parent.rglob("*.json"))        if len(model_config_fpaths)>0 and model_config_fpaths[0].exists():            with model_config_fpaths[0].open("r", encoding="utf-8") as f:                hparams.loadJson(json.load(f))        else:  # save a config            hparams.dumpJson(weights_fpath.parent.joinpath(run_id).with_suffix(".json"))就是这一段 
关于synthesizer的疑问
大佬们，如果我只是想针对性模仿一个人的声音，能否训练一个只有那个人的synthesizer模型，如果可以的话大概需要多少的数据量。因为试过issue里260k的aidatatang_200zh的model感觉效果还是一般。 
“出现注意力模型”是什么意思？指图像出现线条吗？
我自己准备的数据量大概1GB，步长调到96跑了一天，现在15k了，Loss在几个小时之后就降到0.2之下了，现在是0.12出现的所有图谱都只是在最顶端有一条紧贴顶边的彩色，除此之外连个线头都没有OTZ是步长设置的太大了吗？如何接着别人的模型用自己的数据进行训练？是在预处理阶段把pretrained.pt换成作者提供的pretrained-11-7-21_75k.pt，还是在训练合成器开始之后生成的MockingBird\synthesizer\saved_models\run_id\run_id.pt中，把这个.pt文件换成pretrained-11-7-21_75k.pt之后继续训练合成器？> 我自己准备的数据量大概1GB，步长调到96跑了一天，现在15k了，Loss在几个小时之后就降到0.2之下了，现在是0.12 出现的所有图谱都只是在最顶端有一条紧贴顶边的彩色，除此之外连个线头都没有OTZ 是步长设置的太大了吗？> > 如何接着别人的模型用自己的数据进行训练？ 是在预处理阶段把pretrained.pt换成作者提供的pretrained-11-7-21_75k.pt，还是在训练合成器开始之后生成的MockingBird\synthesizer\saved_models\run_id\run_id.pt中，把这个.pt文件换成pretrained-11-7-21_75k.pt之后继续训练合成器？不是步长问题，而是从头训练的话对数据集的要求会高一点（一般是上百个小时的语音）回答第二个问题，你这个方法ok的，程序就是根据run id来找对应的文件，文件名改好就好。好的，想问一下您现在最好的模型是哪个？还是pretrained-11-7-21_75k.pt吗？最近会继续放出其他模型吗？> 好的，想问一下您现在最好的模型是哪个？还是pretrained-11-7-21_75k.pt吗？最近会继续放出其他模型吗？如果你是finetune训练的话，最好暂时应该是一个多数据集混合训练的25k。近期不会放新模型，可以关注issue网友不是，是我自己从视频里剪辑的，质量和数量都不太行，预处理之后只有一半能用OTZ------------------&nbsp;原始邮件&nbsp;------------------发件人: ***@***.***&gt;; 发送时间: 2022年3月1日(星期二) 下午4:54收件人: ***@***.***&gt;; 抄送: ***@***.***&gt;; ***@***.***&gt;; 主题: Re: [babysor/MockingBird] “出现注意力模型”是什么意思？指图像出现线条吗？ (Issue #399) 题主你好，我想请教一下你自己准备的1GB数据集是从那三个指定的中文数据集中分割出来的吗？是否方便给我一份你的1GB数据集？谢谢！ —Reply to this email directly, view it on GitHub, or unsubscribe.Triage notifications on the go with GitHub Mobile for iOS or Android. You are receiving this because you authored the thread.Message ID: ***@***.***&gt;> 这个pretrained-11-7-21_75k.pt直接用，也有很强烈的电音。我在它的基础上怎么训练，都消不掉电音。不知有没有更清楚一点的模型> > 好的，想问一下您现在最好的模型是哪个？还是pretrained-11-7-21_75k.pt吗？最近会继续放出其他模型吗？> > 如果你是finetune训练的话，最好暂时应该是一个多数据集混合训练的25k。近期不会放新模型，可以关注issue网友是readme里第二个需要切换到tag0.0.1的那个25k模型吗？> > 这个pretrained-11-7-21_75k.pt直接用，也有很强烈的电音。> > 我在它的基础上怎么训练，都消不掉电音。> > 不知有没有更清楚一点的模型我也是，朋友最近解决了这个问题么？按照视频走了一遍，但是效果和视频里的差距挺明显 
pre.py的预处理是否调用了GPU

如果新手不会，建议看已看这篇文章，详细的环境安装过程，内有爬坑
width="729" alt="image" src="https://user-images.githubusercontent.com/4476322/155311067-f0d13dea-bacc-4bd6-8502-891c2ee9929e.png">界面交互改为这样貌似好理解点。 
ouput无法正常生成音频
readme.md  中的 magicdata 数据集，但是随机了几个源， synthesizer使用ceshi  依然无法转换成正常的音频 
前所未有的问题……预处理时显示到100%，但是什么都没生成，什么情况啊
aidatatang_200zh: 100%|█████████████████████████████████████████████████████| 2247/2247 [00:08<00:00, 276.10speakers/s]The dataset consists of 0 utterances, 0 mel frames, 0 audio timesteps (0.00 hours).Traceback (most recent call last):  File "C:\WorkSpace\Project\MockingBird-main\pre.py", line 74, in <module>    preprocess_dataset(**vars(args))  File "C:\WorkSpace\Project\MockingBird-main\synthesizer\preprocess.py", line 88, in preprocess_dataset    print("Max input length (text chars): %d" % max(len(m[5]) for m in metadata))ValueError: max() arg is an empty sequence这是什么报错OTZ问了一圈好像没人遇到这个……数据集在其他MockingBird那里跑的是正常的，代码用的是最新版也应该没有问题，所以是环境的问题吗……环境是Anaconda3+python3.9+pytorch1.10.2，组件只装了readme里提过的那些OTZ更新：目前好像是解决了，是我音频路径放错了位置OTZ音频放在train/output里，我没建立Output文件夹不过现在还是不知道怎么基于别人的模型来继续训练……处理完之后就是运行了，这里的run id指定为别人的文件名即可开源者您好！反复看过readme说明之后，我觉得我可能对项目的功能有一些误解？希望能够得到您的解答！1.项目的功能是什么？我是看B站视频才知道的，在此之前我的理解是：制作某个人A的说话音频及其对应文本，而后在训练合成器模型过后，运行工具箱或web.py时，可以通过输入一小段A的音频样本，将文本用A的声音生成音频（用谁的声音进行训练，最后就能用谁的声音说话）但是看过Github上关于开源数据集的描述，预期状态是否是这样的：通过众多数据（许多人的声音+文本）训练合成器之后，输入一小段任意某个人的声音（比如A），再输入文本时，可以用这个人的声音生成音频？（收集足够多人的声音之后，随便来个人输入一小段声音后都能用这个人的声音说话）2.关于使用其他人的模型：我已经制作了一个人的数据集，放到aidatatang_200zh文件夹中，执行预处理+训练合成器后，效果不太理想，因此想接着其他人的训练集继续训练，该如何使用？我已经下载了pretrained-11-7-21_75k.pt，那么我是需要在预处理阶段替换encoder\saved_models\pretrained.pt？还是用这个替换synthesizer\saved_models\run_id\run_id.pt之后执行python synthesizer_train.py run_id？3.关于hparams.py中tts_schedule的部分，您分享经验时提到50k步后loss小于0.4，tts_schedule = [(2,&nbsp; 1e-3,&nbsp; 10_000,&nbsp; 12),&nbsp; (2,&nbsp; 5e-4,&nbsp; 15_000,&nbsp; 12),...]这里是跑完10K步之后自动保存一次模型然后再跑15k步吗？50k步是10k+15k+20k...累加的结果吗？4.我想最大程度复刻A的声音，但是A的数据只有1000多条，那么如果我加入其他数据集（比如18G的aidatatang_200zh），会影响还原A音色吗？是否需要对hparams.py中的参数进行一些调整？我没学过机器学习相关的知识，但是现在能尝试使用相关的技术，非常感谢您的开源付出！------------------&nbsp;原始邮件&nbsp;------------------发件人:                                                                                                                        "babysor/MockingBird"                                                                                    ***@***.***&gt;;发送时间:&nbsp;2022年2月22日(星期二) 晚上9:07***@***.***&gt;;***@***.******@***.***&gt;;主题:&nbsp;Re: [babysor/MockingBird] 前所未有的问题……预处理时显示到100%，但是什么都没生成，什么情况啊 (Issue #395) 处理完之后就是运行了，这里的run id指定为别人的文件名即可 —Reply to this email directly, view it on GitHub, or unsubscribe.Triage notifications on the go with GitHub Mobile for iOS or Android. You are receiving this because you authored the thread.Message ID: ***@***.***&gt; 
安装pytorch问题
import 
【作者快看】大家觉得哪个模型好啊？
大家好，我现在用的这个模型和原声太不像了，大家有那种克隆更真实的模型推荐吗？能发我一下吗？谢谢各位了用过很多模型，大部分模型出来的结果都是差强人意，即使是用数据集里的语音作为素材，输入一段短文输出的话效果也非常差，还不如机翻的效果或者差不多，噱头多哦哦，谢谢大佬这个项目的方法还是受限于训练数据，当然数据越大越好；因为是“二阶段”模型，所以最终音质和自然度受到中间变量的匹配问题。你想要更像一些模型。可以看下这个项目https://github.com/CMsmartvoice/One-Shot-Voice-Cloning 
作者快看
作者，我给你看一眼这张照片，你看是什么原因![屏幕截图 2022-02-15 作者，那你推荐哪个模型?发我一下拉最新代码， readme分享的我的那个，效果虽然不是最好，但后面你可以自己练。当然也可以找下issue里面其他大哥分享的 
数据集预处理时报错
1.如果我在train目录下新建一个文件夹，并把音频文件放进去的话，pre.py就会一直卡住不动：Using data from:    d:\asoul\aidatatang_200zh\corpus\trainaidatatang_200zh:   0%|                                                                    | 0/1 [00:00<?, ?speakers/s]2.如果我把音频文件直接放在tran目录下就会报这样的错误：Using data from:    d:\asoul\aidatatang_200zh\corpus\trainaidatatang_200zh: 100%|█████████████████████████████████████████████████████| 1226/1226 [00:05<00:00, 241.65speakers/s]The dataset consists of 0 utterances, 0 mel frames, 0 audio timesteps (0.00 hours).Traceback (most recent call last):  File "D:\MockingBird\pre.py", line 74, in <module>    preprocess_dataset(**vars(args))  File "D:\MockingBird\synthesizer\preprocess.py", line 88, in preprocess_dataset    print("Max input length (text chars): %d" % max(len(m[5]) for m in metadata))ValueError: max() arg is an empty sequence已解决 应采用第一个办法并需要等待一段时间train目录下的音频是什么格式的音乐，gz文件吗还是wav的？我是gz文件无法识别，还要一个一个解压到wav格式吗？谢谢大佬！ 
Hifigan Support train from existed checkpoint.
Already tested with no problem. 
aishell2 aidatang_200zh 哪个好？
如题，按数据量来说是 aishell2 更多实测在本项目中aidatatang 有优势，估计是说话人的特征覆盖面会广一点（采样比较平均分散？），训练出来的模型在克隆上会更贴近一点。 
作者，有BUG
作者，我今天发现了一个bug（应该是），我的这个MockingBird在合成后一直出杂声，刷新了好几次都不行，不同的模型也都试了，都不行，同样的音频在别人那里就可以，在我这里就不行，全是杂音，根本就不能听，麻烦作者帮我一下，修复一下这个问题，如果不是BUG，也麻烦各位大佬帮帮我，谢谢了，Github也发不了图片，作者或者大佬们看到这个帖子想问我要图片可以加我QQ：1685415568English：（机翻）Author, I found a bug today (it should be). My Mockingbird has been making noise after synthesis. It can't be refreshed several times. Different models have tried, but it can't be. The same audio can be used by others, but it can't be used by me. It's all noise and can't be heard at all. Please help me fix this problem, If it's not a bug, please help me. Thank you. GitHub can't send pictures. When the author or leaders see this post and want to ask me for pictures, they can add my QQ: 1685415568github发图片，直接把图片拖到文字输入框，等待上传结束即可![Uploading 屏幕截图 2022-02-15 2022-02-15 
各位大佬！帮我看看这是什么错误
D:\迅雷下载\MockingBird-main>python demo_toolbox.pyArguments:    datasets_root:    None    enc_models_dir:   encoder\saved_models    syn_models_dir:   synthesizer\saved_models    voc_models_dir:   vocoder\saved_models    cpu:              False    seed:             None    no_mp3_support:   FalseWarning: you did not pass a root directory for datasets as argument.The recognized datasets are:        LibriSpeech/dev-clean        LibriSpeech/dev-other        LibriSpeech/test-clean        LibriSpeech/test-other        LibriSpeech/train-clean-100        LibriSpeech/train-clean-360        LibriSpeech/train-other-500        LibriTTS/dev-clean        LibriTTS/dev-other        LibriTTS/test-clean        LibriTTS/test-other        LibriTTS/train-clean-100        LibriTTS/train-clean-360        LibriTTS/train-other-500        LJSpeech-1.1        VoxCeleb1/wav        VoxCeleb1/test_wav        VoxCeleb2/dev/aac        VoxCeleb2/test/aac        VCTK-Corpus/wav48        aidatatang_200zh/corpus/dev        aidatatang_200zh/corpus/test        aishell3/test/wav        magicdata/trainFeel free to add your own. You can still use the toolbox by recording samples yourself.我已经把对应的模板放入相应的文件夹中了还是不行、要输入多一个参数：datasets_root什么是datasets_root> 什么是datasets_root对啊，这个参数输入什么地址？ 
对声码器的展望
我发现合成器在达到一定数量的训练后，对质量影响更大的反而是声码器，hifigan能用更好的效果但始终有电噪音，现在有些新的项目采样lpcnet而非wavernn，据说是复杂度要远低于wavernn，但质量优于wavernn，可以达到类似hifigan的效果而不带电噪音，请问有考虑过引入lpcnet之类的新声码器的计划呢> 我发现合成器在达到一定数量的训练后，对质量影响更大的反而是声码器，hifigan能用更好的效果但始终有电噪音，现在有些新的项目采样lpcnet而非wavernn，据说是复杂度要远低于wavernn，但质量优于wavernn，可以达到类似hifigan的效果而不带电噪音，请问有考虑过引入lpcnet之类的新声码器的计划呢非常好的建议，你这边有推荐的paper或repo吗？如果还有后续讨论请移步 #436 
两种预置声码器各有优缺点，该在什么方向上改进？
预置的两种声码器g_hifigan和pretained，用g_hifigan的生成出来的音频，音色特别准，但是会带有电音用pretained的生成出来的音频，音色没那么准，音量也会变小，但是就不会带电音这种问题应该往哪个方向去改进？使得结果两种优点都具有是声码器训练问题?还是源音频的问题？还是合成器？可以看看最新一些论文研究，vocoder目前大家还是比较倾向hifigan, 学术界应该有一些优化模型，欢迎升级本项目的hifigan或新增一些其他的尝试。 
小白發問 訓練的用意?
我之前有研究過DeepFaceLab換臉屬於專人專模所以每換一個人, 都需要再特別訓練這個人的模型今天剛開始研究MockingBird也下載了網友提供的已訓練模型但很妙的是, 只需要提供本地的一小段音訊來源, 就能有不錯的合成效果既然如此, 有必要特別去訓練一個新的模型嗎? (或是特地製作一個新的資料集/語料庫)如果你想要专门产生某个人的特化模型，只需要很小的数据集就能训练出更唯妙唯俏的模型，另外，模仿的效果受数据集影响，有些数据集中没有的字词可能没法很好的发音@AyahaShirane 感謝解答另外我想問我已下載了 @FawenYo (台灣口音) 的模型原本開發者提供的數據集都是大陸口音如果我想特化的人是台灣口音的話1. 那麼我除了準備此人的數據集之外, 是否需要跟原本開發者提供的數據一起訓練?還是單純練此人的就可以了?2. 我是否可以用別人已練好的模型為底子, 用新的數據集(我想特化的對象)繼續訓練?专项训练参照这个视频MockingBird数据集制作教程-手把手教你克隆海子姐的声线_哔哩哔哩_bilibili<https://www.bilibili.com/video/BV1dq4y137pH>实测在已有模型基础上训练20K左右就能改变成想要的语音语调了。你如果是想要泛用型台湾口音的话，就尽可能收集更多人的数据集，否则会偏向特定某一个人的口音，而且断句和停顿似乎也会受到新数据集的影响Sent from Mail<https://go.microsoft.com/fwlink/?LinkId=550986> for WindowsFrom: ***@***.***>Sent: 2022年2月11日 1:28To: ***@***.***>Cc: ***@***.***>; ***@***.***>Subject: Re: [babysor/MockingBird] 小白l 的用意? (Issue #380)@AyahaShirane<https://github.com/AyahaShirane>感x解答另外我想我已下d了 @FawenYo<https://github.com/FawenYo> (台晨谝) 的模型原本_l者提供的际谴箨口音如果我想特化的人是台晨谝舻脑  1.  那N我除了浯巳说, 是否需要跟原本_l者提供的黄鹩?是渭此人的就可以了?  2.  我是否可以用e人已好的模型榈鬃, 用新的(我想特化的ο)^m?―Reply to this email directly, view it on GitHub<https://github.com/babysor/MockingBird/issues/380#issuecomment-1035205730>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AT53I7Q3RN5FDDFTZQMU4NTU2PYVDANCNFSM5OAGBC7A>.Triage notifications on the go with GitHub Mobile for iOS<https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android<https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.You are receiving this because you were mentioned.Message ID: ***@***.***>已被收集至 great thanks to @AyahaShirane 
fixed the issues #372
修复了一些参数传递造成的问题，把过时的torch.nn.functional.tanh()改成了torch.tanh()，本地测试可用 
指定librosa版本
0.9.0版本的librosa升级后会报错，限定为0.8.1版本 
TypeError: load() takes 1 positional argument but 2 were given报错怎么解决呢重装Anaconda也无用
TypeError: load() takes 1 positional argument but 2 were given找到了个临时的办法，把librosa库进行降级在命令栏中输入  的话我就不再出现这个错误了感谢大佬，通过pip install librosa==0.8.1实测有效 
loss要训练到多少才算合格？目前快100K了，loss在0.28上下浮动，感觉下不去了。
用的是aidatatang_200zh，如果想要继续降loss，是不是该换个数据集然后继续训练？混合多个数据集试试> 混合多个数据集试试好的，谢谢 
无法读取WAV或MP3音频
 这该如何处理？我第一次使用的时候没这个问题，后来借着学习这个项目入坑了python，过了两天反而出现GUI无响应的问题。重装python之后，就变成这个问题了，不知道是不是因为有什么库冲突了。。。。我也是这个问题！！#372 
UnicodeDecodeError and 工具箱传录音错误
运行web.py时能打开浏览器跳转网页之后报错File "C:\Users\17935\AppData\Local\Programs\Python\Python37\lib\site-packages\gevent\threadpool.py", line 167, in __run_task    thread_result.set(func(*args, **kwargs))UnicodeDecodeError: 'utf-8' codec can't decode byte 0xdf in position 2: invalid continuation byte然后用工具箱打开，传入录音时报错File "C:\Users\17935\PycharmProjects\MockingBird-main\synthesizer\inference.py", line 146, in load_preprocess_wav    wav = librosa.load(str(fpath),hparams.sample_rate)[0]TypeError: load() takes 1 positional argument but 2 were given小白求解答https://github.com/babysor/MockingBird/wiki/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AFduplicated 
load() takes 1 positional argument but 2 were given
导入音频文件出现load() takes 1 positional argument but 2 were given错误提示我也遇到了这个问题把D:\Code\MockingBird-main\synthesizer.py 的146行改成  能解决一部分问题，但还会有新的bug我在python pre.py <datasets_root> 这一步出现了该错误> 把D:\Code\MockingBird-main\synthesizer.py 的146行改成  能解决一部分问题，但还会有新的bug但我synthesizer.py文件为什么都没有录音完成后出现错误： 我的python版本是3.9我直接修改了librosa这个库的源代码：1. 把site-packages/libsora/filter.py中的 改为 也就是把传入参数的星号去掉2.同理，把把site-packages/libsora/feature/spectral.py中的 改成 以上是笨方法，因为不知道为什么这里会语法错误。不过改了之后demo可以运行并合成语音了。同问，在录入语音时报错，之前都没有这种情况的，今天突然出现这样，有没有大牛指导一下是怎么回事，感谢Traceback (most recent call last):  File "E:\Downloads\MockingBird-main\toolbox\__init__.py", line 103, in <lambda>    func = lambda: self.load_from_browser(self.ui.browse_file())  File "E:\Downloads\MockingBird-main\toolbox\__init__.py", line 170, in load_from_browser    wav = Synthesizer.load_preprocess_wav(fpath)  File "E:\Downloads\MockingBird-main\synthesizer\inference.py", line 146, in load_preprocess_wav    wav = librosa.load(str(fpath), hparams.sample_rate)[0]TypeError: load() takes 1 positional argument but 2 were given同问 我也是导入音频报的错  之前还能正常用 今天重装了一下环境配置好了就出现了这个错误 不知道是缺了什么音频组件还是啥的xdm，看pull questions第一条，已经修了bug了https://github.com/babysor/MockingBird/pull/379感谢感谢！！！👍，高效的网友 
不能运行pre.py 怎么办呀大佬们
D:\MockingBird-main\MockingBird-main>python pre.py D:\shujuchuliUsing data from:    D:\shujuchuli\aidatatang_200zh\corpus\trainTraceback (most recent call last):  File "pre.py", line 74, in <module>    preprocess_dataset(**vars(args))  File "D:\MockingBird-main\MockingBird-main\synthesizer\preprocess.py", line 64, in preprocess_dataset    for v in dict_transcript:  File "D:\python\lib\codecs.py", line 322, in decode    (result, consumed) = self._buffer_decode(data, self.errors, final)UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb6 in position 10: invalid start bytehttps://github.com/babysor/MockingBird/wiki/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF 
没有torch._C
torch没安装安装了呀，为什么我看那个库里只有__开头的函数？> > torch没安装> > 安装了呀，为什么我看那个库里只有__开头的函数？重新开虚拟环境装一下最新版本试试？你这个应该是运行环境有问题 
How can I call the function Synthesize And Vocode with Python
Hi, friend. I'd like to write a software program,but I don't kown how to call the function **Synthesize And Vocode** with Python. Could you tell me how to do it?You can check the  methods in source code.Ok, let me try. 
提示无法正常运行
我换了一台电脑继续运行运行以下命令：python C:\MockingBird-main\demo_toolbox.py -d .\samples后提示：Traceback (most recent call last):  File "C:\MockingBird-main\demo_toolbox.py", line 2, in <module>    from toolbox import Toolbox  File "C:\MockingBird-main\toolbox\__init__.py", line 1, in <module>    from toolbox.ui import UI  File "C:\MockingBird-main\toolbox\ui.py", line 16, in <module>    import umapModuleNotFoundError: No module named 'umap'于是我又换了种说法:python C:\MockingBird-main\demo_toolbox.py又提示：Traceback (most recent call last):  File "C:\MockingBird-main\demo_toolbox.py", line 2, in <module>    from toolbox import Toolbox  File "C:\MockingBird-main\toolbox\__init__.py", line 1, in <module>    from toolbox.ui import UI  File "C:\MockingBird-main\toolbox\ui.py", line 16, in <module>    import umapModuleNotFoundError: No module named 'umap'这台新机器是双显版本1.Intel iris Xe 11代酷睿搭载 2.NVDIA GeForce MX350 CUDA版本为11.6.99对于小白来说，请问如何解决？？？python版本为3.9.10可以尝试运行 pip install umap不然的话建议回滚python到3.8谢谢 
loss只有0.14，但是注意力模型一直是一条直线
很遗憾是的，如果没正确形成注意力模型，loss是没有意义的我重新跑了三个模型，都没有收敛的情况，请问是不是因为数据集的问题，数据集是我自己制作校对的，但是只有700条左右是的，数据集的语音总和长度要在百小时级别才能有保障。你这种情况，要在别人练好的基础上继续跑你自己的数据> 是的，数据集的语音总和长度要在百小时级别才能有保障。你这种情况，要在别人练好的基础上继续跑你自己的数据请问大佬，要咋在别人训练好的基础上跑，我替换了别人的PT，就不能运行了。这一步不知道具体操作，求教saved_models、模型文件下，直接替换别人的70K，就运行不了训练了> 是的，数据集的语音总和长度要在百小时级别才能有保障。 你这种情况，要在别人练好的基础上继续跑你自己的数据大佬，想请教，我不能继续别人的模型训练，都说是模型和软件版本不匹配，这怎么解决？诚心求教> > 是的，数据集的语音总和长度要在百小时级别才能有保障。 你这种情况，要在别人练好的基础上继续跑你自己的数据> > > 是的，数据集的语音总和长度要在百小时级别才能有保障。 你这种情况，要在别人练好的基础上继续跑你自己的数据> > 大佬，想请教，我不能继续别人的模型训练，都说是模型和软件版本不匹配，这怎么解决？ 诚心求教> > > > 是的，数据集的语音总和长度要在百小时级别才能有保障。 你这种情况，要在别人练好的基础上继续跑你自己的数据要根据模型，切换到对应的软件版本，学一下git 
最后一步GUI打不开，没有界面
模型已经导入之后，但是在cmd运行demo finished with exit code -1073740940 (0xC0000374) 
将模型文件导入无法运行
我将模型文件导入后，总是报错：Error: Model files not found. Please download the models难道模型文件不是放在：C:\MockingBird-main\synthesizer\saved_models里面吗？？？（我的文件放在C盘根目录下面）是的啊，是不是没正确解压之类的不就是将.pt文件放到C:\MockingBird-main\synthesizer\saved_models这个目录下吗> 不就是将.pt文件放到C:\MockingBird-main\synthesizer\saved_models这个目录下吗这个文件完整下载后应该是几百兆353MB 在小白新手教程下的https://github.com/babysor/MockingBird/wiki/Quick-Start-(Newbie)另一个为501MB 在https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g下的 就是首页模型库那里都不行，是不是我的台式机太拉了，显卡是 Intel HD Graphics 510> 353MB 在小白新手教程下的https://github.com/babysor/MockingBird/wiki/Quick-Start-(Newbie) 另一个为501MB 就是首页模型库那里都不行，是不是我的台式机太拉了，显卡是 Intel HD Graphics 510没有显卡也可以运行，并且显卡问题也不是提示这个错误。建议检查一下文件路径，例如文件夹名称是否有错 
【Uncaught Error】Uncaught TypeError: Cannot read properties of undefined (reading 'value')
谢谢 测试成功 
只有 CPU的情況, 感覺 web 效果比 toolbox 好一點
web 和 toolbox 都可以運行但感覺 web 效果比 toolbox 好一點toolbox 的電流音比較明顯而多而且有時候輸入的是男聲, 他輸出的變成女聲了已有下載 dataset, 是用最新版本, 用 pretrained的 data.相似度仍是有很大的距離,是要自己 train 嗎?輸入的 wav 再長一點有用嗎? 通常多久最好?另有一點奇怪, web 好像不用 datasets,  但為什麼效果會更好呢?剛在試了一下沒給 -d -d E:\data  , 也可以正常合成啊@_@明白了那 dataset 在 toolbox 中是如何用的了><要先After unzip aidatatang_200zh, you need to unzip all the files under aidatatang_200zh\corpus\devAfter unzip aidatatang_200zh, you need to unzip all the files under aidatatang_200zh\corpus\test才可以用到> 另有一點奇怪, web 好像不用 datasets, 但為什麼效果會更好呢?dataset跟web和toolbox没啥关系呀 
运行pre.py的时候可以使用gpu 么
运行pre.py的时候查看nvidia-smi并没有使用GPU，是靠CPU在处理的，这一步应该怎么改才能使用GPU？会有部分使用，你可以看下这部分代码，比起torch部分还简单点 
只有CPU,并且无法正常使用的看这里
本人使用的预训练文件:     @miven   也就是这位up的  :   2秒杂音, 无法编码(在98%后卡住)如果你出现的问题和我一样,可以尝试和我一样步骤首先是这个   将文件  中的:  和  均设置为 然后是这个   中的 改为 由于我是直接使用  命令直接运行的所以我没有测试   这个是否有效的确可以解决问题厉害，可用，谢谢感谢，可用，再搭配这个https://github.com/babysor/MockingBird/issues/249#issuecomment-991498945非常感谢，可以解决问题 
为啥我的训练目录下面plot是空的
按B站上“克隆海子姐”的教程做的。最后一步是将训练停止，用“pretrained-11-7-21_75k.pt”这个文件替换已经生成的pt文件，接着作者的模型继续训练。这里我有几个疑问。1. 替换后的文件，随着训练的进行，文件大小不变，这个正常吗？2. 训练开始以后，plots文件夹下面始终都是空的。现在是Epoch：3/3864。目前还是空的。3. 之前没有替换pt文件时，plots文件夹也是空的，即使训练进行到 Epoch: 16/455 (7/22)。谢谢指教！500条会保存一次，会生成预览图 
安卓📱Android成功运行本项目
设备：小米平板5 6+128GB处理器：骁龙860（等同骁龙855+的性能）软件：AidLux（酷安可以搜索下载）以下是官网下载链接https://www.aidlux.com/product?anchor=software软件内置Linux环境，使用aarch64架构的Debian以下是运行截图![Screenshot_2022-01-25-16-12-44-720_com size几秒钟就会导致设备完全卡死，改为batch size 6以后坚持了一分钟。推测是由于设备可用内存太少导致的。本机虽然有6GB RAM，但由于MIUI的臃肿导致可用的实际RAM并不多。3.默认的nunpy无法运行，需要pip直接安装最新的使用。仅供娱乐参考，有更好玩的也可以在在这个软件里面运行，软件介绍是支持GPU的，不过我目前还没办法在本项目调用。如果已经有可用的模型了的话，有可能做成个安卓tts软件吗？ 
Record报错
Archlinux系统用Docker使用archlinux镜像，(主要是依赖太多了，因此想用docker，防止运行失败产生过多无用依赖，真希望可以出一个官方Dockerfile)，按照快速开始(友好版)教程运行后，在点击Record录制的时候，终端报错Error querying device -1，并且无法停止，只能关闭程序音频播放设备没有找到的问题，考虑搞个虚拟的> 音频播放设备没有找到的问题，考虑搞个虚拟的还是点Record的时候报错Error opening InputStream: Invalid sample rate [PaErrorCode -9997]请问这是什么原因，怎么才能解决那就是音频输入设备未弄好，要搞下驱动 
以后会不会扩展其他语言
比如说,emmm,日语,你懂的 请后续关注 
data_parallel_workaround 这个方法没有定义
是不是漏掉了这个？导致多显卡不能用 
mac可以用吗
m1 能不能跑得动可以的，搜一下issue里m1 
成功运行之后，对于结果的一个疑问
为什么，同样的音频文件，同样的话术，每次运行出来的结果有好有坏，每次都不大一样> 为什么，同样的音频文件，同样的话术，每次运行出来的结果有好有坏，每次都不大一样你可以看下推理 inference的逻辑，是有一定的随机因子输入 
【求助】预训练后没有生成任何新数据
刚开始时是运行中有显示“页面文件太小”出错，但是在调整pagefile.sys之后，仍然没产生任何新文件。前半段是这样的F:\zhivoice\MockingBird-main>python pre.py F:/zhivoice -d aidatatang_200zh -n 6Using data from:    F:\zhivoice\aidatatang_200zh\corpus\trainaidatatang_200zh:   0%|                                                                    | 0/1 [00:00<?, ?speakers/s]no wordSno wordSno wordSno wordSno wordSno wordSno wordSno wordS中间全部都是 no wordS后半段是这样的no wordSno wordSno wordSno wordSno wordSno wordSno wordSaidatatang_200zh: 100%|████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.15s/speakers]The dataset consists of 0 utterances, 0 mel frames, 0 audio timesteps (0.00 hours).Traceback (most recent call last):  File "F:\zhivoice\MockingBird-main\pre.py", line 74, in <module>    preprocess_dataset(**vars(args))  File "F:\zhivoice\MockingBird-main\synthesizer\preprocess.py", line 88, in preprocess_dataset    print("Max input length (text chars): %d" % max(len(m[5]) for m in metadata))ValueError: max() arg is an empty sequenceF:\zhivoice\MockingBird-main>步骤都是按照教程一步步来的，恳请大佬指点一下，花了好长时间卡在这里实在没办法了教程里面有提到 解压 aidatatang_200zh\corpus\train 下的压缩文件，你做了吗？> 教程里面有提到 解压 aidatatang_200zh\corpus\train 下的压缩文件，你做了吗？请问这个需要下载那个18G的原始数据集么？文档里提到解压所有的压缩文件，但是我看视频里是把处理后的数据集复制进去直接预处理和训练，并替换模型有点疑惑，没搞明白 
wav音频格式问题
自己win10录音的数据，转为wav后，比特率1411kbps，太高了，想获得像原始训练数据集那样256kbps的数据，应该怎么获取呢 
【求助】录音完成播放声音之后程序未响应，排查错误在inference.py文件
执行到这一句之后就未响应了_model = SpeakerEncoder(_device, torch.device("cpu"))解决了步骤一：pip uninstall torch步骤二：pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f 
该项目由克隆英文改进为克隆中文系统哪些模块进行了改动？
如题可以看下issue 知乎也有一篇量子位的文章有介绍 
无法成功合成声音，只有电流声
为啥只有电流声呀，web和tool都试了，调整style就不会报错了，但是一直不能成功读出声音，只有电流声好像是模型的问题。链接: 提取码: 3tws 试试这里面的那个模型，放到synthesizer里面的saved_models里面去> 好像是模型的问题。 链接: 提取码: 3tws 试试这里面的那个模型，放到synthesizer里面的saved_models里面去谢谢啊，我昨天已经解决了，是因为模型，代码版本不兼容之间的问题，按照作者最新代码和pretrained-11-7-21_75k.pt这个模型，其他两个模型都用pretrained.pt，就没问题了能成功的模型从哪里下载啊 ceshi这个模型好像是问题模型> 你好，我也遇到了同样的问题，用的是作者的wiki链接里面的模型，不知道怎么会出来电流声，太鬼畜了> > > > 你好，我也遇到了同样的问题，用的是作者的wiki链接里面的模型，不知道怎么会出来电流声，太鬼畜了模型名是？ 
安装过程中遇到的各种问题，求帮助
尝试时用@miven的数据集。没有自己的合成器。下载好以后，分别放入encoder，synthesizer, vocoder下的saved_models目录。后面下载了aidatatang_200zh，放到了mockingbird\samples 下面。GPU比较老，虽然支持CUDA，但是torch还是报错，后面用了CPU版本。最后使用命令：python demo_toolbox.py --cpu -d Only 如图，然后再次点击“Synthesize Only”，显示： 然后不管选哪个vocoder，都是合成噪音。PowerShell 
是否可以支持DirectML  pytorch-directml?
是否可以支持DirectML  pytorch-directml? 
模型声音问题
使用别人训练好的模型 但是我录入不同人的声音出来的都是同一种声音（输出的声音与我录入的各种声音均不相同） 请问这是模型问题吗  如果想输出各种声音 大概需要训练到什么程度呢？要下载数据集请问是需要下载数据集重新进行训练吗 只下载，不训练。在命令行参数内指定目录即可。要和原有声音进行比对请问可以细说一下吗 新手不太懂 万分感谢下载Readme里的数据集，解压在一个目录里，用 告知程序我再试试 感谢大佬按readme里用的预训练模型，但是出来的结果都是电音，效果很差。下载数据没有重新训练还是不行。我试下重新训练吧。切换下Style试试> 切换下Style试试我也是全是电音，不明白为什么，我用马保国的声音，然后写一段文字，想马保国的声音读出来，结果是电流声，你测试成功了嘛> 切换下Style试试一定要下载数据集，用数据集中的声音来测试嘛？不能使用任意网络上的声音嘛？使用别的声音需要下载数据集，输出声音音色与数据集无关，但是数据集是必要的，用来让模型明白“马保国”和预训练模型的差异之处，才能将输出转成你要的“马保国”。> 我看到有aidatatang_200zh，magicdata，aishell3下其中一个还是需要全部下载呢，数据集实在太大了，magicdata稍微小一些任选其一即可> 任选其一即可我试着下了一个数据集，依然是电流声，我直接用的就是数据集的声音，还是读不出相应的文字，我觉得应该不是数据集的原因了我这里正常，你给程序加参数了吗> 我这里正常，你给程序加参数了吗这是我的程序界面，确实看不出问题了，但是出来的声音都是猪叫或者电流声用的是自己Record的吗？> 用的是自己Record的吗？我发现问题了，好难过，我看教程说有个代码需要修改为_characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz12340!\'(),-.:;? '，我发现这样是错的，会让声音变成电流声，如果用作者原始的模型，_characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz1234567890!\'(),-.:;? '，用这个就好，不需要修改，好像不加载数据集也可以，直接用享用的声音，只要模型对就好> 用的是自己Record的吗？感谢您慷慨的回答和帮助，谢谢啊你现在可以克隆自己输入的声音了？看Readme和Wiki萌新探头 
@FawenYo语音模型百度网盘下载链接失效
_**同上! !**_ 
Linux环境下 运行Python web.py后点击上传合成按钮报错ValueError: operands could not be broadcast together with shapes (2200,) (4000,) (2200,)
#代码版本是最新main！ 我或许已经找到了问题所在，可能是模型兼容性问题，详情请查看这篇回答：https://github.com/babysor/MockingBird/issues/37 
训练自己模型正常  可是替换成  社区提供的模型  不一会 就报错然后自动终止（小白求教）
提供的免费模型  我都用了，但无一例外全部报错  ，最后的括号出现（70，512）或者（512，512）或者（0，512）   有人说模型版本和软件版本不配套   求大神 帮忙 解答下  这个该怎么解决。自己训练的模型  实在是不会自己收敛   搞出来的五六百兆的模型根本没法用 一样的问题，同求大佬指导同求，有没有大佬帮忙一下#437 #209 
issue
D:\AI_python\MockingBird-main>python demo_toolbox.pyArguments:    datasets_root:    None    enc_models_dir:   encoder\saved_models    syn_models_dir:   synthesizer\saved_models    voc_models_dir:   vocoder\saved_models    cpu:              False    seed:             None    no_mp3_support:   FalseWarning: you did not pass a root directory for datasets as argument.The recognized datasets are:        LibriSpeech/dev-clean        LibriSpeech/dev-other        LibriSpeech/test-clean        LibriSpeech/test-other        LibriSpeech/train-clean-100        LibriSpeech/train-clean-360        LibriSpeech/train-other-500        LibriTTS/dev-clean        LibriTTS/dev-other        LibriTTS/test-clean        LibriTTS/test-other        LibriTTS/train-clean-100        LibriTTS/train-clean-360        LibriTTS/train-other-500        LJSpeech-1.1        VoxCeleb1/wav        VoxCeleb1/test_wav        VoxCeleb2/dev/aac        VoxCeleb2/test/aac        VCTK-Corpus/wav48        aidatatang_200zh/corpus/dev        aidatatang_200zh/corpus/test        aishell3/test/wav        magicdata/trainFeel free to add your own. You can still use the toolbox by recording samples yourself.Traceback (most recent call last):  File "D:\AI_python\MockingBird-main\demo_toolbox.py", line 43, in <module>    Toolbox(**vars(args))  File "D:\AI_python\MockingBird-main\toolbox\__init__.py", line 75, in __init__    self.reset_ui(enc_models_dir, syn_models_dir, voc_models_dir, seed)  File "D:\AI_python\MockingBird-main\toolbox\__init__.py", line 143, in reset_ui    self.ui.populate_models(encoder_models_dir, synthesizer_models_dir, vocoder_models_dir)  File "D:\AI_python\MockingBird-main\toolbox\ui.py", line 340, in populate_models    raise Exception("No synthesizer models found in %s" % synthesizer_models_dir)Exception: No synthesizer models found in synthesizer\saved_models这个是为什么啊？哪位大佬可以帮下，最后一步了。。模型路径没放对， 
小白求教  训练模型 刚开始正常  两三秒之后就报错RuntimeError: CUDA out of memory. 
小白求教  训练模型 刚开始正常  两三秒之后就报错-  RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 1.02 GiB already allocated; 3.96 MiB free; 1.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF然后便自行终止   求大神帮忙看看  怎样解决显卡显存太小了。这个至少要4GB差不多我是4GB 的显存为何也报同样的错？最低要求4g，有方法改，但建议提高显存更方便Lafayette ***@***.***> 于2022年12月11日周日 23:28写道：> 我是4GB 的显存为何也报同样的错？>> —> Reply to this email directly, view it on GitHub> <https://github.com/babysor/MockingBird/issues/340#issuecomment-1345583201>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AZ6O4WWYLA2M4DPWDD2YKGTWMXXJ7ANCNFSM5L6EJW6A>> .> You are receiving this because you are subscribed to this thread.Message> ID: ***@***.***>>-- jackchen 
如何使用aishell数据集
如何使用aishell数据集，一直报错什么错误呢？参考训练攻略了嘛？ 
小白求助，前面所有步骤ok，运行demo_toolbox.py时Error: Model files not found.
(pytorch) C:\Users\666>python D:\MockingBird-main\demo_toolbox.pyArguments:    datasets_root:    None    enc_models_dir:   encoder\saved_models    syn_models_dir:   synthesizer\saved_models    voc_models_dir:   vocoder\saved_models    cpu:              False    seed:             None    no_mp3_support:   False********************************************************************************Error: Model files not found. Please download the models********************************************************************************试了好多方法都不行。各位大佬，这种问题应该怎么操作？之前有一位仁兄，同样的报错，通过“在项目根目录下运行demo_toolbox.py文件”解决了该问题，可是本人小白，不知道应该怎么具体操作，请各位大佬指导详细一点，谢谢先输入一次cd D:\MockingBird-mainAnaconda Prompt只能在根目录下进行切换盘符要使用cd ..切换到上级文件夹然后cd D:\MockingBird-main最后python demo_toolbox.py完美运行> 先输入一次cd D:\MockingBird-main感谢大佬，看了你的指导，已经跑起来了 
现在支持双GPU训练吗？
抱歉，还没来得及仔细读代码，先在这问一下。现在的代码是否支持双GPU训练？刚看了代码，好像只支持单GPU训练。。。。。其实pytorch是支持的， 但是需要共享一些变量，没双卡也没来得及搞> 好像可以双卡，我试了batchsize=48，两个显卡各用了10G左右> 你好,问一下 安装的cuda和对应的源都是支持 GPU训练 可以使用别人提供的模型进行播放    但要训练自己的模型 出现CPU满跑 GPU 不进行运算 显卡是2060 请问 GPU训练 是需要改动那些文件么> 解决了吗？你可以看一下#501，最近也在学习这个 
上传本地音频文件后程序无响应。停留在Loading the encoder encoder\saved_models\pretrained.pt...这一步
停留在Loading the encoder encoder\saved_models\pretrained.pt...这一步无法继续进行。 
麻烦帮忙看下这算不算正常收敛，需不需要重新训练

小白求助：Mocking Bird GUI运行python demo_toolbox.py的时候报错
Mocking Bird GUI运行python demo_toolbox.py的时候报错，显示没有“torch”moduleD:\MockingBird jiaocheng\MockingBird-main>python demo_toolbox.pyTraceback (most recent call last):File "D:\MockingBird jiaocheng\MockingBird-main\demo_toolbox.py", line 2, in <module>from toolbox import ToolboxFile "D:\MockingBird jiaocheng\MockingBird-main\toolbox\__init__.py", line 1, in <module>from toolbox.ui import UIFile "D:\MockingBird jiaocheng\MockingBird-main\toolbox\ui.py", line 7, in <module>from encoder.inference import plot_embedding_as_heatmapFile "D:\MockingBird jiaocheng\MockingBird-main\encoder\inference.py", line 2, in <module>from encoder.model import SpeakerEncoderFile "D:\MockingBird jiaocheng\MockingBird-main\encoder\model.py", line 5, in <module>from torch.nn.utils import clip_grad_norm_ModuleNotFoundError: No module named 'torch'请问有大佬指导下吗wwpytorch没装有没有闪退呢，我之前启动GUI闪退了然后，按照作者这个方法就可以了，链接：https://github.com/babysor/MockingBird/wiki/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF 
关于中文的Text to Sequence处理部分
我想问下， 我看到Synthesize/utils 下有对中文文本的cmudict.py 处理脚本，但好像并没有看到被引用（我看预处理的时候直接导入用的默认的英文symbols 和 cleaner）。请问你们是用处理英文的方式直接处理中文文本的么？不是，搜索 pypinyin 
报错
Traceback (most recent call last):  File "demo_toolbox.py", line 1, in <module>    from pathlib import PathImportError: No module named pathlib兄弟解决这个问题了吗？我也遇到了这个依赖就非常基础了，你是不是没运行pip install成功？？> 这个依赖就非常基础了，你是不是没运行pip install成功？？pip运行成功了，我是一段时间没用这个然后就这样了> 兄弟解决这个问题了吗？我也遇到了我也不知道怎么解决的不好意思哈 
CUDA error: device-side assert triggered
PS Q:\MockingBird-main> python synthesizer_train.py mandarin Q:\数据集\SV2TTS\synthesizer\Arguments:    run_id:          mandarin    syn_dir:         Q:\数据集\SV2TTS\synthesizer\    models_dir:      synthesizer/saved_models/    save_every:      1000    backup_every:    25000    log_every:       200    force_restart:   False    hparams:Checkpoint path: synthesizer\saved_models\mandarin\mandarin.ptLoading training data from: Q:\数据集\SV2TTS\synthesizer\train.txtUsing model: TacotronUsing device: cudaInitialising Tacotron Model...Trainable Parameters: 32.869MLoading weights at synthesizer\saved_models\mandarin\mandarin.ptTacotron weights loaded from step 0Using inputs from:        Q:\数据集\SV2TTS\synthesizer\train.txt        Q:\数据集\SV2TTS\synthesizer\mels        Q:\数据集\SV2TTS\synthesizer\embedsFound 62 samples+----------------+------------+---------------+------------------+| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |+----------------+------------+---------------+------------------+|   10k Steps    |     8      |     0.001     |        2         |+----------------+------------+---------------+------------------+C:\Users\canca\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead."){| Epoch: 1/1250 (8/8) | Loss: 10.02 | 0.60 steps/s | Step: 0k | }{| Epoch: 2/1250 (8/8) | Loss: 6.688 | 0.64 steps/s | Step: 0k | }{| Epoch: 3/1250 (8/8) | Loss: 5.300 | 0.66 steps/s | Step: 0k | }{| Epoch: 4/1250 (8/8) | Loss: 4.547 | 0.67 steps/s | Step: 0k | }{| Epoch: 5/1250 (8/8) | Loss: 4.038 | 0.67 steps/s | Step: 0k | }{| Epoch: 6/1250 (8/8) | Loss: 3.700 | 0.68 steps/s | Step: 0k | }{| Epoch: 7/1250 (8/8) | Loss: 3.432 | 0.68 steps/s | Step: 0k | }{| Epoch: 8/1250 (8/8) | Loss: 3.218 | 0.68 steps/s | Step: 0k | }{| Epoch: 9/1250 (8/8) | Loss: 3.039 | 0.68 steps/s | Step: 0k | }{| Epoch: 10/1250 (8/8) | Loss: 2.887 | 0.68 steps/s | Step: 0k | }{| Epoch: 11/1250 (8/8) | Loss: 2.759 | 0.68 steps/s | Step: 0k | }{| Epoch: 12/1250 (8/8) | Loss: 2.645 | 0.68 steps/s | Step: 0k | }{| Epoch: 13/1250 (3/8) | Loss: 2.610 | 0.68 steps/s | Step: 0k | }grad_norm was NaN!{| Epoch: 13/1250 (4/8) | Loss: 2.596 | 0.68 steps/s | Step: 0k | }C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [0,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [1,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [2,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [3,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [4,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [5,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [6,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [7,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [8,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [9,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [10,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [11,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [12,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [13,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [14,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [15,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [16,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [17,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [18,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [19,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [20,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [21,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [22,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [23,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [24,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [25,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [26,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [27,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [28,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [29,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [30,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [31,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [32,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [33,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [34,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [35,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [36,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [37,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [38,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [39,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [40,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [41,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [42,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [43,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [44,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [45,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [46,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [47,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [48,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [49,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [50,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [51,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [52,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [53,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [54,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [55,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [56,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [57,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [58,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [59,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [60,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [61,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [62,0,0] Assertion  failed.C:\w\b\windows\pytorch\aten\src\ATen\native\cuda\Loss.cu:115: block: [7,0,0], thread: [63,0,0] Assertion  failed.Traceback (most recent call last):  File "Q:\MockingBird-main\synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "Q:\MockingBird-main\synthesizer\train.py", line 201, in train    loss.backward()  File "C:\Users\canca\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\_tensor.py", line 307, in backward    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)  File "C:\Users\canca\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\autograd\__init__.py", line 154, in backward    Variable._execution_engine.run_backward(RuntimeError: CUDA error: device-side assert triggeredCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.For debugging consider passing CUDA_LAUNCH_BLOCKING=1. 
训练了180K结果硬盘容量被吃了一个T怎么办阿
![D4P53{ USH 
我的mac m1所有都用的arm包运行起来闪退
 Traceback (most recent call last):   File "/Users/huyouwangzecai/Documents/项目/个人项目/声音项目/MockingBird-main/toolbox/ui.py", line 160, in setup_audio_devices    sd.check_output_settings(device=device_namecheck, samplerate=sample_rate)  File "/Users/huyouwangzecai/miniforge3/envs/muisc_copy/lib/python3.9/site-packages/sounddevice.py", line 691, in check_output_settings    _check(_lib.Pa_IsFormatSupported(_ffi.NULL, parameters, samplerate))  File "/Users/huyouwangzecai/miniforge3/envs/muisc_copy/lib/python3.9/site-packages/sounddevice.py", line 2736, in _check    raise PortAudioError(errormsg, err, hosterror_info)sounddevice.PortAudioError: <unprintable PortAudioError object>这是我的报错内容，sounddevice check_output_settings方法运行不成功有了解的么一样遇到这个问题。解决了吗？ 一样的问题！一样的问题 求解；打开就闪退> Traceback (most recent call last): File "/Users/huyouwangzecai/Documents/项目/个人项目/声音项目/MockingBird-main/toolbox/ui.py", line 160, in setup_audio_devices sd.check_output_settings(device=device_namecheck, samplerate=sample_rate) File "/Users/huyouwangzecai/miniforge3/envs/muisc_copy/lib/python3.9/site-packages/sounddevice.py", line 691, in check_output_settings _check(_lib.Pa_IsFormatSupported(_ffi.NULL, parameters, samplerate)) File "/Users/huyouwangzecai/miniforge3/envs/muisc_copy/lib/python3.9/site-packages/sounddevice.py", line 2736, in _check raise PortAudioError(errormsg, err, hosterror_info) sounddevice.PortAudioError: 这是我的报错内容，sounddevice check_output_settings方法运行不成功有了解的么后面解决了吗> > Traceback (most recent call last): File "/Users/huyouwangzecai/Documents/项目/个人项目/声音项目/MockingBird-main/toolbox/ui.py", line 160, in setup_audio_devices sd.check_output_settings(device=device_namecheck, samplerate=sample_rate) File "/Users/huyouwangzecai/miniforge3/envs/muisc_copy/lib/python3.9/site-packages/sounddevice.py", line 691, in check_output_settings _check(_lib.Pa_IsFormatSupported(_ffi.NULL, parameters, samplerate)) File "/Users/huyouwangzecai/miniforge3/envs/muisc_copy/lib/python3.9/site-packages/sounddevice.py", line 2736, in _check raise PortAudioError(errormsg, err, hosterror_info) sounddevice.PortAudioError: 这是我的报错内容，sounddevice check_output_settings方法运行不成功有了解的么> > 后面解决了吗全部找m1专用的arm架构包， 不要用x86架构包，就可以正常运行Arguments:    datasets_root:          test    vc_mode:                False    enc_models_dir:         encoder/saved_models    syn_models_dir:         synthesizer/saved_models    voc_models_dir:         vocoder/saved_models    extractor_models_dir:   ppg_extractor/saved_models    convertor_models_dir:   ppg2mel/saved_models    cpu:                    False    seed:                   None    no_mp3_support:         FalseWarning: you do not have any of the recognized datasets in test Please note use 'E:\datasets' as root path instead of 'E:\datasetsidatatang_200zh\corpus	est' as an example .The recognized datasets are:	LibriSpeech/dev-clean	LibriSpeech/dev-other	LibriSpeech/test-clean	LibriSpeech/test-other	LibriSpeech/train-clean-100	LibriSpeech/train-clean-360	LibriSpeech/train-other-500	LibriTTS/dev-clean	LibriTTS/dev-other	LibriTTS/test-clean	LibriTTS/test-other	LibriTTS/train-clean-100	LibriTTS/train-clean-360	LibriTTS/train-other-500	LJSpeech-1.1	VoxCeleb1/wav	VoxCeleb1/test_wav	VoxCeleb2/dev/aac	VoxCeleb2/test/aac	VCTK-Corpus/wav48	aidatatang_200zh/corpus	aishell3/test/wav	magicdata/trainFeel free to add your own. You can still use the toolbox by recording samples yourself.||PaMacCore (AUHAL)|| AUHAL component not found.||PaMacCore (AUHAL)|| OpenStream @ 16000 returned: -9999: Unanticipated host error||PaMacCore (AUHAL)|| AUHAL component not found.||PaMacCore (AUHAL)|| OpenStream @ 16000 returned: -9999: Unanticipated host errorTraceback (most recent call last):  File "/Library/WebServer/Documents/Projects/PowerAI/Voice/MockingBird/toolbox/ui.py", line 159, in setup_audio_devices    sd.check_output_settings(device=device["name"], samplerate=sample_rate)  File "/Users/michaelhu/miniforge3/envs/voice-clone/lib/python3.9/site-packages/sounddevice.py", line 691, in check_output_settings    _check(_lib.Pa_IsFormatSupported(_ffi.NULL, parameters, samplerate))  File "/Users/michaelhu/miniforge3/envs/voice-clone/lib/python3.9/site-packages/sounddevice.py", line 2736, in _check    raise PortAudioError(errormsg, err, hosterror_info)sounddevice.PortAudioError: <unprintable PortAudioError object>During handling of the above exception, another exception occurred:Traceback (most recent call last):  File "/Library/WebServer/Documents/Projects/PowerAI/Voice/MockingBird/demo_toolbox.py", line 49, in <module>    Toolbox(**vars(args))  File "/Library/WebServer/Documents/Projects/PowerAI/Voice/MockingBird/toolbox/__init__.py", line 80, in __init__    self.setup_events()  File "/Library/WebServer/Documents/Projects/PowerAI/Voice/MockingBird/toolbox/__init__.py", line 126, in setup_events    self.ui.setup_audio_devices(Synthesizer.sample_rate)  File "/Library/WebServer/Documents/Projects/PowerAI/Voice/MockingBird/toolbox/ui.py", line 164, in setup_audio_devices    warn("Unsupported output device %s for the sample rate: %d \nError: %s" % (device["name"], sample_rate, str(e)))  File "/Users/michaelhu/miniforge3/envs/voice-clone/lib/python3.9/site-packages/sounddevice.py", line 2220, in __str__    hostname = query_hostapis(host_api)['name']  File "/Users/michaelhu/miniforge3/envs/voice-clone/lib/python3.9/site-packages/sounddevice.py", line 640, in query_hostapis    raise PortAudioError('Error querying host API {}'.format(index))sounddevice.PortAudioError: Error querying host API -9979Mac上安装参考 
什么时候出个封装包呢！
安装出现好多问题呀！ 
一直出现是噪音，ValueError: Axis limits cannot be NaN or Inf
按照教程打开demo_toolbox一切正常打开窗口后，无论是加载音频文件（mp3）还是通过toolbox自己录制都会出现报错ValueError: Axis limits cannot be NaN or Inf所以，最后输出的音频全都是噪音。请问大佬，这个问题要如何解决。尝试过把软件窗口拉大一点吗我也是，在macbook上，有解决么> ValueError: Axis limits cannot be NaN or Inf把软件窗口拉大之后可以解决这个问题 
小白问个问题，可以用特没普的英文音频素材，搞出中文的句子，以制作鬼畜么？我想做的逼真点，不知道行不行得通................
小白问个问题，可以用特没普的英文音频素材，搞出中文的句子，以制作鬼畜么？我想做的逼真点，不知道行不行得通................应该可以的这个想法不错。试验的如何了？不会捣鼓........后续可以关注并提问 
运行toolbox后进行录音，随后卡死
运行toolbox，使用https://github.com/babysor/MockingBird/wiki/Quick-Start-(Newbie) 每次启动toolbox，只要选择vocoder或者进行录音，都会卡死后台进程看看是否有冲突，因为调用了系统的ffmpeg，所以有可能是有兼容问题我也有相同的问题torch版本更新導致不相容 把環境的torch解除安裝"conda uninstall pytorch""pip uninstall torch"安裝這個"pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f torch版本更新導致不相容 把環境的torch解除安裝 "conda uninstall pytorch" "pip uninstall torch" 安裝這個 "pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f 
pyhton版本不能用3.10
用3.10 pytorch和一大票packages都安装不了换成3.9.7一切问题都解决了，之前卡在装pytorch好久 
在ubuntu系统训练时，找不到train.txt目录
如图。在windows端完全没问题，但上服务器就报错(base) root@f7a701305fdd:~/data/MockingBird-main# python synthesizer_train.py 22 data/datasets_root/SV2TTS/synthesizerArguments:    run_id:          22    syn_dir:         data/datasets_root/SV2TTS/synthesizer    models_dir:      synthesizer/saved_models/    save_every:      1000    backup_every:    25000    log_every:       200    force_restart:   False    hparams:         Checkpoint path: synthesizer/saved_models/22/22.ptLoading training data from: data/datasets_root/SV2TTS/synthesizer/train.txtUsing model: TacotronUsing device: cudaInitialising Tacotron Model...Trainable Parameters: 32.869MLoading weights at synthesizer/saved_models/22/22.ptTacotron weights loaded from step 0Using inputs from:        data/datasets_root/SV2TTS/synthesizer/train.txt        data/datasets_root/SV2TTS/synthesizer/mels        data/datasets_root/SV2TTS/synthesizer/embedsTraceback (most recent call last):  File "synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "/root/data/MockingBird-main/synthesizer/train.py", line 121, in train    dataset = SynthesizerDataset(metadata_fpath, mel_dir, embed_dir, hparams)  File "/root/data/MockingBird-main/synthesizer/synthesizer_dataset.py", line 12, in __init__    with metadata_fpath.open("r", encoding="utf-8") as metadata_file:  File "/root/miniconda3/lib/python3.8/pathlib.py", line 1218, in open    return io.open(self, mode, buffering, encoding, errors, newline,  File "/root/miniconda3/lib/python3.8/pathlib.py", line 1074, in _opener    return self._accessor.open(self, flags, mode)FileNotFoundError: [Errno 2] No such file or directory: 'data/datasets_root/SV2TTS/synthesizer/train.txt' 
![image](https://user-images.githubusercontent.com/90098227/148337437-7e860c07-e763-42b0-9ecd-2a4e975b73d8.png)
posted by @Kristen-PRC in 修改没有成功吧 
小白求助，in loading state_dict for {}:\n\t{}'.format&in loading state_dict for Tacotron
可以试一下：文字输入框里的标点只保留逗号“，”删掉其他符号，包括默认带的"！"我没打开合成工具箱，现在只是在别人模型上继续训练 
成功运行过工具箱，后来因为自己训练了一次模型就再也打不开工具箱了是什么情况？
Arguments:    datasets_root:    None    enc_models_dir:   encoder\saved_models    syn_models_dir:   synthesizer\saved_models    voc_models_dir:   vocoder\saved_models    cpu:              False    seed:             None    no_mp3_support:   FalseWarning: you did not pass a root directory for datasets as argument.The recognized datasets are:        LibriSpeech/dev-clean        LibriSpeech/dev-other        LibriSpeech/test-clean        LibriSpeech/test-other        LibriSpeech/train-clean-100        LibriSpeech/train-clean-360        LibriSpeech/train-other-500        LibriTTS/dev-clean        LibriTTS/dev-other        LibriTTS/test-clean        LibriTTS/test-other        LibriTTS/train-clean-100        LibriTTS/train-clean-360        LibriTTS/train-other-500        LJSpeech-1.1        VoxCeleb1/wav        VoxCeleb1/test_wav        VoxCeleb2/dev/aac        VoxCeleb2/test/aac        VCTK-Corpus/wav48        aidatatang_200zh/corpus/dev        aidatatang_200zh/corpus/test        aishell3/test/wav        magicdata/trainFeel free to add your own. You can still use the toolbox by recording samples yourself.Traceback (most recent call last):  File "E:\电子播发器\MockingBird-main\demo_toolbox.py", line 43, in <module>    Toolbox(**vars(args))  File "E:\电子播发器\MockingBird-main\toolbox\__init__.py", line 76, in __init__    self.setup_events()  File "E:\电子播发器\MockingBird-main\toolbox\__init__.py", line 113, in setup_events    self.ui.setup_audio_devices(Synthesizer.sample_rate)  File "E:\电子播发器\MockingBird-main\toolbox\ui.py", line 149, in setup_audio_devices    for device in sd.query_devices():  File "E:\python\Lib\site-packages\sounddevice.py", line 559, in query_devices    return DeviceList(query_devices(i)  File "E:\python\Lib\site-packages\sounddevice.py", line 559, in <genexpr>    return DeviceList(query_devices(i)  File "E:\python\Lib\site-packages\sounddevice.py", line 573, in query_devices    name = name_bytes.decode('utf-8')UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 6: invalid continuation byte报错是这样。本来都可以正常打开工具箱的。 web还是可以启动，也可以合成声音。就是工具箱打不开，这个怎么解决阿。补充一下，工具箱会很快的闪出来一下，然后又关闭。草。。。很怪。之前都开的好好的，根本没想过是这个问题。有同样问题的朋友可以看一下这个https://github.com/babysor/MockingBird/wiki/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF 
demo_toolbox无法运行
我是照着B站的一个教程来的输入python demo_toolbox.py 
运行demo_toolbox后不能部分功能用不了报错
Hello How may I help u today?Yes, thank you for your kindness, I ran into some problems while running> 如果是，那么我会检查代码好吗？Excuse me, now I cannot synthesize the sound I want with the loaded modelI performed the operation and sound synthesis according to the project's tutorial, and the sound that came out was distorted, not normal sound, only current sound and noise> 好的，您是否检查了声音的内部系统设置？> 2022年1月4日星期二下午12:58 MoNet ***@***.***> 写道： 我按照项目的教程进行了操作和声音合成，出来的声音是失真的，不是正常声音，只有当前声音和噪音——直接回复这封邮件，在GitHub上查看< >，或取消订阅< >。使用 GitHub Mobile for iOS < > 或 Android < 您收到此消息是因为您发表了评论。消息 ID：***@***.***>Yes, I checked the dependency package and the version of pytorch according to the requirements before running the project, and proceeded without errors.Let me try to restart my computerIt can be seen that the synthesized sound is problematic, and the Mel spectrum image is 好的，所以我们需要安装一个音量控制器以及一个可以用于解决您遇到的问题的 rah。2022 年 1 月 4 日星期二下午 1:27 Gaming Yoda ***@***.***> 写道：> Ok so it still shows the error? Try to upgrade once? On Tue, Jan 4, 2022 at 1:18 PM MoNet ***@***.***> wrote: > It can be seen that the synthesized sound is problematic, and the Mel > spectrum image is abnormal. > > [image: image] > <https://user-images.githubusercontent.com/48426139/148026104-b1e130a7-6433-48aa-943f-c6a7eff6f6a0.png> > > — > Reply to this email directly, view it on GitHub > <[#314 > or unsubscribe > <https://github.com/notifications/unsubscribe-auth/AW3ZCH3FQKPLXIARPTZJ7KLUUKQ3NANCNFSM5LGYE42Q> > . > Triage notifications on the go with GitHub Mobile for iOS > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> > or Android > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. > > You are receiving this because you commented.Message ID: > ***@***.***> >I think it should be the problem of the model I imported. I directly used my own voice, which is not ideal. It should be tested with the given model. My voice needs to be trained with a certain sample set. I think it’s me. I made a mistake> 所以我看到了你的抱怨，所以我可以帮助你吗？> -- 项目经理 - Github 给我发电子邮件 ***@***.*** 或 ***@***.***Thank you for your enthusiastic help, I have found the problem so far 
出现了新的问题
输入框输入内容只能合成一次声音，再次输入则会合成失败，只能重启，是什么原因呢？报错信息如下Error opening OutputStream: Unanticipated host error [PaErrorCode -9999]: '使用的设备标识号已超出本地系统范围。' [MME error 2]Traceback (most recent call last):  File "D:\MockingBird-main\MockingBird-main\toolbox\__init__.py", line 331, in vocode    self.ui.draw_embed(embed, name, "generated")  File "D:\MockingBird-main\MockingBird-main\toolbox\ui.py", line 70, in draw_embed    embed_ax.figure.canvas.draw()  File "C:\Python38\lib\site-packages\matplotlib\backends\backend_agg.py", line 436, in draw    self.figure.draw(self.renderer)  File "C:\Python38\lib\site-packages\matplotlib\artist.py", line 73, in draw_wrapper    result = draw(artist, renderer, *args, **kwargs)  File "C:\Python38\lib\site-packages\matplotlib\artist.py", line 50, in draw_wrapper    return draw(artist, renderer)  File "C:\Python38\lib\site-packages\matplotlib\figure.py", line 2796, in draw    artists = self._get_draw_artists(renderer)  File "C:\Python38\lib\site-packages\matplotlib\figure.py", line 238, in _get_draw_artists    ax.apply_aspect()  File "C:\Python38\lib\site-packages\matplotlib\axes\_base.py", line 1967, in apply_aspect    self.set_xbound(x_trf.inverted().transform([x0, x1]))  File "C:\Python38\lib\site-packages\matplotlib\axes\_base.py", line 3564, in set_xbound    self.set_xlim(sorted((lower, upper),  File "C:\Python38\lib\site-packages\matplotlib\axes\_base.py", line 3688, in set_xlim    left = self._validate_converted_limits(left, self.convert_xunits)  File "C:\Python38\lib\site-packages\matplotlib\axes\_base.py", line 3605, in _validate_converted_limits    raise ValueError("Axis limits cannot be NaN or Inf")ValueError: Axis limits cannot be NaN or InfTraceback (most recent call last):  File "D:\MockingBird-main\MockingBird-main\toolbox\__init__.py", line 331, in vocode    self.ui.draw_embed(embed, name, "generated")  File "D:\MockingBird-main\MockingBird-main\toolbox\ui.py", line 70, in draw_embed    embed_ax.figure.canvas.draw()  File "C:\Python38\lib\site-packages\matplotlib\backends\backend_agg.py", line 436, in draw    self.figure.draw(self.renderer)  File "C:\Python38\lib\site-packages\matplotlib\artist.py", line 73, in draw_wrapper    result = draw(artist, renderer, *args, **kwargs)  File "C:\Python38\lib\site-packages\matplotlib\artist.py", line 50, in draw_wrapper    return draw(artist, renderer)  File "C:\Python38\lib\site-packages\matplotlib\figure.py", line 2796, in draw    artists = self._get_draw_artists(renderer)  File "C:\Python38\lib\site-packages\matplotlib\figure.py", line 238, in _get_draw_artists    ax.apply_aspect()  File "C:\Python38\lib\site-packages\matplotlib\axes\_base.py", line 1967, in apply_aspect    self.set_xbound(x_trf.inverted().transform([x0, x1]))  File "C:\Python38\lib\site-packages\matplotlib\axes\_base.py", line 3564, in set_xbound    self.set_xlim(sorted((lower, upper),  File "C:\Python38\lib\site-packages\matplotlib\axes\_base.py", line 3688, in set_xlim    left = self._validate_converted_limits(left, self.convert_xunits)  File "C:\Python38\lib\site-packages\matplotlib\axes\_base.py", line 3605, in _validate_converted_limits    raise ValueError("Axis limits cannot be NaN or Inf")ValueError: Axis limits cannot be NaN or Inf窗口大小问题> 窗口大小问题请问一下该如何解决这个问题拉大窗口..> 拉大窗口..谢谢，解决了 
小白提问
使用web.py时报错Synthesizer using device: cpuusing synthesizer model: synthesizer\saved_models\mandarin.ptTrainable Parameters: 32.869M[2022-01-03 17:26:11,443] ERROR in app: Exception on /api/synthesize [POST]Traceback (most recent call last):  File "C:\Python38\lib\site-packages\flask\app.py", line 2073, in wsgi_app    response = self.full_dispatch_request()  File "C:\Python38\lib\site-packages\flask\app.py", line 1518, in full_dispatch_request    rv = self.handle_user_exception(e)  File "C:\Python38\lib\site-packages\flask_restx\api.py", line 672, in error_router    return original_handler(e)  File "C:\Python38\lib\site-packages\flask\app.py", line 1516, in full_dispatch_request    rv = self.dispatch_request()  File "C:\Python38\lib\site-packages\flask\app.py", line 1502, in dispatch_request    return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args)  File "D:\MockingBird-main\MockingBird-main\web\__init__.py", line 108, in synthesize    specs = current_synt.synthesize_spectrograms(texts, embeds)  File "D:\MockingBird-main\MockingBird-main\synthesizer\inference.py", line 87, in synthesize_spectrograms    self.load()  File "D:\MockingBird-main\MockingBird-main\synthesizer\inference.py", line 65, in load    self._model.load(self.model_fpath, self.device)  File "D:\MockingBird-main\MockingBird-main\synthesizer\models\tacotron.py", line 547, in load    self.load_state_dict(checkpoint["model_state"], strict=False)  File "C:\Python38\lib\site-packages\torch\nn\modules\module.py", line 1406, in load_state_dict    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]).        size mismatch for gst.stl.attention.W_query.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).        size mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([384, 1280]).        size mismatch for decoder.rnn_input.weight: copying a param with shape torch.Size([1024, 640]) from checkpoint, the shape in current model is torch.Size([1024, 1152]).        size mismatch for decoder.stop_proj.weight: copying a param with shape torch.Size([1, 1536]) from checkpoint, the shape in current model is torch.Size([1, 2048]).127.0.0.1 - - [2022-01-03 17:26:11] "POST /api/synthesize HTTP/1.1" 500 426 1.459210你是不是用的训练好的模型，好像是不兼容，你用第一个75k那个试试谢谢，解决了，75k可以------------------&nbsp;原始邮件&nbsp;------------------发件人:                                                                                                                        "babysor/MockingBird"                                                                                    ***@***.***&gt;;发送时间:&nbsp;2022年1月3日(星期一) 晚上8:48***@***.***&gt;;***@***.******@***.***&gt;;主题:&nbsp;Re: [babysor/MockingBird] 小白提问 (Issue #312) 你是不是用的训练好的模型，好像是不兼容，你用第一个75k那个试试 —Reply to this email directly, view it on GitHub, or unsubscribe.Triage notifications on the go with GitHub Mobile for iOS or Android. You are receiving this because you authored the thread.Message ID: ***@***.***&gt; 
使用预置模型，Toobox输出的是电音

本人小白，语音合成时遇Errors in loading staste_dict for Tacotron 求解决方法
Error(s) in loading state_dict for Tacotron：size mismatch for encoder-proj.weight: copying a param writh shape torch. Size([128, 512]) from checkpoint, the shape in current model is torch. Size (128, 1024]).size mismatch for decoder. attn_rnn.weight.ih: copying a param with shape torch.size([384, 7687) from checkpoint, the shape in current model is torch. Size([384, 1280]) size mismatch for decoder. rnn_input.weight: copying a param with shape torch. size (1024, 640]) from checkpoint, the shape in current model is torch. Size ([1024, 1152) size mismatch for decoder. stop_proj. weight: copying a param with shape torch. size ( 1, 1536]) from checkpoint, the shape in current model is torch. Size(1, 
声码器训练问题
1 训练hifigan声码器，同一标识再次训练时，不能继续训练，会重头开始。> 1 默认wavernn，不加参数会报错，通过修改代码已解决。 2 训练hifigan声码器，同一标识再次训练时，不能继续训练，会重头开始。第二点应该不会了吧？ @hertz-pj 修改1之后，wavernn是可以在第一个基础上继续训练的。但是wavernn效果测试不如hifigan的效果好，但是hifigan不能接着训练。> 修改1之后，wavernn是可以在第一个基础上继续训练的。但是wavernn效果测试不如hifigan的效果好，但是hifigan不能接着训练。这个我有空改一下，可能之前没注意这个中断训练的逻辑~ @babysor hifigan还是不能接着训练，我翻看了代码好像是.\vocoder\hifigan\train.py第55行和56行调用的那个scan_checkpoint()函数有问题，那个函数根本没法正确的找到模型把vocoder\hifigan\utils.py:53改成 就能加载了，但后果是cuda直接炸显存，batch_size = 4 了都炸> hifigan还是不能接着训练，我翻看了代码好像是.\vocoder\hifigan\train.py第55行和56行调用的那个scan_checkpoint()函数有问题，那个函数根本没法正确的找到模型> > 把vocoder\hifigan\utils.py:53改成 就能加载了，但后果是cuda直接炸显存，batch_size = 4 了都炸sorry，这块一直没空处理，周末找时间处理下。另外显存炸了跟这个不确定是否有关系，你从头训练hifigan是否也会炸显存呢？直接运行不会炸，而且根据我的进一步观察，输出了成功加载的提示，但其实还是没能加载进度，问题应该不只是这个Sent from Mail<https://go.microsoft.com/fwlink/?LinkId=550986> for WindowsFrom: ***@***.***>Sent: 2022年2月10日 17:23To: ***@***.***>Cc: ***@***.***>; ***@***.***>Subject: Re: [babysor/MockingBird] 声码器训练问题 (Issue #308)hifigan还是不能接着训练，我翻看了代码好像是.\vocoder\hifigan\train.py第55行和56行调用的那个scan_checkpoint()函数有问题，那个函数根本没法正确的找到模型把vocoder\hifigan\utils.py:53改成 pattern = os.path.join(cp_dir, prefix + '*')就能加载了，但后果是cuda直接炸显存，batch_size = 4 了都炸sorry，这块一直没空处理，周末找时间处理下。另外显存炸了跟这个不确定是否有关系，你从头训练hifigan是否也会炸显存呢？―Reply to this email directly, view it on GitHub<https://github.com/babysor/MockingBird/issues/308#issuecomment-1034683558>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AT53I7UGDJMN4UCPJDWC4TLU2N72RANCNFSM5LDSAZAQ>.Triage notifications on the go with GitHub Mobile for iOS<https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android<https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.You are receiving this because you commented.Message ID: ***@***.***>@AyahaShirane #389 已解决。另外我这边没有发现炸显存的问题，batch_size 16的占用显存大概是8g@hertz-pj 感激不尽 
mandarin_200k 如何load
使用这个模型汇报维度不匹配 请问如何使用 版本问题，请细看readme或issue问题在这里看到了 。能够跑通，谢了，但是还有个问题，_characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz12340!\'(),-.:;? '这个是用来找embedding的，这个字符集，针对不同模型是不是要单独维护一套？ 或者有什么相关论文推荐么？ 对语音合成是新手，请多指教> 问题在这里看到了 #37 。能够跑通，谢了，但是还有个问题，_characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz12340!'(),-.:;? '这个是用来找embedding的，这个字符集，针对不同模型是不是要单独维护一套？ 或者有什么相关论文推荐么？ 对语音合成是新手，请多指教不需要，只是历史遗留问题，ceshi用的是一个旧的 
模型load不正确
使用 下载了一个 mandarin_200k.pt，然后用web跑 报错： 
页面出现错误
Uncaught Error】Uncaught TypeError: Cannot read properties of undefined (reading 'value')at:279:61 url:http://localhost:8080/TypeError: Cannot read properties of undefined (reading 'value')    at FileReader.reader.onloadend (http://localhost:8080/:279:61)一样出现了这个问题 
AttributeError: module 'setuptools._distutils' has no attribute 'version'
F:\VideoCentTools\MockingBird-main>python synthesizer_train.py offhen F:\VideoCentTools/SV2TTS/synthesizerTraceback (most recent call last):  File "F:\VideoCentTools\MockingBird-main\synthesizer_train.py", line 2, in <module>    from synthesizer.train import train  File "F:\VideoCentTools\MockingBird-main\synthesizer\train.py", line 5, in <module>    from torch.utils.tensorboard import SummaryWriter  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\utils\tensorboard\__init__.py", line 4, in <module>    LooseVersion = distutils.version.LooseVersionAttributeError: module 'setuptools._distutils' has no attribute 'version'tensorboard 在你电脑有版本问题，请用干净的虚拟python环境那么我需要退回到哪一步呢 我发现即使我pip uninstall tensorboard之后run以及再pip install -r requirements.txt之后run都还是出现这个结果> 那么我需要退回到哪一步呢 我发现即使我pip uninstall tensorboard之后run以及再pip install -r requirements.txt之后run都还是出现这个结果建议用anaconda工具创建虚拟环境我只能说感谢您提供的意见。由于我不甚了解python 我只能勉强理解步骤的逻辑我搜索了anaconda工具并且查询了教程安装了pycharm和anaconda我发现它们在安装的时候跳过了很多分支的安装所以我又回头 尝试run在我本体上的A:	安装anaconda和pycharmQ:	发现缺少torch、torchvision、torchaudioA:	安装torchQ:	发现缺少tensorboardA:	pip install tensorboard Q:	发现缺少librosaA:	pip install librosaQ:	F:\VideoCentTools\MockingBird-main>python synthesizer_train.py offhen F:\VideoCentTools/SV2TTS/synthesizerTraceback (most recent call last):  File "F:\VideoCentTools\MockingBird-main\synthesizer_train.py", line 2, in <module>    from synthesizer.train import train  File "F:\VideoCentTools\MockingBird-main\synthesizer\train.py", line 6, in <module>    from synthesizer import audio  File "F:\VideoCentTools\MockingBird-main\synthesizer\audio.py", line 1, in <module>    import librosa  File "L:\Anaconda3\lib\site-packages\librosa\__init__.py", line 211, in <module>    from . import core  File "L:\Anaconda3\lib\site-packages\librosa\core\__init__.py", line 5, in <module>    from .convert import *  # pylint: disable=wildcard-import  File "L:\Anaconda3\lib\site-packages\librosa\core\convert.py", line 7, in <module>    from . import notation  File "L:\Anaconda3\lib\site-packages\librosa\core\notation.py", line 8, in <module>    from ..util.exceptions import ParameterError  File "L:\Anaconda3\lib\site-packages\librosa\util\__init__.py", line 87, in <module>    from ._nnls import *  # pylint: disable=wildcard-import  File "L:\Anaconda3\lib\site-packages\librosa\util\_nnls.py", line 13, in <module>    import scipy.optimize  File "L:\Anaconda3\lib\site-packages\scipy\optimize\__init__.py", line 400, in <module>    from .optimize import *  File "L:\Anaconda3\lib\site-packages\scipy\optimize\optimize.py", line 36, in <module>    from ._numdiff import approx_derivative  File "L:\Anaconda3\lib\site-packages\scipy\optimize\_numdiff.py", line 6, in <module>    from scipy.sparse.linalg import LinearOperator  File "L:\Anaconda3\lib\site-packages\scipy\sparse\linalg\__init__.py", line 114, in <module>    from .eigen import *  File "L:\Anaconda3\lib\site-packages\scipy\sparse\linalg\eigen\__init__.py", line 9, in <module>    from .arpack import *  File "L:\Anaconda3\lib\site-packages\scipy\sparse\linalg\eigen\arpack\__init__.py", line 20, in <module>    from .arpack import *  File "L:\Anaconda3\lib\site-packages\scipy\sparse\linalg\eigen\arpack\arpack.py", line 42, in <module>    from . import _arpackImportError: DLL load failed while importing _arpack: 找不到指定的程序。A:	pip uninstall scipy & pip install scipyQ:	缺少inflectA:	pip install inflectdone.结果发现能run了 喜从天降哈哈 是不是缺少重启 
我这段有问题， 有点搞不懂
Arguments:datasets_root: /Users/xxx/Desktop/hoop/Realtime-Voice-Clone-Chinese/aidatatang_200zhout_dir: /Users/xxx/Desktop/hoop/Realtime-Voice-Clone-Chinese/aidatatang_200zh/SV2TTS/synthesizern_processes: Noneskip_existing: Falsehparams:no_alignments: Falsedataset: aidatatang_200zhUsing data from:/Users/xxx/Desktop/hoop/Realtime-Voice-Clone-Chinese/aidatatang_200zh/aidatatang_200zh/corpus/trainTraceback (most recent call last):File "/Users/xxx/Desktop/hoop/Realtime-Voice-Clone-Chinese/synthesizer_preprocess_audio.py", line 63, in <module>preprocess_dataset(**vars(args))File "/Users/xxx/Desktop/hoop/Realtime-Voice-Clone-Chinese/synthesizer/preprocess.py", line 32, in preprocess_datasetassert all(input_dir.exists() for input_dir in input_dirs)AssertionError路径错误问题解决了吗，我又遇到了 
我用gtx970可以正常运行但是用gtx titanz显卡就不能运行，这是怎么回事啊啊
软件提示Exception:CUDA error:no kernel image is available for execution on the deviceCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.For debugging consider passing CUDA_LAUNCH_BLOCKING=1是不是因为titanz太老的缘故所以用不了啊...cuda版本问题吧 
有人用MacBook跑过嘛0.0
<img width="1667" alt="截屏2021-12-29 下午11 19 16" src="https://user-images.githubusercontent.com/20741235/147676931-143df97f-223d-446f-8531-fc48d24b8844.png">我能问一下你的opencv-python是什么版本？我运行后报了qt.qpa.plugin：Could not find the Qt platform plugin "cocoa" in ""的错误我用macbook pro跟你是同样的问题，除了报错以外，录音不能回放，显示不了录音的图片，合成效果也不如windows好（用win10 同一代码模型做了对比）错误请看 #37 
demo_toolbox加载一个本地音频后卡死
运行demo_toolbox 配置是3900X+32G+3070TI我也遇到了类似情况我也是 请问问题解决了吗torch版本更新導致不相容 把環境的torch解除安裝"conda uninstall pytorch""pip uninstall torch"安裝這個"pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f 3060Ti 路過先是遇到https://github.com/babysor/MockingBird/issues/377一樣的問題, 一樣的解法但之後就遇到本樓的問題我找到你的方法解除安裝+安裝新東西之後基本上是可以開了, 還在研究其他遇到的問題 
plots中看到损失不是逐渐下降的，想要获得最优损失的模型该如何获得
为何设置让模型每次都保存最新的而不是保存损失更小的模型每次都保存的是最新的 loss只是抽样算的 不稳定的 
继续分享模型，250K模型
这个模型忘记分享了，之前只分享了160K的aishell3，这个是250K的aishell3单数据集模型，同样是基于batch size 96，以及老版本上训练的。**实测新版本改兼容代码，以及用作者tag的旧版本依然不可用。所以我直接分享训练时用的旧版本，内置了这个260K的模型。也就是名为mandarin.pt的，已经替换成260K的了。****考虑到很多人没有百度网盘会员，所以这次增加了直链下载，复制到浏览器即可下载。**链接如若失效请评论提醒，我会补。百度云链接：链接：https://pan.baidu.com/s/1qTTNCI42oI2483JAc8b3Bw 提取码：7777 直链链接：https://disk.onji.cn/api/v3/file/source/363983/MockingBird-aishell3%20250K%E6%A8%A1%E5%9E%8B%E6%97%A7%E7%89%88%E6%9C%AC.zip?sign=tMN9Ls0sfJ9bc7yXMPZxDmh7lhuvpm5QrO5oOOsQDrQ%3D%3A0你有对比过作者分享的那个”75k steps 用3个开源数据集混合训练“的模型吗> 你有对比过作者分享的那个”75k steps 用3个开源数据集混合训练“的模型吗实践出真理，请自行测试，我并没用过作者的模型cooooool！感谢大佬分享！这个不错> 想问一下大佬，你用的数据集多大？多少条音频？ 
modified tacotron synthesizer to tacotron2
新增了tacotron2的synthesizer，其余代码没有变动。训练方法可参照Real-time-voice-clone这个项目，如需训练好的模型，可随时提供这一整个request是基于tf的，可以改为基于pytorch的吗？github有不少实现，另外注意本repo的taco不是1也不是2，会贴近2一点点 
provide docker image?
可以提供一个docker镜像吗，配置好麻烦，一直总有错误...[#122 
LOSS=0.269 看着没有收敛的迹象啊！
还没有的话就重新练 
python demo_toolbox.py -d D:\DATA\aidatatang_200zh\corpus\test报错
Warning: you do not have any of the recognized datasets in D:\DATA\aidatatang_200zh\corpus\test.The recognized datasets are:        LibriSpeech/dev-clean        LibriSpeech/dev-other        LibriSpeech/test-clean        LibriSpeech/test-other        LibriSpeech/train-clean-100        LibriSpeech/train-clean-360        LibriSpeech/train-other-500        LibriTTS/dev-clean        LibriTTS/dev-other        LibriTTS/test-clean        LibriTTS/test-other        LibriTTS/train-clean-100        LibriTTS/train-clean-360        LibriTTS/train-other-500        LJSpeech-1.1        VoxCeleb1/wav        VoxCeleb1/test_wav        VoxCeleb2/dev/aac        VoxCeleb2/test/aac        VCTK-Corpus/wav48        aidatatang_200zh/corpus/dev        aidatatang_200zh/corpus/test        aishell3/test/wav        magicdata/trainFeel free to add your own. You can still use the toolbox by recording samples yourself.为什么报错说没有可识别的数据，里面的所有文件我都解压了，求求了，还有那个<dataset_root>是不是要精确到数据集的具体文件，我用的是aidatatang_200zh的数据集python demo_toolbox.py -d D:\DATA  这条你试试> python demo_toolbox.py -d D:\DATA 这条你试试已解决，谢谢啦 
纯小白，虚心求教几个问题
1.工具箱打开的音频文件应该选择什么？是任意音频还是别的？2.训练了50k左右，polt图一直都是一条横线，并未呈现对角线趋势。3.目录中wav里面生成的音频声音听起来类似无线电信号很差的那种感觉，但可以勉强辨别内容。4.工具箱生成的声音则是完全一个音节从头到尾一直念，而且完全无法辨别，机械音严重5.我拿来训练的素材大概有800条短句音频，都是纯单人人声的，是数量不够还是音频质量不行呢？虚心求教，谢谢我也是一条直线，没有收敛，但是loss已经下来了请先详细阅读readme或者  mp3等本机ffmpeg支持的音频2. 收敛有一定失败率，如果15k后没有出现说明已经失败，需要重新开始。 
运行pre.py时报错
完整输出：PS D:\MockingBird> python pre.py D:\Using data from:    D:\aidatatang_200zh\corpus\trainaidatatang_200zh:   6%|███▎                                                   | 50/840 [23:39<6:13:51, 28.39s/speakers]Traceback (most recent call last):  File "D:\MockingBird\pre.py", line 74, in <module>    preprocess_dataset(**vars(args))  File "D:\MockingBird\synthesizer\preprocess.py", line 74, in preprocess_dataset    for speaker_metadata in tqdm(job, dataset, len(speaker_dirs), unit="speakers"):  File "E:\python\lib\site-packages\tqdm\std.py", line 1180, in __iter__Process SpawnPoolWorker-1:Traceback (most recent call last):  File "E:\python\lib\multiprocessing\process.py", line 315, in _bootstrap    self.run()  File "E:\python\lib\multiprocessing\process.py", line 108, in run    self._target(*self._args, **self._kwargs)  File "E:\python\lib\multiprocessing\pool.py", line 114, in worker    task = get()  File "E:\python\lib\multiprocessing\queues.py", line 368, in get    return _ForkingPickler.loads(res)MemoryError    for obj in iterable:  File "E:\python\lib\multiprocessing\pool.py", line 870, in next    raise valuemultiprocessing.pool.MaybeEncodingError: Error sending result: '<multiprocessing.pool.ExceptionWithTraceback object at 0x000002734060DD30>'. Reason: 'PicklingError("Can't pickle <class 'MemoryError'>: it's not the same object as builtins.MemoryError")'Traceback (most recent call last):  File "E:\python\lib\multiprocessing\util.py", line 300, in _run_finalizers    finalizer()  File "E:\python\lib\multiprocessing\util.py", line 224, in __call__    res = self._callback(*self._args, **self._kwargs)  File "E:\python\lib\multiprocessing\pool.py", line 692, in _terminate_pool    cls._help_stuff_finish(inqueue, task_handler, len(pool))  File "E:\python\lib\multiprocessing\pool.py", line 674, in _help_stuff_finish    inqueue._reader.recv()  File "E:\python\lib\multiprocessing\connection.py", line 256, in recv    return _ForkingPickler.loads(buf.getbuffer())MemoryError内存16G，硬盘剩余空间200G，CPU i7-6700HQ，显卡 NVIDIA Geforce GTX 960M，仓库昨天刚clone的，PyTorch是最新版1.试试降低n_processes2.试试把MockingBird/synthesizer/preprocess.py 的73~76行改成 > 解决啦，谢谢 
分享两个训练好的synthesizer模型
俩模型都是用最新的代码来训练的，不需要切换回0.0.1第一个模型synthesizer-merged_110k，是在代码支持的四个数据集（aidatatang_200zh，magicdata，aishell3，data_aishell）上联合训练的。learning rate=0.001无衰减，batch rate=0.001无衰减，batch  密码: ir90[Google 👍感谢感谢> 大佬您好，在从openslr下载了magicdata数据集后，是否遇到过train_set.tar.gz解压失败的情况呢，最后报：gzip: stdin: invalid compressed data--format violated，确认是下载完了的。。> gzip: stdin: invalid compressed data--format violated解压时候没遇到任何问题感谢分享！感谢大佬，这个模型非常好可以用在现在最新的main分支上接着跑训练吗？ 谢谢新模型，新气象。感谢大佬 
有可以循环调用、或者批量处理的方法吗？
如果有一段话，想把每一句都导出单独的一句，除了在GUI里进行 **输入→处理→听→选择导出** 的循环外，还有可以直接调用的api吗？或者和其他程序联动，比如定位一个word文档文件，按一定规则对内部文字进行批处理这样。目前正在研究类似的方法，欢迎大佬们来讨论下。demo_cli.py文件就可以直接调用可以拿来自己改> demo_cli.py多谢指路！理论上肯定可以，但由于目前模型稳定性太弱，你这样做的话整体效果很难保证，当然实践之后有经验欢迎分享 @amzsowhat 
关于另一种TTS后端实现的构想，不知是否能够实现
各位大神好！十分感谢各位的付出，我使用MockingBird有一段时间了，感觉十分有帮助。但是还是偶尔会遇到个别句子合成出来效果不理想，看到各位大佬提到训练合成器非常辛苦，我就有了一个大胆的想法：目前阿里云不是TTS效果蛮好的嘛，能否接入阿里云（或其他同类API），调用成熟的TTS来生成基础语音呢？这样的话，理论上就可以获取更流畅的结果，岂不爽哉？如果此路不通的话，还烦请各位大佬批评指正。可以的  我试过了了  我是用公司的TTS引擎生成的语料拿去训练  最后的效果比真人录音语料好很多  没有噪音  也更逼真这样子，这个项目要不了多久就要被封了> 可以的 我试过了了 我是用公司的TTS引擎生成的语料拿去训练 最后的效果比真人录音语料好很多 没有噪音 也更逼真@luobingit 哇，难道是xunfei的大佬嘛！！！> 这样子，这个项目要不了多久就要被封了额，作为作者居然不知道，能有什么办法不让被封吗？把效果改坏？> > 这样子，这个项目要不了多久就要被封了> > 额，作为作者居然不知道，能有什么办法不让被封吗？把效果改坏？就是，哪有说推动技术发展还要瞻前顾后的，支持作者！顺便问一下，这个思路能行得通吗> > > 这样子，这个项目要不了多久就要被封了> > > > > > 额，作为作者居然不知道，能有什么办法不让被封吗？把效果改坏？> > 就是，哪有说推动技术发展还要瞻前顾后的，支持作者！顺便问一下，这个思路能行得通吗上次跟一个网友讨论了一个小时 确认可行  你可以百度一下 这里放个图 ![Uploading width="594" alt="C74131F5-A6F8-4EDE-AA4B-5B4C8A4A3CC5" src="https://user-images.githubusercontent.com/7423248/147401929-1a145bf2-ebe2-41a8-83ea-246768ae29e2.png">真是个天才般的想法，用其他tts产生基础数据集先把注意力训练出来并扩充词汇库，再用特定的数据集来专项特化，应该能作出很好的单人tts这个项目大有可为 
关于照着视频教程做的中间出现的各种错误
我除了使用了anaconda以外，完全按照视频教程一步一步做的，然后后面启动却出现Warning: you did not pass a root directory for datasets as argument.所以一个datasets的文件夹是必须的，那我就按教程新建并传入simple文件夹，可是Warning: you do not have any of the recognized datasets in samples.那就是因为没东西呗，那我想肯定是要把rty4_56k的pt文件或wav文件，放在里面，可是放在里面仍旧是一样的错误然后我看了社区模型，全是覆盖已有的一些文件夹，也就是说，模型不会存在simple文件夹里那我想可能是因为那是用来训练模型用的声音才放里面，先忽略掉，但是不行，直接用的话size mismatch for gst.stl.attention.W_query.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).爆这个错误，我不知道是怎么一回事，有没有好哥哥可以帮帮我呢我就是在想会不会教程里少了什么步骤或者怎么样。。。我也遇到这样的问题， 你或许可以@一下作者@babysor 下面评论区也有别的人出现了这个问题，或许是一个需要修复的BUG，有空了希望可以帮帮忙，谢谢哥哥！虽然出现了warning但【一个datasets的文件夹】不是必须的，不传的情况下，输入或者选择你的音频文件就可以了。后面的 size mismatch是因为使用的模型与代码版本不符，建议看下readme中每个模型的详细描述，> 虽然出现了warning但【一个datasets的文件夹】不是必须的，不传的情况下，输入或者选择你的音频文件就可以了。 后面的 size mismatch是因为使用的模型与代码版本不符，建议看下readme中每个模型的详细描述，好的，谢谢了，是我没仔细搜索别的issue写给后来的人：前面有Issue已经反馈了一样的问题，并且也有比较详细的说明，还有一些我没有尝试的方法，并且babysor说之后会有兼容性提高的版本，希望兼容性提高的更新能早一点出现吧，readme我并没有看到模型的详细描述，里面没有有关于是否新版可用的部分，但是wiki里仔细看确实说了这个问题，另外其中一个issue有人提供了新版可用的模型，大家可以去试试看，相关的问题继续在那个issue去交流吧。因为我不知道issue传送门是怎么编辑的，这里就不多说了，大家去找吧QWQ 
利用脚本测试的时候出现shape不匹配问题
Traceback (most recent call last):  File "F:/MockingBird-main/MockingBird-main/test.py", line 61, in <module>    specs = synthesizer.synthesize_spectrograms(texts, embeds)  File "F:\MockingBird-main\MockingBird-main\synthesizer\inference.py", line 87, in synthesize_spectrograms    self.load()  File "F:\MockingBird-main\MockingBird-main\synthesizer\inference.py", line 65, in load    self._model.load(self.model_fpath, self.device)  File "F:\MockingBird-main\MockingBird-main\synthesizer\models\tacotron.py", line 547, in load    self.load_state_dict(checkpoint["model_state"], strict=False)  File "E:\Conda\condaFile\envs\MockingBird-main\lib\site-packages\torch\nn\modules\module.py", line 1482, in load_state_dict    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(RuntimeError: Error(s) in loading state_dict for Tacotron:	size mismatch for gst.stl.attention.W_query.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).望作者以及各路大神路过的时候瞅一瞅，用的模型是my_run8_25k.pt，问题应该出在synthesizer上了parser = argparse.ArgumentParser(        formatter_class=argparse.ArgumentDefaultsHelpFormatter    )    parser.add_argument("-e", "--enc_model_fpath", type=Path,                            default="./encoder/saved_models/pretrained.pt",                            help="Path to a saved encoder")    parser.add_argument("-s", "--syn_model_fpath", type=Path,                        # default="synthesizer/saved_models/pretrained/pretrained.pt",                        default="./synthesizer/saved_models/my_run8_25k.pt",                        help="Path to a saved synthesizer")    parser.add_argument("-v", "--voc_model_fpath", type=Path,                        # default="vocoder/saved_models/pretrained/pretrained.pt",                        default="./vocoder/saved_models/pretrained/pretrained_mander.pt",                        help="Path to a saved vocoder")    args = parser.parse_args()    print_args(args, parser)    check_model_paths(encoder_path=args.enc_model_fpath,                      synthesizer_path=args.syn_model_fpath,                      vocoder_path=args.voc_model_fpath)    encoder.load_model(args.enc_model_fpath)    synthesizer = Synthesizer(args.syn_model_fpath)    vocoder.load_model(args.voc_model_fpath)    # ***************************************** informal *****************************************    in_fpath = r'C:\Users\think\Desktop\temp_SE\test\SSB0139\SSB01390019.wav'    preprocessed_wav = encoder.preprocess_wav(in_fpath)    # - If the wav is already loaded:    original_wav, sampling_rate = librosa.load(str(in_fpath))    preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)    print("Loaded file succesfully")    embed = encoder.embed_utterance(preprocessed_wav)    print("Created the embedding")    text = r'宁波大学'    texts = [text]    embeds = [embed]    specs = synthesizer.synthesize_spectrograms(texts, embeds)    spec = specs[0]    print("Created the mel spectrogram")    print("Synthesizing the waveform:")    generated_wav = vocoder.infer_waveform(spec)    generated_wav = np.pad(generated_wav, (0, synthesizer.sample_rate), mode="constant")    generated_wav = encoder.preprocess_wav(generated_wav)    sf.write('test.wav', generated_wav.astype(np.float32), synthesizer.sample_rate)@babysor 作者您好 能否帮忙解决一下 hparams.py的最后一个参数已经是True#245 用这个，这大概是唯一一个新版可用的模型了我试了一下，包括那个readme的第一个75k也是可以用的已经成功运行批处理脚本，非常感谢#245很好用 
如何基于ceshi.pt继续训练呢？
1.目前从测试情况来看，测试了好几位大佬提供的模型，发现目前效果最好的还是作者提供的ceshi.pt，不知道作者是基于什么参数训练的，很想依照参数训练复现一下？2.如何在ceshi.pt的基础上进行继续训练呢?将ceshi.pt重命名为mandarin.pt进行继续训练时，初始化报如下错误：ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group，参数未做改动，batch_size用默认的12我是拉取最新的代码在 #245 的基础上继续训练的，作者分享的75k模型也是可以的> 我是拉取最新的代码在 #245 的基础上继续训练的，作者分享的75k模型也是可以的好的，感谢感谢！ 
能不通过gui 而通过py程序直接调用吗
有没有暴露出来的api，可以直接让其他程序调用可以看看web部分，比较粗糙，欢迎改进。> 可以看看web部分，比较粗糙，欢迎改进。@babysor 抱歉又打扰您，我通过GUI版本进行合成的声音挺不错的，然后我尝试web版本，合成出来的全是恐怖的奇怪音效。。。GUI的操作是对照B站教程操作的，web版本我上传了相同的音源，然后我也看了后台确定使用的是同一个模型，实际上也只保存了一个模型，结果输出的却完全不同，这是什么原因呢？我应该怎么做呢？> > 可以看看web部分，比较粗糙，欢迎改进。> > @babysor 抱歉又打扰您，我通过GUI版本进行合成的声音挺不错的，然后我尝试web版本，合成出来的全是恐怖的奇怪音效。。。GUI的操作是对照B站教程操作的，web版本我上传了相同的音源，然后我也看了后台确定使用的是同一个模型，实际上也只保存了一个模型，结果输出的却完全不同，这是什么原因呢？我应该怎么做呢？同问> > 可以看看web部分，比较粗糙，欢迎改进。> > @babysor 抱歉又打扰您，我通过GUI版本进行合成的声音挺不错的，然后我尝试web版本，合成出来的全是恐怖的奇怪音效。。。GUI的操作是对照B站教程操作的，web版本我上传了相同的音源，然后我也看了后台确定使用的是同一个模型，实际上也只保存了一个模型，结果输出的却完全不同，这是什么原因呢？我应该怎么做呢？之前这个web project是和网友一起倒腾的， 我估计是音频传到后台的预处理（采样降噪一块）有问题，可以用调试工具一步步存下来看音频。@luobingit 
收敛效果问题
目前运行了50小时左右 Step255k运行起来仍然是胡言乱语的状态总共识别14000条降噪语音 自己检查了差不多3000条现在的loss是在0.42-0.44之间徘徊请问这个效率是正常的还是有些出问题呢（plot看起来是对角线居多）看到有一些成品100-200k就已经差不多了理论上也差不多效果了，可能跟硬件有关，如果对角线居多，至少说话应该是清晰得，会不会使用姿势有问题：https://zhuanlan.zhihu.com/p/425692267另外你是怎么识别的，我非常好奇，希望能贡献一下方法或数据集，欢迎ping我邮箱 
报错，不报错的时候有杂音
报错信息：Traceback (most recent call last):  File "E:\AI\Project\MockingBird\MockingBird-main\toolbox\__init__.py", line 123, in <lambda>    func = lambda: self.synthesize() or self.vocode()  File "E:\AI\Project\MockingBird\MockingBird-main\toolbox\__init__.py", line 272, in vocode    wav = vocoder.infer_waveform(spec, progress_callback=vocoder_progress)  File "E:\AI\Project\MockingBird\MockingBird-main\vocoder\wavernn\inference.py", line 63, in infer_waveform    wav = _model.generate(mel, batched, target, overlap, hp.mu_law, progress_callback)  File "E:\AI\Project\MockingBird\MockingBird-main\vocoder\wavernn\models\fatchord_version.py", line 253, in generate    output[-20 * self.hop_length:] *= fade_outValueError: operands could not be broadcast together with shapes (2400,) (4000,) (2400,)杂音问题：全是杂音，没有一点人声，支持CUDA，数据集和自己录制声音都不能正确正确生成出一点声音，encoder:pretrained.ptsynthesizer:ceshi.pt, mandarin_200k.ptvocoder:g_hifigan.pt,pretrained.pt,wavernn_pretrained.pt以上任何搭配都无法生成正常的声音按照issue #37 
安装requirements.txt的时候发生以下报错如何解决
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.spyder 5.1.5 requires pyqt5<5.13, but you have pyqt5 5.15.6 which is incompatible.看的意思是我的pyqt版本太高了 但是怎么解决阿 完全不会。。。 
安装requirements.txt的时候发生以下报错如何解决
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.spyder 5.1.5 requires pyqt5<5.13, but you have pyqt5 5.15.6 which is incompatible.看的意思是我的pyqt版本太高了 但是怎么解决阿 完全不会。。。用一下虚拟环境，这样就不会有冲突了> 用一下虚拟环境，这样就发生了是指从base切换到我新设置的Pytorch嘛？可是我已经切换到了pytorch再运行的代码啊。> > 用一下虚拟环境，这样就发生了> > 是指从base切换到我新设置的Pytorch嘛？可是我已经切换到了pytorch再运行的代码啊。啊哈哈哈不知道咋回事好了 没事了没事了 
vncoder不可用的问题
model放错地方了吧，这导入的显然是synthesizer的weight。解决办法就是把你vocoder\saved_models\pretrained-11-7-21_75k.pt这个文件放到synthesizer\saved_models里。谢谢大佬（我当时是两个saved_models文件夹都放了),按您说的搞完之后用其他vocoder效果还不是太好，音色不太像（做不到演示视频里那样几乎分辨不出）调试到后面还不能正常发出人声，那个音谱始终不能跟原音频像，波纹状也很少。又是什么问题呢> 谢谢大佬（我当时是两个saved_models文件夹都放了),按您说的搞完之后用其他vocoder效果还不是太好，音色不太像（做不到演示视频里那样几乎分辨不出）调试到后面还不能正常发出人声，那个音谱始终不能跟原音频像，波纹状也很少。又是什么问题呢可能是大家演示用的模型都是比较好（并没分享最终版），并且音频也是专门挑的  
Fix a UserWarning
Fix a UserWarning in synthesizer/synthesizer_dataset.py, because of converting list of numpy array to torch tensor at Ln.85. 
安装 ffmpeg时的requirements.txt在哪里？
安装 ffmpeg。 1）下载 选择点击打开链接Windows对应的版本下载 2）解压 ffmpeg-xxxx.zip 文件到指定目录； 3）将解压后的文件目录中 bin 目录（包含 ffmpeg.exe ）添加进 path 环境变量中； 4）进入 cmd，输入 ffmpeg -version，可验证当前系统是否识别 ffmpeg 以及查看 ffmpeg 的版本运行pip install -r requirements.txt 来安装剩余的必要包。所以这里不是个问题吗？一开始我也有这个疑问其实，然后你其实可以跟着视频教学做，它的requirements.txt其实是这个MokingBird的代码包，里面有一个requirements.txt，下载打开进入目录跑就可以了 
进行音频和梅尔频谱图预处理报错怎么回事： python encoder_preprocess.py <datasets_root>
The dataset consists of 0 utterances, 0 mel frames, 0 audio timesteps (0.00 hours).Traceback (most recent call last):  File "pre.py", line 74, in <module>    preprocess_dataset(**vars(args))  File "E:\PythonProject\MockingBird\synthesizer\preprocess.py", line 88, in preprocess_dataset    print("Max input length (text chars): %d" % max(len(m[5]) for m in metadata))ValueError: max() arg is an empty sequence同样的问题，请问楼主解决了吗数据集文件没有正常解压出现这个错误可能的原因：1、<你数据集的根目录>\corpus\train   这个路径下面的音频文件是一个压缩文件，此时需要把该文件在当前目录解压缩；2、你可能直接把所有用于训练的n个音频文件直接放在 “ <你数据集的根目录>\corpus\train ” 这个路径下面，而没有把这些音频文件打包成一个文件夹的形式，此时应该新建一个文件夹（文件名为英文，具体命名自拟），然后将所有的音频文件放入其中。我今天也碰到这个问题已成功解决    解压后发现G00XX下面还有个子文件夹G00XX  你需要把第二级的G00XX文件夹直接放在\train下 问题解决！最快捷的办法是对于压缩包GXXXX  选择解压到当前文件夹！解压后可以把原压缩包删除吧 
合成时人声电音爆裂问题

运行synthesizer_preprocess_audio.py进行预处理时报错
的line 100 把wav_fpath = book_dir.joinpath(wav_fname + ".flac")改为wav_fpath = book_dir.joinpath(wav_fname + ".wav")就好了 
延長訓練schedule steps，合成雜音多
rate。一般是先按照相对较大的learning rate训练，然后当Loss降不下去了就换较小的。或者无脑一点的，你就平均的给schedule中每个阶段加step也行，总比只加最后一个阶段的step来的好。> 建议用tensorboard看看记录的loss曲线然后决定什么时候减learning rate。一般是先按照相对较大的learning rate训练，然后当Loss降不下去了就换较小的。 或者无脑一点的，你就平均的给schedule中每个阶段加step也行，总比只加最后一个阶段的step来的好。+1， 手动 finetune 才是王道，有试一下我的freeze layer的代码吗？> +1， 手动 finetune 才是王道，有试一下我的freeze layer的代码吗？没注意到哪里有freeze > +1， 手动 finetune 才是王道，有试一下我的freeze layer的代码吗？> > 没注意到哪里有freeze layer相关的代码哎。。。 synthesizer/hparams.py 里面的67-68行于参数，可以选中你想要的训练的layer 
如何平稳的停止训练，手动中断训练后再次训练报错RuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory
+ fn + pause 
"AssertionError" when starting web.py | 运行web.py时出现"AssertionError"错误
When I started "web.py", I met the error message ended of "AssertionError".:我用python运行"web.py"时，遇到了以下问题，最后一行是“AssertionError”：(mockingbird) C:\Users\yisheng_zhou\Downloads\MockingBird> python web.pyLoaded synthesizer models: 2Loaded encoder "pretrained.pt" trained to step 1564501Building Wave-RNNTrainable Parameters: 4.481MLoading model weights at vocoder\saved_models\pretrained\pretrained.ptBuilding hifiganTraceback (most recent call last):  File "C:\Users\yisheng_zhou\Downloads\MockingBird\web.py", line 6, in <module>    app = webApp()  File "C:\Users\yisheng_zhou\Downloads\MockingBird\web\__init__.py", line 35, in webApp    gan_vocoder.load_model(Path("vocoder/saved_models/pretrained/g_hifigan.pt"))  File "C:\Users\yisheng_zhou\Downloads\MockingBird\vocoder\hifigan\inference.py", line 44, in load_model    state_dict_g = load_checkpoint(  File "C:\Users\yisheng_zhou\Downloads\MockingBird\vocoder\hifigan\inference.py", line 18, in load_checkpoint    assert os.path.isfile(filepath)AssertionErrorWas someone same to me? How to solve?各位大神有遇到和我一样问题的吗？怎么解决的？file of vocoder/saved_models/pretrained/g_hifigan.pt is missing? 
Arguments:     datasets_root:    samples     enc_models_dir:   encoder\saved_models     syn_models_dir:   synthesizer\saved_models     voc_models_dir:   vocoder\saved_models     cpu:              False     seed:             None     no_mp3_support:   False  qt.qpa.plugin: Could not find the Qt platform plugin "windows" in "" This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.
哪位大佬来看一下什么问题？pip install那一步出错误了吧？先解决依赖问题，实在不行建议用 anaconda 安装独立环境 
最后一步报错ModuleNotFoundError: No module named 'umap'
第一次让我选择打开方式，我选的python，之后就报错了，用pip更新显示没包，安装显示已有，但运行python demo_toolbox.py还是报错E:\Internet\MockingBird-main\MockingBird-main>python demo_toolbox.pyTraceback (most recent call last):  File "E:\Internet\MockingBird-main\MockingBird-main\demo_toolbox.py", line 2, in <module>    from toolbox import Toolbox  File "E:\Internet\MockingBird-main\MockingBird-main\toolbox\__init__.py", line 1, in <module>    from toolbox.ui import UI  File "E:\Internet\MockingBird-main\MockingBird-main\toolbox\ui.py", line 16, in <module>    import umapModuleNotFoundError: No module named 'umap'问题已解决，重启然后pip install umap 
求解
usage: demo_toolbox.py [-h] [-d DATASETS_ROOT] [-e ENC_MODELS_DIR] [-s SYN_MODELS_DIR] [-v VOC_MODELS_DIR] [--cpu] [--seed SEED] [--no_mp3_support]demo_toolbox.py: error: argument -d/--datasets_root: expected one argument看的教程太老了，这里需要输入一个音频目录，用于样本音频载入，可以是任意一个空的 
請問如何對特定一個人的語音進行訓練
我目前使用 PreTrain Model的合成器來合成語音，但效果不好(選用的是第三個pretrained model - @FawenYo，因為無法從百度網盤下載，沒試過作者的 Model )是否可以只針對某一個人的語音進行訓練? 需要訓練 synthesizer 還是 vocoder? 有嘗試拿 aidatatang_200zh dataset 重新訓練 synthesizer 效果不好 請問有沒有機會拿 pretrained.pt 加上自己的音檔做 transfer learning謝謝社区有很多成功案例，需要先准备该人语料（最好1分钟至少），参考数据集 aidatatang_200zh 制作数据集，基于原本的模型再训练10k步应该就有效果了。> 社区有很多成功案例，需要先准备该人语料（最好1分钟至少），参考数据集 aidatatang_200zh 制作数据集，基于原本的模型再训练10k步应该就有效果了。不好意思方便给个社区案例链接吗？感谢 找了一下没找到这个是在各个交流群里得实践，你也加一个交流群来一个怎么对特定人声进行训练呢？Repo的训练教程也没有说明。谢谢我也有这个需求，就是在原有的模型上再进行对自己特定的人声进行强化训练，希望出个教程，万分感谢 
运行 python demo_toolbox.py -d .\samples出错
之前步骤都没有报错，运行 python demo_toolbox.py -d .\samples不了C:\mockingbird\MockingBird-main>python demo_toolbox.py -d .\samplesArguments:    datasets_root:    samples    enc_models_dir:   encoder\saved_models    syn_models_dir:   synthesizer\saved_models    voc_models_dir:   vocoder\saved_models    cpu:              False    seed:             None    no_mp3_support:   Falseqt.qpa.plugin: Could not find the Qt platform plugin "windows" in ""This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem. 
是小白，刚用ceshi不知道为什么出现这样的问题，请教一下大佬
合成的声音只有2秒滋滋声，拉取的是最新代码，已参考#37 #209修改，无CUDA，用的是 python demo_toolbox.py --cpu -d v0.0.1的版本，然后加入#issue 37的修改。切换tag的命令：git checkout -b local_tag_0.0.1 v0.0.1在加入#issue 37 的修改后，执行git checkout -b local_tag_0.0.1   再python demo_toolbox.py --cpu -d E:\MockingBird-Add-GST\data问题仍在，RuntimeError: synthesizer\saved_models\mandarin\mandarin.pt is a zip archive (did you mean to use 你需要切换到标签 v0.0.1 的版本，然后加入#issue 37 的修改。 切换标签的命令：git checkout -b local_tag_0.0.1 v0.0.1大佬，请看上面运行的时候可以不用加参数-d，不加数据集也是可以的pytorch版本太低> 你需要切换到tag v0.0.1的版本，然后加入#issue 37的修改。 切换tag的命令：git checkout -b local_tag_0.0.1 你需要切换到tag v0.0.1的版本，然后加入#issue 37的修改。 切换tag的命令：git checkout -b local_tag_0.0.1 v0.0.1切换到tag v0.0.1的版本对我是有效的。切换到tag并不容易，除了大佬提出的方法之外，我提出个笨拙些的解决方法：之间在code的页面找到tag，点击进之后下载就行，解压后进入这个文件进行相关的操作即可。> 是的是的，这个办法好，可以直接下载对应tag打包的源文件，避免了git的操作> > 你需要切换到tag v0.0.1的版本，然后加入#issue 37的修改。 切换tag的命令：git checkout -b local_tag_0.0.1 v0.0.1> > 大佬，我为什么无法切换 纯小白，什么都不懂，还为此下了个 Bash来输入切换tag的命令。你第一个框中的错误是找不到git.exe，原因是windows的命令行窗口需要添加环境变量，把git.exe所在的目录添进去，你应该是没有添，所以就报错了，可以参考百度的这个链接尝试做一下：https://www.cnblogs.com/qingmuchuanqi48/p/12052289.html> 非常感谢😊> > 你需要切换到tag v0.0.1的版本，然后加入#issue 37的修改。 切换tag的命令：git checkout -b local_tag_0.0.1 v0.0.1> > 切换到tag v0.0.1的版本对我是有效的。切换到tag并不容易，除了大佬提出的方法之外，我提出个笨拙些的解决方法：之间在code的页面找到tag，点击进之后下载就行，解压后进入这个文件进行相关的操作即可。这个方法也不错，谢谢了😊 
模型训练时有 Creating a tensor from a list of numpy.ndarrays is extremely slow.警告会影响训练吗
以下两个标注的文本文件需要吗  a tensor from a list of numpy.ndarrays is extremely slow。有什么办法解决吗？用的是git上master最新版本Tue Dec 7。什么都没有改动> 还有我使用aishell3模型进行训练 以下两个标注的文本文件需要吗 还是只要音频文件就行  只要标注content.txt文件就行  可以使用程序自动标注   这个警告应该不影响训练在“synthesizer/synthesizer_dataset.py”的76行加上下面的代码就行了： 当然，训练速度并不会变快很多。> 在“synthesizer/synthesizer_dataset.py”的76行加上下面的代码就行了：  当然，训练速度并不会变快很多。多谢> 在“synthesizer/synthesizer_dataset.py”的76行加上下面的代码就行了：  当然，训练速度并不会变快很多。方便发起pr吗？@babysor 作者你好  方便发一下入群二维码吗> 方便发起pr吗？发了pr啦> @babysor 作者你好 方便发一下入群二维码吗进群了吗？> > 方便发起pr吗？> > 发了pr啦那我关闭本issue了 
进行音频和梅尔频谱图预处理时报错
![屏幕截图 2021-12-10 C:\Kuang\MockingBird-main>python pre.py D:\dataUsing data from:    D:\data\aidatatang_200zh\corpus\trainaidatatang_200zh: 100%|████████████████████████████████████████████████████████| 420/420 [01:00<00:00,  7.92speakers/s]The dataset consists of 0 utterances, 0 mel frames, 0 audio timesteps (0.00 hours).Traceback (most recent call last):  File "pre.py", line 74, in <module>    preprocess_dataset(**vars(args))  File "C:\Kuang\MockingBird-main\synthesizer\preprocess.py", line 88, in preprocess_dataset    print("Max input length (text chars): %d" % max(len(m[5]) for m in metadata))ValueError: max() arg is an empty 
怎么在Google Colab上运行？（在不想使用Toolbox和Web的情况下）
请问有没有人尝试过在Notebook上运行？（Google Colab或者Jupyter）我的想法是只提供一个或多个音频，和一句文字作为input，然后运行且生成一个output音频。_**附上我的Colab，效果不太理想。_[Colab 
demo_toolbox.py 出现 python.exe 中发生了未经处理的win32异常
点击 browse 加载音频文件时出错   系统是  win8.1  64位  显卡 amd  python 3.7.9   pyQt5 版本  pyQt 5.12.0  会不会pyQt5 
Fix TypeError at line 459 in toolbox/ui.py when both PySide6(PyQt6) and PyQt5 installed
### Error Info Error ReasonMatplotlib.backends.qt_compat.py decide the version of qt library according to sys.modules firstly, os.environ secondly and the sequence of PyQt6, PySide6, PyQt5, PySide 2 and etc finally. Import PyQt5 after matplotlib make that there is no PyQt5 in sys.modules so that it choose PyQt6 or PySide6 before PyQt5 if it installed.因为Matplotlib.backends.qt_compat.py优先根据导入的库决定要使用的Python Qt的库，如果没有导入则根据环境变量PYQT_APT决定，再不济就按照PyQt6, PySide6, PyQt5, PySide 2的顺序导入已经安装的库。因为ui.py先导入matplotlib而不是PYQT5导致matplotlib在导入的库里找不到Qt的库，又没有指定环境变量，然后用户安装了Qt6的库的话就导入Qt6的库去了 
ValueError: operands could not be broadcast together with shapes (2200,) (4000,) (2200,) 
Getting this error while running the synthesize() function on Google Colab. Any solution? same to meHave you solved this question?ValueError: operands could not be broadcast together with shapes (2600,) (4000,) (2600,)I am really sorry to attach my erro which is really out of my cognition.Could u guys share more context of this error (when did it show? And what env is u running?)I use the 'cehsi' model from @miven as the Synthesizer, while both Encoder and Vocoder are 'pretrained'. The same Exception shows when I click the "Synthesize and vocode" button. Detailes are as below:Generating the mel spectrogram...Waveform generation: 9500/9600 (batc size: 1, rate: 1.7kHz - 0.10x real time)Exception: operands could not be broadcast together with shapes (2400,) (4000,) (2400,)I remember there 
batch_size报错，
File "E:\python_learning\AI算法\MockingBird-main\synthesizer\models\tacotron.py", line 547, in load    self.load_state_dict(checkpoint["model_state"], strict=False)  File "E:\Anaconda\envs\python_learning\lib\site-packages\torch\nn\modules\module.py",line 1044, in load_state_dict    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]).        size mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([384, 1280]).        size mismatch for decoder.rnn_input.weight: copying a param with shape torch.Size([1024, 640]) from checkpoint, the shape in current model is torch.Size([1024, 1152]).        size mismatch for decoder.stop_proj.weight: copying a param with shape torch.Size([1, 1536]) from checkpoint, the shape in current model is torch.Size([1, 2048]).同样的错误你需要切换到tag v0.0.1的版本，然后加入#issue 37的修改。切换tag的命令：git checkout -b local_tag_0.0.1 v0.0.1 
可不可以添加一个inference.py，在指定一系列dir or文件名后就生成最终的音频呢？
抽离出pyqt的synthesis&generate按键绑定函数单独写一个python文件应该就可以了，这样可能对平时习惯用云端环境的人更友好一些。更重要的是，这样就不用在自己的电脑上配置半天环境了。写个简单的colab就可以更好地普及啦。（不介意的话我可以负责来写www）这个赞！资瓷！关注  可以一起合作呀不错的建议呀，不过我年底陷入kpi中，如果你们尝试搞得话，可以拉一下我做支援 @Rabbit314271 @xiaoqiao 
Update readme for training encoder
在中英文文档中添加了训练encoder的步骤 
报错：RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
作者你好，我使用的web.py运行的demo，百度云下载的synthesizer，生成语音的时候报错，请问有解决办法吗？csdn的一个回答是这样的，我没找到“”“在训练模型的时候，需要搞清楚有没有用多GPU训练使用pytorch正常加载模型的话：model.load_state_dict(torch.load(model_path))1如果在训练的时候使用到了多GPU训练model = torch.nn.DataParallel(model, device_ids=range(opt.ngpu))”“”你应该使用的代码版本比较新，但是百度下载的synthesizer是旧版本的，因此需要先把代码切换到旧版本，即tag v0.0.1，然后再加入#issue 37的修改，就可以跑了。这里跟多gpu训练无关切换分支的命令：git checkout -b local_tag_v0.0.1 v0.0.1#209 
RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)
作者好作者辛苦了，我又来问问题了，我也遇到了CUDA版本不兼容的问题cuda version 11.1 20280TiGPU，经常跑着跑着出现如标题的问题，请问应该怎么办呢这个我在不同的版本也遇见过，应该不是兼容性问题，而是内核不稳定同问，应该怎么解决有群友安装不同显卡驱动解决，试一下 
打扰大佬，请问报错 No such file or directory该如何解决呢，在J:\ImitateSound\MockingBird-main\vocoder\saved_models\pretrained下可以看到pretrained.pt 
Traceback (most recent call last):  File "J:\ImitateSound\MockingBird-main\pre.py", line 76, in <module>    create_embeddings(synthesizer_root=args.out_dir, n_processes=n_processes_embed, encoder_model_fpath=encoder_model_fpath)  File "J:\ImitateSound\MockingBird-main\synthesizer\preprocess.py", line 120, in create_embeddings    list(tqdm(job, "Embedding", len(fpaths), unit="utterances"))  File "C:\Users\62433\AppData\Local\Programs\Python\Python38\lib\site-packages\tqdm\std.py", line 1180, in __iter__    for obj in iterable:  File "C:\Users\62433\AppData\Local\Programs\Python\Python38\lib\multiprocessing\pool.py", line 868, in next    raise valueFileNotFoundError: [Errno 2] No such file or directory: 'encoder\\saved_models\\pretrained.pt'C:\Users\62433>python J:\ImitateSound\MockingBird-main\pre.py J:\xunlianji\ -d aishell3 -n 
直接用由dataset生成语音用什么好？
做一个游戏mod，想由现有的游戏语音文件来合成新配音，tarcon训练出了dataset就不知道怎么搞了。> 這個問題拉取請求已經合併，所以你可以根據鏈接的結果關閉它。謝謝 这里指哪个拉取请求呢？ 
第二次贡献模型，同时谈谈相关经验。
**aidatatang_200zh以及aishell3两个数据集，混合训练，batch size 96。**在训练40K就可以达到0.18了，不过我觉得还是多训练一会好， **所以分享的这个是70K、0.2的模型。**链接：https://pan.baidu.com/s/17yWmyq6_rh5MbCOwE3hH2Q 提取码：7777 - [ ] 最新版本可用接下来为个人训练模型经验，可不看。- [ ] 1、batch size对于loss值影响巨大，同样的混合训练，12和96这两个值会得到完全不同的结果，体现在：默认batch size 12的情况下，loss值很难达到0.35以下，而且不停的波动，且波动数值巨大。比如0.3跳0.5再跳回0.3。故个人建议尽可能开到比较大的batch size。32G显存在训练混合数据集时，最高稳定值在96，供参考。- [ ] 2、本项目对游戏角色语音，例如王者荣耀语音克隆，无法得到好的结果。体现在模型克隆出来出现严重的机器音，或者音色根本不像。个人猜测是因为提供的4个数据集本身就是日常的一个对话，并没有游戏角色语音的情感等，所以无法很好克隆游戏的角色语音。- [ ] 以上仅为萌新的个人经验，不代表是对的，也很有可能是错误的，仅供参考。非常感谢提供模型，刚刚尝试了一下，体验如下：1.整体效果较Readme中的第一个模型（三个数据集混合），还是有一些差距；2.电音有点大，我在测“北京天气很不错”的时候，“错”字会出现回声；3.在切换了录音人的时候，发现生成的是一样的音色，这一点有点不太理解（替换其他模型的时候，不会出现这样的问题）4.对于batch size 深以为然，我用的是batch 12，三个数据集混合，到133k step，依然是电音，效果很差，准备重新训练。再次感谢，希望大家一起交流，训练出好的模型> 非常感谢提供模型，刚刚尝试了一下，体验如下：> > 1.整体效果较Readme中的第一个模型（三个数据集混合），还是有一些差距； 2.电音有点大，我在测“北京天气很不错”的时候，“错”字会出现回声； 3.在切换了录音人的时候，发现生成的是一样的音色，这一点有点不太理解（替换其他模型的时候，不会出现这样的问题） 4.对于batch size 深以为然，我用的是batch 12，三个数据集混合，到133k step，依然是电音，效果很差，准备重新训练。> > 再次感谢，希望大家一起交流，训练出好的模型个人之前用220K的aishell3单数据集克隆真实的女生语音，测试时就很不错。这个只跑了70K应该是比较差的，虽然loss值好看。但我并没有实际测试过这个70K的，因为无法生成有效的游戏角色语音克隆，故就放弃。试用了几个现有的分享，能正常发音就很不错了，大多数情况下，要么是电音，要么就是部分是杂音，更不用说和原始声音相似了，距离克隆声音还有一段路，无奈自己的机器不行，没法自己训练> 试用了几个现有的分享，能正常发音就很不错了，大多数情况下，要么是电音，要么就是部分是杂音，更不用说和原始声音相似了，距离克隆声音还有一段路，无奈自己的机器不行，没法自己训练这个主要分享经验，如果你要相对好的结果，可以看我之前分享的160K aishell3，往后翻翻能找到@ferretgeek 非常感谢各位的无私分享其实iteration不是一个很好的指标，bs(batch BATCH SIZAE怎么调高 我感觉我的3080TI还能再冲一冲> 想咨询一下楼主 BATCH SIZAE怎么调高 我感觉我的3080TI还能再冲一冲訓練合成器時：將 synthesizer/hparams.py中的batch_size參數調小//調整前tts_schedule = [(2,  1e-3,  20_000,  12),   # Progressive training schedule                (2,  5e-4,  40_000,  12),   # (r, lr, step, batch_size)                (2,  2e-4,  80_000,  12),   #                (2,  1e-4, 160_000,  12),   # r = reduction factor (# of mel frames                (2,  3e-5, 320_000,  12),   #     synthesized for each decoder iteration)                (2,  1e-5, 640_000,  12)],  # lr = learning rate//調整後tts_schedule = [(2,  1e-3,  20_000,  8),   # Progressive training schedule                (2,  5e-4,  40_000,  8),   # (r, lr, step, batch_size)                (2,  2e-4,  80_000,  8),   #                (2,  1e-4, 160_000,  8),   # r = reduction factor (# of mel frames                (2,  3e-5, 320_000,  8),   #     synthesized for each decoder iteration)                (2,  1e-5, 640_000,  8)],  # lr = learning rate这里调整。另外3080TI的显存其实并不足以开多大，个人推荐从32开始看看能不能加，如果32都不行就慢慢减少成功了非常感谢~~~  我社区模型自己跑了20K左右 我现在改BATCH SIZE 建议继续跑呢 还是重新跑好 
【初尝试】使用Tacotron2替换Tacotron模型！！
由于之前一直关注Real-Time-Voice-Cloning(https://github.com/CorentinJ/Real-Time-Voice-Cloning) 这个项目，这次Mocking 大佬您好，现在一直在这个阶段，loss下降特别慢。。。。我的代码在https://github.com/Rita-ritally/Tacotron2-CN-tts width="636" alt="截屏2021-12-03 上午10 25 23" src="https://user-images.githubusercontent.com/45085467/144535352-2dd69287-dcc3-4e74-ac0c-93017bd50b6c.png"><img width="636" alt="截屏2021-12-03 上午10 26 15" src="https://user-images.githubusercontent.com/45085467/144535396-456c210e-571a-47c2-a274-ce5a5c29c516.png">现在还在继续训练，不知道是不是训练 步数少的问题。。。。。。现在效果会好一些了吗？我本来也打算fork个分支试试的，你这边可以fork试一下吗，成为contributor？> 现在效果会好一些了吗？我本来也打算fork个分支试试的，你这边可以fork试一下吗，成为contributor？在我的实验中使用aishell3数据集训练的效果比aidatatang效果好，由于aishell3男性说话人语料较小，发现无法正确合成男性说话人的声音。所以我将train和test的所有语料整合在一起训练，可以缓解这个问题。在我的仓库中只有synthesizer的code是有变化的，vocoder中加入了melgan和waveglow声码器，但是效果还不是很好。如果可以的话，非常荣幸能成为这个项目的contributor！！！！大佬牛啊！！！ 
是否实验过m2_hat用linear频谱计算误差呢？
您好，有注意到用了postnet处理mel为linear，但是看训练过程，并没有用linear的频谱，而是用mel的频谱。问下是否实验过m2_hat用linear频谱计算误差呢？如果有实验过，能否谈一下结果如何呢？下面贴上loss的计算：                # Backward pass                m1_loss = F.mse_loss(m1_hat, mels) + F.l1_loss(m1_hat, mels)                m2_loss = F.mse_loss(m2_hat, mels)                stop_loss = F.binary_cross_entropy(stop_pred, stop)hi, 久仰，这块有网友学生在做，但未同步实验结果。我这边主要在尝试方向是替换整个tacotron，不知道你这边有什么建议吗？语音合成模型现在有很多方案了，理论上都可以试试，可以看看这篇综述，有意思的可以拿过来。系统调研450篇文献，微软亚洲研究院推出超详尽语音合成综述https://www.msra.cn/zh-cn/news/features/neural-speech-synthesis-survey论文原文：https://arxiv.org/pdf/2106.15561.pdf我之前探索的经验是tacotron系列比较靠谱稳定，tacotron魔改也挺有用的。不过近一年来的算法我还没实验过，可能有质的飞跃呢！这个实现5秒语音克隆声音的状态是如何了呢？克隆语音与真人语音的逼真程度如何？克隆语音的音质、发音节奏、发音稳定性如何呢？> 这个实现5秒语音克隆声音的状态是如何了呢？ 克隆语音与真人语音的逼真程度如何？ 克隆语音的音质、发音节奏、发音稳定性如何呢？有足够多的开源数据集跑的情况下，现在音质和稳定性就比较好，就是节奏、语气模拟相似度几乎没有。我认为5秒克隆的话，可能在逼真度的上限比较低了，而且追求逼真的话有其他路径，所以可能找下适合的应用场景 
只有CPU没有GPU的机器，始终无法生成成功，只有2秒的电流音
**python版本：** **各类库文件版本** **报错信息（）：** > **python版本：** > > **各类库文件版本** > > **报错信息（）：**  不管是web版本，还是GUI版本，都无法生成正确的录音，都只有2秒的电流音。已经按照https://github.com/babysor/MockingBird/issues/37    这个里面的提示修改了字符集。测试环境：腾讯云2C+4G的标准版云服务器，Windows 2019版本。也在群里面按照其他同学的提示，修改了synthesizer/train.py 文件中的loaded_shape = torch.load(str(weights_fpath), map_location=device)，强行定义为loaded_shape = torch.load(str(weights_fpath), 尝试拉最新main，最新commit有做了一定的device信息传递感谢回复，刚刚拉了最新版的，问题依旧看提示，需要你改成  而不是 使用 python demo_toolbox.py --cpu  之后一样是只有两声电流声@work4seo 尝试过吗，改成 map_location=torch.device('cpu') 我的也是我的也是cpu机器，强制将train.py改到loaded_shape = torch.load(str(weights_fpath), map_location=torch.device("cpu"))后问题依旧；RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]).        size mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([384, 1280]).        size mismatch for decoder.rnn_input.weight: copying a param with shape torch.Size([1024, 640]) from checkpoint, the shape in current model is torch.Size([1024, 1152]).        size mismatch for decoder.stop_proj.weight: copying a param with shape torch.Size([1, 1536]) from checkpoint, the shape in current model is torch.Size([1, 2048]).> 我的也是cpu机器，强制将train.py改到loaded_shape = torch.load(str(weights_fpath), map_location=torch.device("cpu"))后问题依旧； RuntimeError: Error(s) in loading state_dict for Tacotron: size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]). size mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([384, 1280]). size mismatch for decoder.rnn_input.weight: copying a param with shape torch.Size([1024, 640]) from checkpoint, the shape in current model is torch.Size([1024, 1152]). size mismatch for decoder.stop_proj.weight: copying a param with shape torch.Size([1, 1536]) from checkpoint, the shape in current model is torch.Size([1, 2048]).这个跟cpu没关系，看下issue相关解决方法 
Fix inference on cpu device
Fix the failure of inference on CPU-only device. 
1k steps to save tmp hifigan model
Hifigan‘ tmp model is saved every 1000 steps. 
分享600kstep模型
mockingbirdsynthesizer-model链接: 提取码: j23j 使用aidatatang_200zh训练至600ksteploss到0.3，在0.27-0.54波动好人一生平安感谢分享！> mockingbird synthesizer-model 链接: 提取码: j23j> > 使用aidatatang_200zh训练至600kstep j23jRuntimeError: Error(s) in loading state_dict for Tacotron:	size mismatch for gst.stl.attention.W_query.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).你使用的代码，是什么时候的，最新版本报错这个训练的相当不错这个不错，很厉害> > j23j> > RuntimeError: Error(s) in loading state_dict for Tacotron: size mismatch for gst.stl.attention.W_query.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]). 你使用的代码，是什么时候的，最新版本报错使用的代码在0.0.1之前分享的模型，要求0.0.1版的代码，老版本的在哪儿有@Yuqi154 你好  可以分享一下代码吗  我使用0.0.1版本的代码  用分享的模型还是报错哦@Yuqi154 同问，大大可以分享下这个模型用的是什么时候的代码版本吗？我也是用了tag0.0.1的版本，也是报错。报错如下Traceback (most recent call last):  File "D:\Github\MockingBird-0.0.1\toolbox\__init__.py", line 123, in <lambda>    func = lambda: self.synthesize() or self.vocode()  File "D:\Github\MockingBird-0.0.1\toolbox\__init__.py", line 238, in synthesize    specs = self.synthesizer.synthesize_spectrograms(texts, embeds, style_idx=int(self.ui.style_slider.value()), min_stop_token=min_token)  File "D:\Github\MockingBird-0.0.1\synthesizer\inference.py", line 87, in synthesize_spectrograms    self.load()  File "D:\Github\MockingBird-0.0.1\synthesizer\inference.py", line 65, in load    self._model.load(self.model_fpath)  File "D:\Github\MockingBird-0.0.1\synthesizer\models\tacotron.py", line 523, in load    self.load_state_dict(checkpoint["model_state"], strict=False)  File "D:\PCInstall\Code\Anaconda3\envs\bird\lib\site-packages\torch\nn\modules\module.py", line 1482, in load_state_dict    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).        size mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 1280]) from checkpoint, the shape in current model is torch.Size([384, 768]).        size mismatch for decoder.rnn_input.weight: copying a param with shape torch.Size([1024, 1152]) from checkpoint, the shape in current model is torch.Size([1024, 640]).        size mismatch for decoder.stop_proj.weight: copying a param with shape torch.Size([1, 2048]) from checkpoint, the shape in current model is torch.Size([1, 1536]).Traceback (most recent call last):  File "D:\PCInstall\Code\Anaconda3\envs\bird\lib\site-packages\matplotlib\backends\backend_qt.py", line 455, in _draw_idle    self.draw()  File "D:\PCInstall\Code\Anaconda3\envs\bird\lib\site-packages\matplotlib\backends\backend_agg.py", line 436, in draw    self.figure.draw(self.renderer)  File "D:\PCInstall\Code\Anaconda3\envs\bird\lib\site-packages\matplotlib\artist.py", line 73, in draw_wrapper    result = draw(artist, renderer, *args, **kwargs)  File "D:\PCInstall\Code\Anaconda3\envs\bird\lib\site-packages\matplotlib\artist.py", line 50, in draw_wrapper    return draw(artist, renderer)  File "D:\PCInstall\Code\Anaconda3\envs\bird\lib\site-packages\matplotlib\figure.py", line 2789, in draw    artists = self._get_draw_artists(renderer)  File "D:\PCInstall\Code\Anaconda3\envs\bird\lib\site-packages\matplotlib\figure.py", line 238, in _get_draw_artists    ax.apply_aspect()  File "D:\PCInstall\Code\Anaconda3\envs\bird\lib\site-packages\matplotlib\axes\_base.py", line 1967, in apply_aspect    self.set_xbound(x_trf.inverted().transform([x0, x1]))  File "D:\PCInstall\Code\Anaconda3\envs\bird\lib\site-packages\matplotlib\axes\_base.py", line 3560, in set_xbound    self.set_xlim(sorted((lower, upper),  File "D:\PCInstall\Code\Anaconda3\envs\bird\lib\site-packages\matplotlib\axes\_base.py", line 3684, in set_xlim    left = self._validate_converted_limits(left, self.convert_xunits)  File "D:\PCInstall\Code\Anaconda3\envs\bird\lib\site-packages\matplotlib\axes\_base.py", line 3601, in _validate_converted_limits    raise ValueError("Axis limits cannot be NaN or Inf")ValueError: Axis limits cannot be NaN or Inf版本是11.6号的zip，下载链接: 提取码: 24bf 这个模型我用带.mp4后缀的文件作为输入，输出的声音就糊了，我把后缀改了之后依旧如此，其他.wav好像没有问题可以請求一個模型放在google雲的連結嗎? 謝謝> 可以請求一個模型放在google雲的連結嗎? 謝謝可能不行，OneDrive可以吗？> > 可以請求一個模型放在google雲的連結嗎? 謝謝> > 可能不行，OneDrive可以吗？沒有問題 麻煩您了 再次感謝> > > 可以請求一個模型放在google雲的連結嗎? 謝謝> > > > > > 可能不行，OneDrive可以吗？> > 沒有問題 麻煩您了 再次感謝感谢大陆比乌龟还慢的网速，现在有了https://1drv.ms/u/s!ArkeI_kGgBy2gtAHYi5T5xvfMhSpYg?e=UgGXN5> mockingbird synthesizer-model 链接: 提取码: j23j> > 使用aidatatang_200zh训练至600kstep loss到0.3，在0.27-0.54波动请问batchsize设的多少呀？我用aidatatang_200zh数据集训练，到200kstep loss一直在0.5左右下不去了，这样正常吗？ 
用社区分享的模型训练报错 不知道原因
用社区分享的模型训练报错 不知道原因 而且不知道咋保存模型 是不是必须要每500步才会自动保存 求各位大佬解惑 感谢！RuntimeError: The size of tensor a (1024) must match the size of tensor b (3) at non-singleton dimension 3![屏幕截图 2021-11-28 2021-11-28 把symbols的修改改回去您好，改回去就显示“RuntimeError: The size of tensor a (1024) must match the size of tensor b (3) at non-singleton dimension 3”，又回去了拉最新main的代码 > 拉最新main的代码感谢大佬 现在能运行了 再请问一下如何保存进度，是按ctrl+c吗> 训练的时候通过参数可以设置多少step保存模型，多少step保存一个新的pt模型> > 拉最新main的代码> > 感谢大佬 现在能运行了 再请问一下如何保存进度，是按ctrl+c吗自动保存，1000 step一次 @Olivia-Ye > > > 拉最新main的代码> > > > > > 感谢大佬 现在能运行了 再请问一下如何保存进度，是按ctrl+c吗> > 自动保存，1000 step一次 @Olivia-Ye感谢大佬解答> > > > 训练的时候通过参数可以设置多少step保存模型，多少step保存一个新的pt模型感谢大佬解答 
训练hifigan vocoder不输出.pt模型，只有log
如题，训练能跑起来但不输出.pt。这应该不是个案了最好发一下系统平台配置，以及’pip freeze’ 软件版本，这样才有人会回复保存点的steps之前写太多了，没有临时保存，等会修复一下。已解决 #240 还是只输出了log，我已经把代码里面的save_every改为10了  还是不行 大佬  你的可以了吗> @HuaHuaOfficial 大佬 你的可以了吗你拉取最新版本的代码了吗？> > @HuaHuaOfficial 大佬 你的可以了吗> > 你拉取最新版本的代码了吗？拉取了 最新的main分支 训练时有个警告  然后就一直在训练 但是目录没有模型生成  可以试试看  我改了一下代码可以用了  最新代码训练wavernn模型时也有bug 可以试试看 我改了一下代码可以用了 因为不是专业写python的就不pr了 
最大输入文数目
作者大大，以及群里的各位大神，求助哈。我在使用网页版的时候，发现最多只能识别10个字（不加标点符号，不加换行符）为了增加可以识别的字符数，我更改gst_hyperparameters.py 中的 token_num = 40 但是训练之后，依然没有效果，请问如果想要支持四十个字输入，参数应该怎么调整，再次感谢，你代码已经是最新的吗？这里不是修改token num而是在synthesizer inference的时候 ，有个最大iteration参数，参考demo tool的最大句长参数修改即可已经通过和大神的交流，问题解决了，在这里把方法记录一下，修改 tacotron.py 中 generate函数中的step参数，目前最新版本已经修该到了2000，已经没有字数限制了。 
请问可否设计一份训练中断后重启并避开已训练数据集的功能?
如题， 在 戴尔G15 rtx3050 4G, cuda11.3 的环境下遇到了显存不足后训练中断的情况 (一天内发生三次)在输入相同指令后，程序又把原本跑过的数据集又跑了一遍...训练数据集使用_200zh， Batch Size =4啊，cuda版本是 看错了抱歉...我用的是1050ti 内存不足就调小了batch_size  一定要配置好cuda 做训练，统一内存怕是还不如1050Ti . Pytorch在m1上目前只支持cpu 数据集每次加载都会随机打乱，不需要刻意避开。 
请问训练到25k的时候注意力线还是没有出来，并且文件多出了一个出来这个是正常的吗？
Training    tts_schedule = [(2,  1e-3,  10_000,  6),   # Progressive training schedule                    (2,  5e-4,  15_000,  6),   # (r, lr, step, batch_size)                    (2,  2e-4,  20_000,  6),   # (r, lr, step, batch_size)                    (2,  1e-4,  30_000,  6),   #                    (2,  5e-5,  40_000,  6),   #                    (2,  1e-5,  60_000,  6),   #                    (2,  5e-6, 160_000,  6),   # r = reduction factor (# of mel frames                    (2,  3e-6, 320_000,  6),   #     synthesized for each decoder iteration)                    (2,  1e-6, 640_000,  6)],  # lr = learning rate还请大佬给指点指点，谢谢！还有想问下有没有交流群可以拉下，学习学习_25k ，_50k 等文件是你在步数达到该步后生成的模型文件(相当于游戏存档了啊哈哈)另外...我的注意力线是在步数达到72k之后才出现的...届时的loss 是 0.61, 还达不到标准1505ti 是指1050ti的话，训练速度还要更慢一些✓ 请耐心等待>吐槽一句，我的环境跑了两个整天(48h)了都还是有明显的低频电流音...,Loss也是维持在0.6就没变过了...@ 在别人分享的模型基础之上继续训练，训练集要求和之前的一样吗。我的显存8g，batchsize设置多少不会报错，太小了就没必要折腾了训练集可以完全不一样，batchsize在20以内，你可以试探一下，只要不崩溃就可以调大你的数据集用了多少条音源？，做下参考我20k出现注意力线还以为是最慢的了。。> 嗯。看这个attention图貌似问题比较大，考虑基于我分享的模型上跑？想请教，您分享打的模型在哪里看到？我要咋操作在模型基础上继续跑？要重新开始吗？> > 在别人分享的模型基础之上继续训练，训练集要求和之前的一样吗。我的显存8g，batchsize设置多少不会报错，太小了就没必要折腾了> > 训练集可以完全不一样，batchsize在20以内，你可以试探一下，只要不崩溃就可以调大请问怎么调整batch_size，没有找到参数设置位置，默认好像是12？可以下载别人做好的模型，名字改成和自己模型名字一样就行。显卡1050跑这个很吃力，可以在别人基础上跑。 
能输入日语吗
有足够的样本的前提下我想自己训练出来一个日语模型，不知道行不行你这边有足够的数据集的话，应该是没有问题，要找到一个tts前端处理一下日语为phenomenon > 你这边有足够的数据集的话，应该是没有问题，要找到一个tts前端处理一下日语为phenomenon好的，我试一下。另外您提到  是什么意思呢？求解答就是把中文句子例如 “你好” 变成 “ni2 hao3”请问下足够的数据集指的是用许多不同人的声音的数据集来训练synthesizer模型吗？大概需要多少人和多少个小时的数据呢？如果我只想获得一个特定的人的日语声音并且拥有该特定人的足够多声音（2000条以上），是否能够实现这一个人的日语输出呢> 请问下足够的数据集指的是用许多不同人的声音的数据集来训练synthesizer模型吗？大概需要多少人和多少个小时的数据呢？ 如果我只想获得一个特定的人的日语声音并且拥有该特定人的足够多声音（2000条以上），是否能够实现这一个人的日语输出呢看着不够，时长多少呢？> > 请问下足够的数据集指的是用许多不同人的声音的数据集来训练synthesizer模型吗？大概需要多少人和多少个小时的数据呢？ 如果我只想获得一个特定的人的日语声音并且拥有该特定人的足够多声音（2000条以上），是否能够实现这一个人的日语输出呢> > 看着不够，时长多少呢？15个小时左右吧，我只想要获取这一个人的声音也不可以吗那你先得训练一个可用的模型，再finetune就可以> 那你先得训练一个可用的模型，再finetune就可以finetune是和原来的pretrain模型进行还是和这个项目的模型进行效果会更好？还是说没进行之前也不知道结果呢。还有如果是不同的语言的话（和这个的中文和原项目的英文不同的语言），是否需要进行2.3的训练声码器呢后续讨论请移步： 
方言的训练相关问题
想请教如何创建一个方言数据集并合理生成模型? 目标是训练 **上海话** 以及其它吴语系方言已经有足量的数据集了吗？> 已经有足量的数据集了吗？有条件生成足量的音源样本，但是不知道训练所需的文件格式(除了.wav之外的文件)>啊>想到应该怎么问这个问题了请问方言在数据集内如何标注拼音呢不同方言应该不太一样，举个例子我如果处理潮汕话或者粤语，可能 phenomenon 后面跟随的数字会是 1-8，而不是现在普通话的1-5@ycMia 方言这个问题可以群里提问，你数据没问题的话，我们可以交流一下~> 已经有足量的数据集了吗？请问足量的数据集是指多少呢，最低要求大概是多少呢，谢谢您> > 已经有足量的数据集了吗？> > 请问足量的数据集是指多少呢，最低要求大概是多少呢，谢谢您最好是百小时级别，如果无法达到要求，就需要比较多其他专业技巧了请后续关注 
tacotron.py-Multi GPU with DataParallel
主要解决多GPU并行处理数据的问题，有三个方面的修改：一、class Tacotron->forward()中将device = next(self.parameters()).device改为device = texts.device（parameters() on the replicated models are no longer populated. ）注：文件中其它next(self.parameters()).device依此修改二、class Decoder->zoneout(elf, prev, current, device, p=0.1)中增加了device参数，否则报两个device错误（cuda:0和cpu）forward()中device = encoder_seq.device，并在调用zoneout时指定device三、class CBHG->forward()中将self._flatten_parameters()（无效，有warning）改为self.rnn.flatten_parameters() 
is the hifigan vocoder is working with the tool box that you have provided?
I do want to try this  with a vocoder which has trained for the english speakers.. how can i do that? can i have a little guidance on that?This repository is forked from Real-Time-Voice-Cloning which only support English.You'd better see this maybe you can train synthesizer with English dataset, and keep the vocoder and encoder remain> vocoderwhat vocoder model do you want to use? The one of WaveRNN in this repo is originally trained with English. For the question you asked in issue title, answer is yes, and it works event better than the wavernnhighly appreciated your reply. I  have used the toolbox provided by the with the Then I have used your work to change the vocoder into the hifigan. but in the end, there is no difference in the outputs . It gives the same outputs for the bother wave rnn and hifigan vocoder.  I wanted to know whether this scenario is normal to not? ## I have not trained hifigan model, instead of that I have used ur weight files.> This repository is forked from Real-Time-Voice-Cloning which only support English. You'd better see this I have referred it . thanks > highly appreciated your reply. I have used the toolbox provided by the with the Then I have used your work to change the vocoder into the hifigan. but in the end, there is no difference in the outputs . It gives the same outputs for the bother wave rnn and hifigan vocoder. I wanted to know whether this scenario is normal to not? ## I have not trained hifigan model, instead of that I have used ur weight files.The toolbox from won't work with our HifiGan in this repo for sure.  Please try the toolbox in this repo and see how different it works with this vocoder.I want to have the results for the English speakers, can you please give help with that? can I do it by replacing the pretrained.pt file of the synthesizer in this repo with the pretrained.pt  file of the  "https://github.com/CorentinJ/Real-Time-Voice-Cloning" repo?sorry for the inconvenience !> This won't work as well. Now these two models has too much difference on states to be compatible.Then I have to train the synthesizer in this repo using an English speaker data set. Since I don't have enough resources to train a model , cant I use a pre-trained model (trained with English speakers) for the synthesizer? if it is so where can i find a compatible pretrained model ? do you have any guidance for that ? thank you in advance!!!!!! 
AttributeError: 'HParams' object has no attribute 'tts_schedule'

attention效果图不好

无法运行demo_toolbox
打不开demo_toolbox,出现了下述情况C:\ffmpeg-4.4.1-essentials_build\MockingBird-main>python demo_toolbox.pyArguments:    datasets_root:    None    enc_models_dir:   encoder\saved_models    syn_models_dir:   synthesizer\saved_models    voc_models_dir:   vocoder\saved_models    cpu:              False    seed:             None    no_mp3_support:   Falseqt.qpa.plugin: Could not find the Qt platform plugin "windows" in ""This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.我重装了pyqt5，无法解决系统相关信息可以贴一下吗？看情况你没有装python虚拟环境？我是按照一个视频流程来的，确实没装anaconda,我去试试> 我是按照一个视频流程来的，确实没装anaconda,我去试试如果系统之前python有比较多依赖的冲突，很难跑起来虽然不知道发生了什么，但是解决了，确实是要放在环境里> 虽然不知道发生了什么，但是解决了，确实是要放在环境里Warning: you did not pass a root directory for datasets as argument.> > 虽然不知道发生了什么，但是解决了，确实是要放在环境里> > Warning: you did not pass a root directory for datasets as argument.百度翻译一下就知道了。。。 
出现很重的电子音
我没有使用数据集, 我使用的是训练好的模型pretrained-11-7-21_75k结果合成出来的音频有很严重的电子音, 尝试合成多次都无法消除我看到了之前您提出的解决方法'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz12340!\'(),-.:;? '但修改后反而出现了self.load_state_dict(checkpoint["model_state"], strict=False)  File "C:\null\python3.8\lib\site-packages\torch\nn\modules\module.py", line 1482, in load_state_dict    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([75, 512]) from checkpoint, the shape in current model is torch.Size([70, 512]).没有修改之前是可以正常使用的, 但会有无法消除的电子音, 我应该如何解决这个问题?同问> 我没有使用数据集, 我使用的是训练好的模型pretrained-11-7-21_75k 结果合成出来的音频有很严重的电子音, 尝试合成多次都无法消除 我看到了之前您提出的解决方法'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz12340!'(),-.:;? ' 但修改后反而出现了 self.load_state_dict(checkpoint["model_state"], strict=False) File "C:\null\python3.8\lib\site-packages\torch\nn\modules\module.py", line 1482, in load_state_dict raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format( RuntimeError: Error(s) in loading state_dict for Tacotron: size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([75, 512]) from checkpoint, the shape in current model is torch.Size([70, 512]).> > 没有修改之前是可以正常使用的, 但会有无法消除的电子音, 我应该如何解决这个问题?如何是能听清楚发音 但是有电子音的话 可以看下教程中提到的使用技巧或者自己再训练。 
在跑了一下之后更换75k的预训练模型报错
按照之前issue里的方法修改了仍然报错，Tag0.0.1也不行Arguments:    run_id:          test    syn_dir:         ..\DataMaker\SV2TTS\synthesizer    models_dir:      synthesizer/saved_models/    save_every:      1000    backup_every:    25000    log_every:       200    force_restart:   False    hparams:Checkpoint path: synthesizer\saved_models\test\test.ptLoading training data from: ..\DataMaker\SV2TTS\synthesizer\train.txtUsing model: TacotronUsing device: cudaInitialising Tacotron Model...WARNING: you are using compatible mode due to wrong sympols length, please modify varible _characters in Trainable Parameters: 32.866MLoading weights at synthesizer\saved_models\test\test.ptTraceback (most recent call last):  File "D:\ProgrameWork\Deep\Voice\MockingBird\synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "D:\ProgrameWork\Deep\Voice\MockingBird\synthesizer\train.py", line 114, in train    model.load(weights_fpath, optimizer)  File "D:\ProgrameWork\Deep\Voice\MockingBird\synthesizer\models\tacotron.py", line 547, in load    self.load_state_dict(checkpoint["model_state"], strict=False)  File "C:\Users\Hetols\AppData\Roaming\Python\Python39\site-packages\torch\nn\modules\module.py", line 1482, in load_state_dict    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([75, 512]) from checkpoint, the shape in current model is torch.Size([70, 512]).是新训练的模型吗？> 是新训练的模型吗？是在开始训练新模型后（已经生成了文件），按Ctrl+C退出控制台，替换后再次执行就会出错。在此之后，如果删除替换的模型（重新开始） 或 替换回之前的模型，它又可以正常运行。已经解决了，11月18日更新的代码无修改+75K可以正常运行 
有的时候点击合成，就出现报错
报错内容：Loaded encoder "pretrained.pt" trained to step 1594501Synthesizer using device: cudaTrainable Parameters: 32.869MTraceback (most recent call last):  File "C:\德丽莎\toolbox\__init__.py", line 123, in <lambda>    func = lambda: self.synthesize() or self.vocode()  File "C:\德丽莎\toolbox\__init__.py", line 238, in synthesize    specs = self.synthesizer.synthesize_spectrograms(texts, embeds, style_idx=int(self.ui.style_slider.value()), min_stop_token=min_token, steps=int(self.ui.length_slider.value())*200)  File "C:\德丽莎\synthesizer\inference.py", line 87, in synthesize_spectrograms    self.load()  File "C:\德丽莎\synthesizer\inference.py", line 65, in load    self._model.load(self.model_fpath)  File "C:\德丽莎\synthesizer\models\tacotron.py", line 547, in load    self.load_state_dict(checkpoint["model_state"], strict=False)  File "D:\anaconda3\envs\Theresa\lib\site-packages\torch\nn\modules\module.py", line 1482, in load_state_dict    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for gst.stl.attention.W_query.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).然后重新生成就又可以用了有兼容错误，最新的代码分支吗？是的用的哪个模型？https://www.bilibili.com/video/BV1DL4y1q7VL这个视频里给的> 这个视频里给的第三方模型嘛。。可能训练的时候用的旧代码，要切换一下 
可以音频文件导入吗
录音进去 那步可以 好的 谢谢 
运行不了，找不到指定的模块
Traceback (most recent call last):  File "C:\ProgramData\Anaconda3\lib\site-packages\numpy\core\__init__.py", line 16, in <module>    from . import multiarrayImportError: DLL load failed: 找不到指定的模块。During handling of the above exception, another exception occurred:Traceback (most recent call last):  File "demo_toolbox.py", line 2, in <module>    from toolbox import Toolbox  File "D:\MockingBird-main\toolbox\__init__.py", line 1, in <module>    from toolbox.ui import UI  File "D:\MockingBird-main\toolbox\ui.py", line 1, in <module>    import matplotlib.pyplot as plt  File "C:\ProgramData\Anaconda3\lib\site-packages\matplotlib\__init__.py", line 126, in <module>    from . import cbook  File "C:\ProgramData\Anaconda3\lib\site-packages\matplotlib\cbook\__init__.py", line 34, in <module>    import numpy as np  File "C:\ProgramData\Anaconda3\lib\site-packages\numpy\__init__.py", line 142, in <module>    from . import add_newdocs  File "C:\ProgramData\Anaconda3\lib\site-packages\numpy\add_newdocs.py", line 13, in <module>    from numpy.lib import add_newdoc  File "C:\ProgramData\Anaconda3\lib\site-packages\numpy\lib\__init__.py", line 8, in <module>    from .type_check import *  File "C:\ProgramData\Anaconda3\lib\site-packages\numpy\lib\type_check.py", line 11, in <module>    import numpy.core.numeric as _nx  File "C:\ProgramData\Anaconda3\lib\site-packages\numpy\core\__init__.py", line 26, in <module>    raise ImportError(msg)ImportError:Importing the multiarray numpy extension module failed.  Mostlikely you are trying to import a failed build of numpy.If you're working with a numpy git repo, try  (removes allfiles not under version control).  Otherwise reinstall numpy.Original error was: DLL load failed: 找不到指定的模块。重装numpy后multiarray解决了，但还是不行Traceback (most recent call last):  File "demo_toolbox.py", line 2, in <module>    from toolbox import Toolbox  File "D:\MockingBird-main\toolbox\__init__.py", line 1, in <module>    from toolbox.ui import UI  File "D:\MockingBird-main\toolbox\ui.py", line 2, in <module>    from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas  File "C:\ProgramData\Anaconda3\lib\site-packages\matplotlib\backends\backend_qt5agg.py", line 5, in <module>    from .backend_qtagg import (  File "C:\ProgramData\Anaconda3\lib\site-packages\matplotlib\backends\backend_qtagg.py", line 9, in <module>    from .qt_compat import QT_API, _enum, _setDevicePixelRatio  File "C:\ProgramData\Anaconda3\lib\site-packages\matplotlib\backends\qt_compat.py", line 128, in <module>    raise ImportError("Failed to import any qt binding")ImportError: Failed to import any qt bindingpip install过程有报错吗？可能依赖冲突了 试着用anaconda 
快速上手最后一步执行时出现这个问题
E:\MockingBird-main>python demo_toolbox.py -d .\samplesTraceback (most recent call last):  File "E:\MockingBird-main\demo_toolbox.py", line 2, in <module>    from toolbox import Toolbox  File "E:\MockingBird-main\toolbox\__init__.py", line 1, in <module>    from toolbox.ui import UI  File "E:\MockingBird-main\toolbox\ui.py", line 1, in <module>    import matplotlib.pyplot as pltModuleNotFoundError: No module named 'matplotlib'是不是环境里有冲突 pip install的时候没安装好依赖我也 遇到这个问题 不知道 什么 原因已经解决，我发现是我用的python版本太新导致的QJ ***@***.***> 于 2021年11月20日周六 下午8:24写道：> 我也 遇到这个问题 不知道 什么 原因>> —> You are receiving this because you authored the thread.> Reply to this email directly, view it on GitHub> <https://github.com/babysor/MockingBird/issues/220#issuecomment-974642516>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AWJTRCCIZIZ6TIOXNVLFKWLUM6HO3ANCNFSM5IIWRNYQ>> .> Triage notifications on the go with GitHub Mobile for iOS> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>> or Android> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.>>有没有闪退呢，我之前启动GUI闪退了然后按照作者这个方法就可以了，链接：https://github.com/babysor/MockingBird/wiki/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF 
训练时出现这个错误
(mockingbird) K:\mb>python synthesizer_train.py 75k k:/mb/datame/SV2TTS/synthesizer -b 1000Arguments:    run_id:          75k    syn_dir:         k:/mb/datame/SV2TTS/synthesizer    models_dir:      synthesizer/saved_models/    save_every:      1000    backup_every:    1000    log_every:       200    force_restart:   False    hparams:Checkpoint path: synthesizer\saved_models\75k\75k.ptLoading training data from: k:\mb\datame\SV2TTS\synthesizer\train.txtUsing model: TacotronUsing device: cpuInitialising Tacotron Model...Trainable Parameters: 32.869MLoading weights at synthesizer\saved_models\75k\75k.ptTacotron weights loaded from step 93000Using inputs from:        k:\mb\datame\SV2TTS\synthesizer\train.txt        k:\mb\datame\SV2TTS\synthesizer\mels        k:\mb\datame\SV2TTS\synthesizer\embedsFound 12 samples+----------------+------------+---------------+------------------+| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |+----------------+------------+---------------+------------------+|   67k Steps    |     12     |     5e-06     |        2         |+----------------+------------+---------------+------------------+K:\mb\synthesizer\synthesizer_dataset.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\torch\csrc\utils\tensor_new.cpp:201.)  embeds = torch.tensor(embeds)f:\anaconda3\envs\mockingbird\lib\site-packages\torch\nn\functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")Traceback (most recent call last):  File "synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "K:\mb\synthesizer\train.py", line 208, in train    optimizer.step()  File "f:\anaconda3\envs\mockingbird\lib\site-packages\torch\optim\optimizer.py", line 88, in wrapper    return func(*args, **kwargs)  File "f:\anaconda3\envs\mockingbird\lib\site-packages\torch\autograd\grad_mode.py", line 28, in decorate_context    return func(*args, **kwargs)  File "f:\anaconda3\envs\mockingbird\lib\site-packages\torch\optim\adam.py", line 133, in step    F.adam(params_with_grad,  File "f:\anaconda3\envs\mockingbird\lib\site-packages\torch\optim\_functional.py", line 86, in adam    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)RuntimeError: The size of tensor a (1024) must match the size of tensor b (3) at non-singleton dimension 3这个模型用的是主页提供的75k steps模型似乎也是复用模型导致的，我从一个全新的模型开始训练的话就正常，但是我已经试过把hparams里的use_gst 和 use_ser_for_gst 都分别和一起生成False了，还是报错拉最新的代码试试，应该有个bug - 最新commit描述了：pytorch对模块顺序有一定要求。> 拉最新的代码试试，应该有个bug - 最新commit描述了：pytorch对模块顺序有一定要求。用的就是最新的代码，结果是主页上挂的模型都无法复用，反而是我稍早一点下的版本里，75K那个模型可以接着训练最新的应该可以啦 只有一个commit是不行的 
生成电子杂音，按#37修改没用
> 没有切换到tag0.01，所以不行，现在可以了请问下tag0.01是指的什么 
demo闪退
Feel free to add your own. You can still use the toolbox by recording samples yourself.Traceback (most recent call last):  File "C:\Users\sha7dow\OneDrive - longessay\文档\GitHub\MockingBird\demo_toolbox.py", line 43, in <module>    Toolbox(**vars(args))  File "C:\Users\sha7dow\OneDrive - longessay\文档\GitHub\MockingBird\toolbox\__init__.py", line 76, in __init__    self.setup_events()  File "C:\Users\sha7dow\OneDrive - longessay\文档\GitHub\MockingBird\toolbox\__init__.py", line 113, in setup_events    self.ui.setup_audio_devices(Synthesizer.sample_rate)  File "C:\Users\sha7dow\OneDrive - longessay\文档\GitHub\MockingBird\toolbox\ui.py", line 149, in setup_audio_devices    for device in sd.query_devices():  File "C:\ProgramData\Anaconda3\envs\a\lib\site-packages\sounddevice.py", line 559, in query_devices    return DeviceList(query_devices(i)  File "C:\ProgramData\Anaconda3\envs\a\lib\site-packages\sounddevice.py", line 559, in <genexpr>    return DeviceList(query_devices(i)  File "C:\ProgramData\Anaconda3\envs\a\lib\site-packages\sounddevice.py", line 573, in query_devices    name = name_bytes.decode('utf-8')UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 6: invalid continuation byte网页版报错Uncaught Error】Uncaught SyntaxError: Unexpected token '<'at:1:1 url:http://localhost:8080/SyntaxError: Unexpected token '<'    at Function.onload (chrome-extension://gcalenpjmijncebpfijmoaglllgpjagf/userscript.html?name=Magic%2520Userscript%252B%2520%253A%2520Show%2520Site%2520All%2520UserJS.user.js&id=91699e5b-1e20-450a-bbaf-8e2d6d8785d8:71:17)    at s (:8:264)    at eval (eval at exec_fn (:1:455), :21:226)    at s (:8:264)    at i (eval at exec_fn (:1:455), :3:215)    at eval (eval at exec_fn (:1:455), :3:342)    at :5:275    at HTMLDocument._ (:5:294)> Feel free to add your own. You can still use the toolbox by recording samples yourself. Traceback (most recent call last): File "C:\Users\sha7dow\OneDrive - longessay\文档\GitHub\MockingBird\demo_toolbox.py", line 43, in Toolbox(**vars(args)) File "C:\Users\sha7dow\OneDrive - longessay\文档\GitHub\MockingBird\toolbox__init__.py", line 76, in **init** self.setup_events() File "C:\Users\sha7dow\OneDrive - longessay\文档\GitHub\MockingBird\toolbox__init__.py", line 113, in setup_events self.ui.setup_audio_devices(Synthesizer.sample_rate) File "C:\Users\sha7dow\OneDrive - longessay\文档\GitHub\MockingBird\toolbox\ui.py", line 149, in setup_audio_devices for device in sd.query_devices(): File "C:\ProgramData\Anaconda3\envs\a\lib\site-packages\sounddevice.py", line 559, in query_devices return DeviceList(query_devices(i) File "C:\ProgramData\Anaconda3\envs\a\lib\site-packages\sounddevice.py", line 559, in return DeviceList(query_devices(i) File "C:\ProgramData\Anaconda3\envs\a\lib\site-packages\sounddevice.py", line 573, in query_devices name = name_bytes.decode('utf-8') UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 6: invalid continuation byte > Feel free to add your own. You can still use the toolbox by recording samples yourself. Traceback (most recent call last): File "C:\Users\sha7dow\OneDrive - longessay\文档\GitHub\MockingBird\demo_toolbox.py", line 43, in Toolbox(**vars(args)) File "C:\Users\sha7dow\OneDrive - longessay\文档\GitHub\MockingBird\toolbox__init__.py", line 76, in **init** self.setup_events() File "C:\Users\sha7dow\OneDrive - longessay\文档\GitHub\MockingBird\toolbox__init__.py", line 113, in setup_events self.ui.setup_audio_devices(Synthesizer.sample_rate) File "C:\Users\sha7dow\OneDrive - longessay\文档\GitHub\MockingBird\toolbox\ui.py", line 149, in setup_audio_devices for device in sd.query_devices(): File "C:\ProgramData\Anaconda3\envs\a\lib\site-packages\sounddevice.py", line 559, in query_devices return DeviceList(query_devices(i) File "C:\ProgramData\Anaconda3\envs\a\lib\site-packages\sounddevice.py", line 559, in return DeviceList(query_devices(i) File "C:\ProgramData\Anaconda3\envs\a\lib\site-packages\sounddevice.py", line 573, in query_devices name = name_bytes.decode('utf-8') UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 6: invalid continuation byte> > 查看readme的常见问题更新数据库，也改了issue37，调整了size大小，还是报错> > > Feel free to add your own. You can still use the toolbox by recording samples yourself. Traceback (most recent call last): File "C:\Users\sha7dow\OneDrive - longessay\文档\GitHub\MockingBird\demo_toolbox.py", line 43, in Toolbox(**vars(args)) File "C:\Users\sha7dow\OneDrive - longessay\文档\GitHub\MockingBird\toolbox__init__.py", line 76, in **init** self.setup_events() File "C:\Users\sha7dow\OneDrive - longessay\文档\GitHub\MockingBird\toolbox__init__.py", line 113, in setup_events self.ui.setup_audio_devices(Synthesizer.sample_rate) File "C:\Users\sha7dow\OneDrive - longessay\文档\GitHub\MockingBird\toolbox\ui.py", line 149, in setup_audio_devices for device in sd.query_devices(): File "C:\ProgramData\Anaconda3\envs\a\lib\site-packages\sounddevice.py", line 559, in query_devices return DeviceList(query_devices(i) File "C:\ProgramData\Anaconda3\envs\a\lib\site-packages\sounddevice.py", line 559, in return DeviceList(query_devices(i) File "C:\ProgramData\Anaconda3\envs\a\lib\site-packages\sounddevice.py", line 573, in query_devices name = name_bytes.decode('utf-8') UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 6: invalid continuation byte> > > > > > 查看readme的常见问题> > 已经可用了吗？有没有闪退呢，我之前启动GUI闪退了然后按照作者这个方法就可以了，链接：https://github.com/babysor/MockingBird/wiki/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF 
载入模型pretrained-11-7-21_75k.pt。报错：size mismatch for gst.stl.attention.W_query.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([512, 256]).
打开demo_toolbox后，载入模型pretrained-11-7-21_75k.pt， 点击第一次synthesize，报上述size mismatch错，再点击一次synthesize可以生成。另外对synthesizer重新训练，在载入模型时也会报size mismatch错我也是这种情况，救命@babysor 求助作者，提供的模型跑出来全是类似这个情况，咋解呀Fixed. 拉最新代码或在 synthesizer/hparams.py 最后一参数改为True俺也一样。第二次跑出来的全是杂音> Fixed. 拉最新代码或在 synthesizer/hparams.py 最后一参数改为True解决了，感谢> Fixed. 拉最新代码或在 synthesizer/hparams.py 最后一参数改为True问题解决，非常感谢 
通过python demo_toolbox.py打开程序时 出现以下报错： ImportError: DLL load failed while importing _arpack: 找不到指定的程序。
亲爱的作者你好     本人小白 在通过python demo_toolbox.py打开程序时 出现以下报错：ImportError: DLL load failed while importing _arpack: 找不到指定的程序![屏幕截图 2021-11-12 
关于英文语音的支持情况
@babysor :作者你好！我现在使用v0.001tag的版本，并对ReadMe中公开的4个预训练模型均进行了本地测试，其中除了ceshi模型可以较好地实现中文语音效果外，其他模型的中英文测试都有异常（即使参照issue解决了部分问题）。现在我更倾向于实现英文语音的模仿，请问以上提到的预训练模型只针对中文语音吗？如果是的，我该如何使用本项目做英文语音的训练？这个项目就是从英文语音项目分叉来的，你可以直接去看这个https://github.com/CorentinJ/Real-Time-Voice-Cloning 他们提供了效果很好的英文模型使用原项目提供的英文模型得到的语音是杂音，可以改进吗直接训练synthesizer喂英文语料即可。encoder、vocoder可以不变https://github.com/babysor/MockingBird/issues/440 
集成tacotron2
@babysor  
使用v0.0.1版本，并按照#37修改，使用my_run8_25k模型，还是一堆杂音，还是有size mismatch报错，要哭了。。。。
如题，求助换了pretrained-11-7-21_75k.pt，web页面点生成第一次报错size mismatch，第二次成功，但也还是杂音，另外#37的修改改之前和之后也都是杂音，> 换了pretrained-11-7-21_75k.pt，web页面点生成第一次报错size 按你这个报错，你用的明显是作者最新的模型，要用最新代码，不能按#37修改能否给一个组合，最新代码配合哪个预训练模型可以跑通呢？最新的我用默认给的前两个模型也会出问题最新代码无修改情况下配合我（作者）的75k， 
训练时出错：RuntimeError: Error(s) in loading state_dict for Tacotron:
Arguments:    run_id:          mandarin    syn_dir:         k:/mockingbird/datame/SV2TTS/synthesizer    models_dir:      synthesizer/saved_models/    save_every:      1000    backup_every:    25000    log_every:       200    force_restart:   False    hparams:Checkpoint path: synthesizer\saved_models\mandarin\mandarin.ptLoading training data from: k:\mockingbird\datame\SV2TTS\synthesizer\train.txtUsing model: TacotronUsing device: cpuInitialising Tacotron Model...Trainable Parameters: 32.866MLoading weights at synthesizer\saved_models\mandarin\mandarin.ptTraceback (most recent call last):  File "synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "K:\MockingBird\synthesizer\train.py", line 114, in train    model.load(weights_fpath, optimizer)  File "K:\MockingBird\synthesizer\models\tacotron.py", line 536, in load    self.load_state_dict(checkpoint["model_state"], strict=False)  File "f:\anaconda3\envs\mockingbird\lib\site-packages\torch\nn\modules\module.py", line 1482, in load_state_dict    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]).        size mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([384, 1280]).        size mismatch for decoder.rnn_input.weight: copying a param with shape torch.Size([1024, 640]) from checkpoint, the shape in current model is torch.Size([1024, 1152]).        size mismatch for decoder.stop_proj.weight: copying a param with shape torch.Size([1, 1536]) from checkpoint, the shape in current model is torch.Size([1, 2048]).我已经把symbol里的那行字符改成旧版的那个了，还是报这个错。我这里用的是自己的数据，模仿aishell3的结构放了，已经做了 pre.py 的预处理，在开始训练这一步的时候就出了这个错我换成 my_run8_25k.pt 这个模型来训练就可以了，readme里用的mandarin.pt就不行另外他每一步都出现这个警告是什么意思呢？{| Epoch: 25/10000 (1/1) | Loss: 4.027 | 0.15 steps/s | Step: 0k | }K:\MockingBird\synthesizer\synthesizer_dataset.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\torch\csrc\utils\tensor_new.cpp:201.)  embeds = torch.tensor(embeds)> 我换成 my_run8_25k.pt 这个模型来训练就可以了，readme里用的mandarin.pt就不行> > 另外他每一步都出现这个警告是什么意思呢？ {| Epoch: (1/1) | Loss: 4.027 | 0.15 steps/s | Step: 0k | } K:\MockingBird\synthesizer\synthesizer_dataset.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\torch\csrc\utils\tensor_new.cpp:201.) embeds = torch.tensor(embeds)参考 #209 
服务器训练模型在本地继续训练后，报错
在服务器上训练的magicdata BS 84，训练到110K,。模型下载到本地，BS改为12，继续训练后，报错代码如下：服务器训练采用的版本日期是10.24日。F:\MockingBird\synthesizer\synthesizer_dataset.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\torch\csrc\utils\tensor_new.cpp:201.)  embeds = torch.tensor(embeds)F:\MockingBird\synthesizer\synthesizer_dataset.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\torch\csrc\utils\tensor_new.cpp:201.)  embeds = torch.tensor(embeds)C:\Users\ferret\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")Traceback (most recent call last):  File "F:\MockingBird\synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "F:\MockingBird\synthesizer\train.py", line 208, in train    optimizer.step()  File "C:\Users\ferret\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\optimizer.py", line 88, in wrapper    return func(*args, **kwargs)  File "C:\Users\ferret\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\autograd\grad_mode.py", line 28, in decorate_context    return func(*args, **kwargs)  File "C:\Users\ferret\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\adam.py", line 133, in step    F.adam(params_with_grad,  File "C:\Users\ferret\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\_functional.py", line 86, in adam    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)RuntimeError: The size of tensor a (1024) must match the size of tensor b (3) at non-singleton dimension 3本地的commit日期是？试过切换到10-24号的吗> 本地的commit日期是？试过切换到10-24号的吗切换到24号的版本后正常了，想知道这个没有跟上最新版本，会影响训练效果吗？影响比较小 当然建议如果效果一般的话尝试一下新版本> 影响比较小 当然建议如果效果一般的话尝试一下新版本好的非常感谢。能回答这么多人的这么多问题，难为你了> > 影响比较小 当然建议如果效果一般的话尝试一下新版本> > 好的非常感谢。能回答这么多人的这么多问题，难为你了需要像你一样的新兴力量加盟 
模型兼容问题加强 Compatibility Enhance of Pretrained Models and code base
由于最近调试模型代码，经常导致原模型报错，大家除了可以把代码切换到旧的commit上使用之外（参考readme中相关模型的简介），我也注意到了这个问题带来的困扰，于是将在之后的调试中加入一些flag机制尽可能保证兼容性。现在如果想用最新的代码，并使用社区已经分享的cechi、mandarin等预训练模型，可以将文件  中的:  均设置为False，部分在2021年10月10至2021年10月25分享的模型，仅需将  设置为False，即可正常使用。并没有找到上述提到的两项代码在哪完全小白，摸索切换到0.01版本用了两天现在还是没法跑训练，仍然和#37报一样的错我的代码已经按#37所修改> 并没有找到上述提到的两项代码在哪 完全小白，摸索切换到0.01版本用了两天 现在还是没法跑训练，仍然和#37报一样的错 我的代码已经按#37所修改拉取最新的代码才有> > 并没有找到上述提到的两项代码在哪 完全小白，摸索切换到0.01版本用了两天 现在还是没法跑训练，仍然和#37报一样的错 我的代码已经按#37所修改> > 拉取最新的代码才有我这边使用了最新的代码，而且也改了hparams的两个参数，我这边使用的是ceshi.pt。大佬求指出哪里有问题？<img width="685" alt="微信截图_20211208164352" src="https://user-images.githubusercontent.com/4926619/145177036-8440661c-1630-4f9f-adbb-5a89805974bd.png">https://github.com/babysor/MockingBird/issues/37#issuecomment-903457759用这个方法可以跑通程序但是出来的都是杂音。> > > 并没有找到上述提到的两项代码在哪 完全小白，摸索切换到0.01版本用了两天 现在还是没法跑训练，仍然和#37报一样的错 我的代码已经按#37所修改> > > > > > 拉取最新的代码才有> > 我这边使用了最新的代码，而且也改了hparams的两个参数，我这边使用的是ceshi.pt。大佬求指出哪里有问题？ <img alt="微信截图_20211208164352" width="685" src="https://user-images.githubusercontent.com/4926619/145177036-8440661c-1630-4f9f-adbb-5a89805974bd.png">同问  最新的代码只能用作者分享的75k的模型#245 最新版的代码这个模型也可以用 其他都会报错实测可用，感谢！使用cpu运算已使用的解决方案：改字符集本文加--cpu正解！修改这两个参数后解决问题> 并没有找到上述提到的两项代码在哪 完全小白，摸索切换到0.01版本用了两天 现在还是没法跑训练，仍然和#37报一样的错 我的代码已经按#37所修改兄弟，我也是纯小白，请教一下怎么切换到tag0.01根据大佬的提示，修改两处代码，跑通了且声音正常。第一处代码是以将文件 synthesizer\hparams.py 中的: use_gst use_ser_for_gst 均设置为False，部分在2021年10月10至2021年10月25分享的模型，仅需将 use_ser_for_gst 设置为False，即可正常使用。   第二处代码是#37里面的那部分。即改为_characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz12340!\'(),-.:;? '确实是这个问题，已经解决！ 
事小白，用ceshi不知道为什么出现这样的问题，请教一下大佬
#37 #209 @babysor 在hparams.py里找不到 use_ser_for_gst和use_gst，以及根据readme中所写该如何切换到tag0.0.1> @babysor 在hparams.py里找不到 use_ser_for_gst和use_gst，以及根据readme中所写该如何切换到tag0.0.1拉取最新代码 运行git pull 或者在github桌面端使用拉取 
有没有 tts 文本前端模块呀?
有没有比较完备的中文 tts 文本前端模块能描述一下你的场景吗？ 
使用训练库时出现问题
ERROR in app: Exception on /api/synthesize [POST]Traceback (most recent call last):  File "d:\Anaconda3\lib\site-packages\flask\app.py", line 2447, in wsgi_app    response = self.full_dispatch_request()  File "d:\Anaconda3\lib\site-packages\flask\app.py", line 1952, in full_dispatch_request    rv = self.handle_user_exception(e)  File "d:\Anaconda3\lib\site-packages\flask_restx\api.py", line 672, in error_router    return original_handler(e)  File "d:\Anaconda3\lib\site-packages\flask\app.py", line 1821, in handle_user_exception    reraise(exc_type, exc_value, tb)  File "d:\Anaconda3\lib\site-packages\flask\_compat.py", line 39, in reraise    raise value  File "d:\Anaconda3\lib\site-packages\flask\app.py", line 1950, in full_dispatch_request    rv = self.dispatch_request()  File "d:\Anaconda3\lib\site-packages\flask\app.py", line 1936, in dispatch_request    return  File "D:\声音模仿\web\__init__.py", line 111, in synthesize    wav = rnn_vocoder.infer_waveform(spec)  File "D:\声音模仿\vocoder\wavernn\inference.py", line 63, in infer_waveform    wav = _model.generate(mel, batched, target, overlap, hp.mu_law, progress_callback)  File "D:\声音模仿\vocoder\wavernn\models\fatchord_version.py", line 253, in generate    output[-20 * self.hop_length:] *= fade_outValueError: operands could not be broadcast together with shapes (2400,) (4000,) (2400,)请问怎么解决输入得音频格式可能又点问题我，你有试过直接录音吗谢谢解答，有时间我会尝试并反馈 
无法克隆除自带的之外的字，日志会出现循环
![T%@ CPRK 
使用新語料 toolbox inference錯誤
最近使用了一個新的語料庫去訓練synthesizer(沒有使用作者提供的語料)，訓練格式都沒有改。但是將模型拿去toolbox做inference的時候遇到下列的錯誤。 可能运行的代码跟训练的版本有点不同导致的 
修复synthesizer/models/tacotron.Encoder注释错误
fix Issue #202 
synthesizer/models/tacotron.Encoder注释错误
扩展后大小为 而不是 执行 后 的大小为 
声音合成出现电子音
RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).        size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]).        size mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([384, 1280]).        size mismatch for decoder.rnn_input.weight: copying a param with shape torch.Size([1024, 640]) from checkpoint, the shape in current model is torch.Size([1024, 1152]).        size mismatch for decoder.stop_proj.weight: copying a param with shape torch.Size([1, 1536]) from checkpoint, the shape in current model is torch.Size([1, 2048]).  然后按 #37 处理一下 记得以后切换其他模型的时候要恢复回来 
合成失败
麻烦请问一下，合成播放一次例句后，修改文字再合成就出现“Exception：Sizes of tensors must match except in dimension 1.Expected size 2but got size 1 for tensor number in the list ”怎么办？ 
今天开启了四库训练，报错。
感谢作者加了新的datasheet。我开启了四库混训，一开头就出警告，不知何种原因。C:\Users\wy\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")这个警告可以忽略，是pytorch升级问题，不过试着改一下可能性能变好大佬，四库训练出来效果如何> 大佬，四库训练出来效果如何泛型强，但是效果感觉不如一个的 
Hindi Version REQUIRED
Can also support language like Hindi by the use of Inltk. 
按照quick start 构建，下载训练好的模型，为啥输出一直是杂音？
刚开始弄，下载社区训练好的模型，全是杂音啊。mel spectrogram明显不对ceshi的模型需要将代码切换到10月20号左右的commit之后，再按issue #37 修改之后就可以用了而作者的模型，需要将代码切换到10月20号左右的commit之后> ceshi的模型需要将代码切换到10月20号左右的commit之后，再按issue #37 修改之后就可以用了 而作者的模型，需要将代码切换到10月20号左右的commit之后好的，我试一下。“ceshi的模型需要将代码切换到10月20号左右的commit之后，再按issue #37 修改之后就可以用了，而作者的模型，需要将代码切换到10月20号左右的commit之后”，你这是写错了吧？都是要10月20日之后的？还是有一个打错字了？> “ceshi的模型需要将代码切换到10月20号左右的commit之后，再按issue #37 修改之后就可以用了，而作者的模型，需要将代码切换到10月20号左右的commit之后”，你这是写错了吧？都是要10月20日之后的？还是有一个打错字了？语文差。。是切换到那个时间左右的commit，之后/再使用就可以了 
ModuleNotFoundError: No module named 'tqdm.auto'
PS C:\MockingBird> python demo_toolbox.py -d .\samples键入命令后报错Traceback (most recent call last):  File "demo_toolbox.py", line 2, in <module>    from toolbox import Toolbox  File "C:\MockingBird\toolbox\__init__.py", line 1, in <module>    from toolbox.ui import UI  File "C:\MockingBird\toolbox\ui.py", line 16, in <module>    import umap  File "C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py", line 2, in <module>    from .umap_ import UMAP  File "C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py", line 41, in <module>    from umap.layouts import (  File "C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py", line 5, in <module>    from tqdm.auto import tqdmModuleNotFoundError: No module named 'tqdm.auto'这是什么问题啊tqdm 版本好像有问题，你的环境有跟其他项目共用吗没有其他项目 之前我并没有过python的使用经验我把Anacodna卸载了 直接运行 现在报错发生了变化正在努力解决新的报错 遇到我无法解决的报错会再发出来再次安装必须库后已经可以正常运行了 应该是我在安装Anacodna和配置环境的时候哪里出了问题> 已经可以正常运行了 应该是我在安装Anacodna和配置环境的时候哪里出了问题环境 是最大的坎坷 
Update tacotron.py
change the logic of GST inference 
ImportError: Failed to import any qt binding
Traceback (most recent call last):  File "demo_toolbox.py", line 2, in <module>    from toolbox import Toolbox  File "/Users/Henry/Downloads/MockingBird-main/toolbox/__init__.py", line 1, in <module>    from toolbox.ui import UI  File "/Users/Henry/Downloads/MockingBird-main/toolbox/ui.py", line 2, in <module>    from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas  File "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backends/backend_qt5agg.py", line 11, in <module>    from .backend_qt5 import (  File "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backends/backend_qt5.py", line 16, in <module>    import matplotlib.backends.qt_editor.figureoptions as figureoptions  File "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backends/qt_editor/figureoptions.py", line 11, in <module>    from matplotlib.backends.qt_compat import QtGui  File "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backends/qt_compat.py", line 175, in <module>    raise ImportError("Failed to import any qt binding")ImportError: Failed to import any qt binding 
能否出个方便标注的批处理程序或者脚本
magicdata虽然标注很简单，但貌似很难出效果。aishell3实测效果还可以，但是aishell3和aidatatang的标注有点太繁琐了。想问作者能否出个简易的标注脚本之类的，可以的话就万分感激了。 
为啥不用numpy.random.choice

中断合成进程后再重新启动
请问，中断了synthesizer_train后，重新执行，或者是换个数据集再合成，名字都相同，是不是都会在原有数据上继续合成？是的 
toolbox运行时报错：Model files not found. Please download the models
使用的是Windows11 系统预先训练好的模型已经按照要求放在了 synthesizer/saved_models 下![屏幕截图 2021-11-03 shell运行时会报错![屏幕截图 2021-11-03 要在项目根目录下运行toolbox解决了，谢谢大佬🤭怎么解决的？我也是一模一样的报错。“要在项目根目录下运行toolbox”是怎么操作？> 怎么解决的？我也是一模一样的报错。“要在项目根目录下运行toolbox”是怎么操作？就是以项目根目录作为power shell 或者cmd的工作目录，在项目根目录内右键，选择“在power shell中打开”或者使用cd命令切换工作目录 
mockingbird的web界面的vocoder怎么换啊
我在web/.__init__.py里修改调用的模型，终端不报错，但是web界面中的音频无法播放，是灰色的无法点击。到底是什么出了问题啊？![$~QR1WD07K@{UPBHWT@_4 更换完就好了吗换了这两行注释运行之后就是web界面无法播放 
请教下mocking bird的头像怎么获得的？
即“大脑+鸟”那个图标感觉非常不错，是代码生成的嘛？盲猜是作者找组里的美工做的 花费应该在5杯奶茶之内miro 一分钱不花> miro 一分钱不花miro.com? 自己手动做的嘛？> > miro 一分钱不花> > miro.com? 自己手动做的嘛？嗯，折腾 
合成失败  size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).
File "G:\Labs\MockingBird\.venv385\lib\site-packages\torch\nn\modules\module.py", line 1051, in load_state_dict    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).        size mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 1280]) from checkpoint, the shape in current model is torch.Size([384, 768]).        size mismatch for decoder.rnn_input.weight: copying a param with shape torch.Size([1024, 1152]) from checkpoint, the shape in current model is torch.Size([1024, 640]).        size mismatch for decoder.stop_proj.weight: copying a param with shape torch.Size([1, 2048]) from checkpoint, the shape in current model is torch.Size([1, 1536]).我用的是 目前git的版本，模式加载的是 my_run8_25k.pt在合成的时候报以上错误，请问是什么原因？跟torch的版本有关系吗？我的系统是win10 gtx1070 cuda的版本是10.1 torch的版本是1.7.1+cu101没有关系，是模型比较老不兼容，可以切换代码到10-22号之前分支 
重新进行合成器训练，发现一些数字，有影响吗？
{| Epoch: 1/1 (9500/51223) | Loss: 0.5770 | 0.61 steps/s | Step: 9k | }Input at step 9500: 52311 bei3 jing1 ya4 ding1 wan1 shang1 wu4 jiu3 dian4~__________________________________________________________________________{| Epoch: 1/1 (10000/51223) | Loss: 0.5880 | 0.67 steps/s | Step: 10k | }Input at step 10000: 52554 shi1 mian2 la5 ji2 xu1 ji3 shou3 ai1 shang1 de5 liu2 xing2 ge1 qu3 bai4 tuo1 le5~___拼音前的52311是啥意思？是不是数据集有问题？对训练结果有无影响？这个应该是magicdata预处理有个bug，现在应该是已经修复了，重新跑预处理就好。不过这个对最终效果影响不大。 
训练到640K时，就自动退出，不能再训练了。
一开始是用默认库训练，大概训练到350K左右。后来觉得效果不好，把另外两个库Aishell和magicdata也预处理了，使用三个库进行训练。训练到640K，就不能继续了，自动退出了。重新启动训练也是如此。在hparameter改batch size的地方，调高 schedule 請問我這樣修改對嘛? 訓練完還是一堆雜音，語料大概161小時。204個語者。有沒有辦法預估這樣的語料量需要訓練幾個step?> > 
用colab和谷歌云服务器预处理时，出现了AssertionError
~/MockingBird$ python pre.py dateAISHELL3 -d aishell3Traceback (most recent call last):  File "pre.py", line 57, in <module>    assert args.datasets_root.exists()AssertionErrorcolab和谷歌云都出现了相同的错误，版本都是用git clone 抓取的最新版路径错误，已经解决> 大佬是什么路径问题啊，怎么解决了我预处理也遇到这个AssertionError的问题了 
训练vcoder时选择hifigan能看到一直在输出 但是没有保存新的pt文件

训练vcoder时出现问题
如果命令中不带上vcoder_type这个参数 
合成音频像是电子音
使用my_run8_25k.pt模型生成音频，但合成后的音频听起来就像是电子音看到运行时有报错说：UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.不知道会不会和这个有关系已尝试把代码中所有提到nn.functional.tanh替换为torcht.tanh代替，仍有以上报错下附完整运行日志： 
operands could not be broadcast together with shapes
Very strange error, have anyone met this problem (非常奇怪的问题，有人知道怎么解决吗):  非常少见，看起来像是输入音频比较特别 触发的问题 
写了一篇MockingBird的体验博客
第一次见到MockingBird克隆语音黑科技只能用“惊艳”二字来形容，感谢作者做出支持中文语音，搭建了环境写了一篇体验博客，希望对大家有帮助实时中文语音克隆 —— 第一次见到MockingBird克隆语音黑科技只能用“惊艳”二字来形容，感谢作者做出支持中文语音，搭建了环境写了一篇体验博客，希望对大家有帮助 实时中文语音克隆 —— 开源项目MockingBird体验 手机版：https://mp.weixin.qq.com/s/Gzxm5wyzPPsebv5BQh5EPw PC版：https://security.tencent.com/index.php/blog/msg/204大佬写的很详细，看完受益匪浅，谢谢。同时也意外发现了，这个博客的访问量，是按点击次数来计算的，这样会不会有点不太客观，能否增加一个IP检测，即单IP只计算一次访问量。 
报错 TwT
RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]).        size mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([384, 1280]).        size mismatch for decoder.rnn_input.weight: copying a param with shape torch.Size([1024, 640]) from checkpoint, the shape in current model is torch.Size([1024, 1152]).        size mismatch for decoder.stop_proj.weight: copying a param with shape torch.Size([1, 1536]) from checkpoint, the shape in current model is torch.Size([1, 2048]).使用的老版本models，需要切换到10-22之前的代码分支或参考 #209 
FileNotFoundError: [Errno 2] No such file or directory: 'encoder\\saved_models\\pretrained.pt'
我把已经下载好的模型，放到了文件D:\声音克隆\MockingBird-main\synthesizer\saved_models下并且还在D:\声音克隆\MockingBird-main\encoder\saved_models里也放了一个把模型my_run,py改名为pretrained.pt的文件然后运行web.py文件(base) C:\Users\13549>python D:\声音克隆\MockingBird-main\web.pyLoaded synthesizer models: 0Traceback (most recent call last):  File "D:\声音克隆\MockingBird-main\web.py", line 6, in <module>    app = webApp()  File "D:\声音克隆\MockingBird-main\web\__init__.py", line 33, in webApp    encoder.load_model(Path("encoder/saved_models/pretrained.pt"))  File "D:\声音克隆\MockingBird-main\encoder\inference.py", line 33, in load_model    checkpoint = torch.load(weights_fpath, _device)  File "D:\anaconda\lib\site-packages\torch\serialization.py", line 525, in load    with _open_file_like(f, 'rb') as opened_file:  File "D:\anaconda\lib\site-packages\torch\serialization.py", line 212, in _open_file_like    return _open_file(name_or_buffer, mode)  File "D:\anaconda\lib\site-packages\torch\serialization.py", line 193, in __init__    super(_open_file, self).__init__(open(name, mode))FileNotFoundError: [Errno 2] No such file or directory: 'encoder\\saved_models\\pretrained.pt'请问一下该怎么办呢，即使我把模型文件名修改成pretrained仍然会报同样的错误目录里不是没有pretrained.pt 文件吗？qinan-nlx ***@***.***>于2021年10月26日 周二下午5:52写道：> [image: ***@***.***(G]> <https://user-images.githubusercontent.com/62303408/138854687-0f808f57-9935-4364-a855-843b82c97144.png>>> —> You are receiving this because you are subscribed to this thread.> Reply to this email directly, view it on GitHub> <https://github.com/babysor/MockingBird/issues/175#issuecomment-951772703>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/ABJKKBGWWQCM4S65JIFLYJLUIZ265ANCNFSM5GXJHCPA>> .>-- -- Michael Lin Sent from mobile phone> 目录里不是没有pretrained.pt 文件吗？ qinan-nlx ***@***.***>于2021年10月26日 周二下午5:52写道：> [image: ***@***.***(G] <https://user-images.githubusercontent.com/62303408/138854687-0f808f57-9935-4364-a855-843b82c97144.png> — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <[#175 or unsubscribe <https://github.com/notifications/unsubscribe-auth/ABJKKBGWWQCM4S65JIFLYJLUIZ265ANCNFSM5GXJHCPA> .> -- -- Michael Lin Sent from mobile phone不行呀，我把pretrained.pt文件放进去也还是这样，是我目录错了吗> 目录里不是没有pretrained.pt 文件吗？ qinan-nlx ***@***.***>于2021年10月26日 周二下午5:52写道：> [image: ***@***.***(G] <https://user-images.githubusercontent.com/62303408/138854687-0f808f57-9935-4364-a855-843b82c97144.png> — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <[#175 or unsubscribe <https://github.com/notifications/unsubscribe-auth/ABJKKBGWWQCM4S65JIFLYJLUIZ265ANCNFSM5GXJHCPA> .> -- -- Michael Lin Sent from mobile phone![L01ZMF 5UFO()F{ }%H}3 目录路径中建议不要出现中文汉字的名称“声音文件”，再试试看。 D:\声音克隆\MockingBird-main\synthesizer\saved_models 改为： D:\voice_clone\MockingBird-main\synthesizer\saved_models还是不行呀  和 路径不要搞混了，models是不能互用的>  和 路径不要搞混了，models是不能互用的噢非常感谢，我还想请问一下那个Loaded synthesizer models: 0而且ModuleNotFoundError: No module named > 在项目的根目录下运行， 噢好的非常感谢，已解决啦 
预处理数据出错
(base) C:\Users\13549>python D:\声音克隆\MockingBird-main\pre.py datasets_rootD:\声音克隆\MockingBird-main\encoder\audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.  warn("Unable to import 'webrtcvad'. This package enables noise removal and is recommended.")Traceback (most recent call last):  File "D:\声音克隆\MockingBird-main\pre.py", line 57, in <module>    assert args.datasets_root.exists()AssertionError(base) C:\Users\13549>pip install webrtcvadLooking in indexes: webrtcvad  Using cached (66 kB)Building wheels for collected packages: webrtcvad  Building wheel for webrtcvad (setup.py) ... error ERROR: Command errored out with exit status 1:   command: 'D:\anaconda\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '"'"'C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\pip-install-0cq85qvw\\webrtcvad\\setup.py'"'"'; __file__='"'"'C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\pip-install-0cq85qvw\\webrtcvad\\setup.py'"'"';f=getattr(tokenize, '"'"'open'"'"', open)(__file__);code=f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' bdist_wheel -d 'C:\Users\Public\Documents\Wondershare\CreatorTemp\pip-wheel-c2o8r6hs'       cwd: C:\Users\Public\Documents\Wondershare\CreatorTemp\pip-install-0cq85qvw\webrtcvad\  Complete output (9 lines):  running bdist_wheel  running build  running build_py  creating build  creating build\lib.win-amd64-3.8  copying webrtcvad.py -> build\lib.win-amd64-3.8  running build_ext  building '_webrtcvad' extension  error: Microsoft Visual C++ 14.0 or greater is required. Get it with "Microsoft C++ Build Tools":  ----------------------------------------  ERROR: Failed building wheel for webrtcvad  Running setup.py clean for webrtcvadFailed to build webrtcvadInstalling collected packages: webrtcvad    Running setup.py install for webrtcvad ... error    ERROR: Command errored out with exit status 1:     command: 'D:\anaconda\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '"'"'C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\pip-install-0cq85qvw\\webrtcvad\\setup.py'"'"'; __file__='"'"'C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\pip-install-0cq85qvw\\webrtcvad\\setup.py'"'"';f=getattr(tokenize, '"'"'open'"'"', open)(__file__);code=f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' install --record 'C:\Users\Public\Documents\Wondershare\CreatorTemp\pip-record-r07zesjt\install-record.txt' --single-version-externally-managed --compile --install-headers 'D:\anaconda\Include\webrtcvad'         cwd: C:\Users\Public\Documents\Wondershare\CreatorTemp\pip-install-0cq85qvw\webrtcvad\    Complete output (9 lines):    running install    running build    running build_py    creating build    creating build\lib.win-amd64-3.8    copying webrtcvad.py -> build\lib.win-amd64-3.8    running build_ext    building '_webrtcvad' extension    error: Microsoft Visual C++ 14.0 or greater is required. Get it with "Microsoft C++ Build Tools":    ----------------------------------------ERROR: Command errored out with exit status 1: 'D:\anaconda\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '"'"'C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\pip-install-0cq85qvw\\webrtcvad\\setup.py'"'"'; __file__='"'"'C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\pip-install-0cq85qvw\\webrtcvad\\setup.py'"'"';f=getattr(tokenize, '"'"'open'"'"', open)(__file__);code=f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' install --record 'C:\Users\Public\Documents\Wondershare\CreatorTemp\pip-record-r07zesjt\install-record.txt' --single-version-externally-managed --compile --install-headers 'D:\anaconda\Include\webrtcvad' Check the logs for full command output. 刚才我也出现了 发现路径错了 
声音就像电钻
ValueError: Axis limits cannot be NaN or Inf+1ValueError: Axis limits cannot be NaN or Inf+1参考 链接：https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw 提取码：om7f --来自百度网盘超级会员V3的分享> 参考 并使用模型 链接：https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw 提取码：om7f --来自百度网盘超级会员V3的分享这个模型是放在哪里？> > 参考 并使用模型 链接：https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw 提取码：om7f --来自百度网盘超级会员V3的分享> > 这个模型是放在哪里？  目录下ValueError: Axis limits cannot be NaN or Infcheck this similar @chenbo666-a @jiashihuigithub 
模仿对象的音频要如何处理？
没看懂readme.md，模仿对象是要做成数据集吗？如果是的话从哪里能看到制作方法，如果不是的话要如何做呢？不需要，只需要清晰无背景噪音的3-10秒说话声即可长语句模仿对象会截断后面的内容。如何解决。如：本次录音是用于参与科大讯飞自训练音库进行录制。从”自训练音库进行录制“开始就听不到了。> 长语句模仿对象会截断后面的内容。如何解决。 如：本次录音是用于参与科大讯飞自训练音库进行录制。> > 从”自训练音库进行录制“开始就听不到了。已经修复了，可以试一下 
获取模型，每次都重新开始，-s没有效果
pre.py T:\MockingBird\data -s   每次还是从0开始，一个个重新运行 -s没有效果，断了就又又得一整天不同的软件版本，相同的数据集和输出文件夹，也得重新从0开始执行这个命令是预处理，因为时间比较短所以没有断点续传设计，你这边只需成功运行一次就好，你用的是colab？这个是提取数据集到SV2TTS，主要是对成功的数据做个日志记录吧，我的配置要大半天，不过注意点别断了，问题也不大，colab我不知道是啥，嘿嘿 
每次生成一条新语音后，都需要关掉软件，重新运行demo_toolbox，无法连续操作
我用的是macOS，每次生成了一条新语音后，再输入新的文字，或者加载别的语音素材，都提示错误    raise ValueError("Axis limits cannot be NaN or Inf")ValueError: Axis limits cannot be NaN or Inf重新运行 python demo_toolbox.py -d .\samples，就又可以操作了，但还是只能一次。请问是我操作上出现了问题吗？还有，我下载了提供的训练模型，只有ceshi这个可以正常使用，其他都是输出沙沙声，这个是需要调整吗？另外，可以再发一次微信群的二维码吗？好多问题想问问解决了，我把图形界面拉长一点，就可以了 
訓練hifigan聲碼器報錯
是不是用的预处理代码有bug？我看你这个是自己修改的处理代码？之前有發生無法正常寫入文本的bug，不過已經排除，查看文本內容也是正常的文本。這邊處理的方式只有加上如圖片中的  其餘都是原本的代碼@babysor  訓練hifigan的代碼: 运行过程有error或warning吗> 
按照#37的修改，还是一直出现杂音
按步骤准备好环境启动工具箱后，一切默认，上传目录下的temp.wav。点击 Sythesize and vcode后，第一次报跟 #37 一样的错，直接忽略，再次点击 Sythesize and vcode后，又没报错了，这时生成的是杂音。已经按照 #37 的改法修改了 解决了吗？me,too你可能是更新到10/23之后的版本了.建议退回去试试. @klaylizhe> 你可能是更新到10/23之后的版本了.建议退回去试试. @klaylizhe感谢 问题解决了退回去10/23的版本，还是出现同样的杂音使用v0.0.1版本，并按照#37修改，使用my_run8_25k模型，还是一堆杂音，还是有size mismatch报错，要哭了。。。。> 你可能是更新到10/23之后的版本了.建议退回去试试. @klaylizhe您好，方便详细说一下如何回退requirement的版本吗，本人刚刚接触，不是太明白，望详细说明，谢谢 
CondaValueError: could not parse 'webrtcvad; platform_system != "Windows"' in: requirements.txt
When I installed packages by requirement.txt, I got this following error: What command did you run? pip install?I use .这里的语法有platform依赖，用于pip运行，你可以去掉 requirement文件里的所有 
来贡献模型了，aishell3的
，提取码：7777aishell3数据集，Tesla V100 32G，BS 96训练的160K，loss值0.24同时本人有两台V100 32G闲置，为BUG时撸的云服务器，有想训练啥的也可以提要求，反正闲着也是闲着。更新到最新版(2021/10/23)之后.合成报错了 > 更新到最新版(2021/10/23)之后.利德 · 利德> > 我是用10.12日发布的版本训练的，最新版本修改了什么导致模型无法复用，你得去问作者注意：如果用最新的commit，会无法使用以上模型，最新commit我做了一个较大改动，在想办法兼容中。谢谢分享> ，提取码：7777> > aishell3数据集，Tesla V100 32G，BS 96训练的160K，loss值0.24> > 同时本人有两台V100 32G闲置，为BUG时撸的云服务器，有想训练啥的也可以提要求，反正闲着也是闲着。新版本兼容性有问题，等待作者修复，现在要用这个模型请用我分享的旧版本。链接：https://pan.baidu.com/s/14UweWwENPc0myDLezy8L3Q 提取码：7777 感谢分享。但是，以下语句：本次录音是用于参与科大讯飞自训练音库进行录制。无法完整的播放出来。后面的“自训练音库进行录制。”截断了，听不到声音。后台是正确的：Read ['本次录音是用于参与科大讯飞自训练音库进行录制']Synthesizing ['ben3 ci4 lu4 yin1 shi4 yong4 yu2 can1 yu4 ke1 da4 xun4 fei1 zi4 xun4 lian4 yin1 ku4 jin4 xing2 lu4 zhi4']> 感谢分享。 但是，以下语句： 本次录音是用于参与科大讯飞自训练音库进行录制。 无法完整的播放出来。后面的“自训练音库进行录制。”截断了，听不到声音。> > 后台是正确的： Read ['本次录音是用于参与科大讯飞自训练音库进行录制'] Synthesizing ['ben3 ci4 lu4 yin1 shi4 yong4 yu2 can1 yu4 ke1 da4 xun4 fei1 zi4 xun4 lian4 yin1 ku4 jin4 xing2 lu4 zhi4']https://zhuanlan.zhihu.com/p/425692267 里面提到> ，提取码：7777> > aishell3数据集，Tesla V100 32G，BS 96训练的160K，loss值0.24> > 同时本人有两台V100 32G闲置，为BUG时撸的云服务器，有想训练啥的也可以提要求，反正闲着也是闲着。纯小白。抱歉打扰了，请问这个文件要怎么使用啊？我按照新手友好版教程把@miven的synthesizer的xxx.pt文件下载下来并放到了github项目文件的synthesizer-save_models里面，但结果无一例外合成音都是刺耳电流音。本以为是没有下载数据集，但是当我把aidatatang_200zh近18G的文件下载下来后，依然还是电流声。我的encoder和vocoder是pretrained，synthesizer是选ceshi或者pretrained。因为是按照新手教程去做的所以没有训练，而是直接使用训练好的模型。这个问题困扰了我很久，如果能解决的话这将是我第一个成功运行的github项目。谢谢。> > > aishell3数据集，特斯拉V100 32G，BS 96训练的160K，损失值0.24> > 同时本人有两台V100 32G闲置，为BUG时撸的云服务器，有想练啥的也可以提要求，休息一下也是闲着。> > 新版本有问题，作者修复，现在要用模型这个请用我分享的旧版本。 : //pan.baidu.com/s/14UweWwENPc0myDLezy8L3Q提取码：7777抱歉，打扰了，现在才看到这个，已经解决问题了。十分感谢分享，这对我意义重大！再次感谢> > > > > aishell3数据集，特斯拉V100 32G，BS 96训练的160K，损失值0.24> > > 同时本人有两台V100 32G闲置，为BUG时撸的云服务器，有想练啥的也可以提要求，休息一下也是闲着。> > > > > > 新版本有问题，作者修复，现在要用模型这个请用我分享的旧版本。 : //pan.baidu.com/s/14UweWwENPc0myDLezy8L3Q提取码：7777> > 抱歉，打扰了，现在才看到这个，已经解决问题了。十分感谢分享，这对我意义重大！再次感谢看到你成功运行我也很开心，自己运行第一个项目能够成功确实是值得庆幸的事情！这个模型效果不错。> > ，提取码：7777> > aishell3数据集，Tesla V100 32G，BS 96训练的160K，loss值0.24> > 同时本人有两台V100 32G闲置，为BUG时撸的云服务器，有想训练啥的也可以提要求，反正闲着也是闲着。> > 新版本兼容性有问题，等待作者修复，现在要用这个模型请用我分享的旧版本。 链接：https://pan.baidu.com/s/14UweWwENPc0myDLezy8L3Q 提取码：7777这个链接失效了，可以劳烦再发一次嘛> > > ，提取码：7777> > > aishell3数据集，Tesla V100 32G，BS 96训练的160K，loss值0.24> > > 同时本人有两台V100 32G闲置，为BUG时撸的云服务器，有想训练啥的也可以提要求，反正闲着也是闲着。> > > > > > 新版本兼容性有问题，等待作者修复，现在要用这个模型请用我分享的旧版本。 链接：https://pan.baidu.com/s/14UweWwENPc0myDLezy8L3Q 提取码：7777> > 这个链接失效了，可以劳烦再发一次嘛无需使用这个了，在本项目主页的tag当中，有作者发布的旧版本压缩包> > > > ，提取码：7777> > > > aishell3数据集，Tesla V100 32G，BS 96训练的160K，loss值0.24> > > > 同时本人有两台V100 32G闲置，为BUG时撸的云服务器，有想训练啥的也可以提要求，反正闲着也是闲着。> > > > > > > > > 新版本兼容性有问题，等待作者修复，现在要用这个模型请用我分享的旧版本。 链接：https://pan.baidu.com/s/14UweWwENPc0myDLezy8L3Q 提取码：7777> > > > > > 这个链接失效了，可以劳烦再发一次嘛> > 无需使用这个了，在本项目主页的tag当中，有作者发布的旧版本压缩包好的，非常感谢！> > > > ，提取码：7777> > > > aishell3数据集，Tesla V100 32G，BS 96训练的160K，loss值0.24> > > > 同时本人有两台V100 32G闲置，为BUG时撸的云服务器，有想训练啥的也可以提要求，反正闲着也是闲着。> > > > > > > > > 新版本兼容性有问题，等待作者修复，现在要用这个模型请用我分享的旧版本。 链接：https://pan.baidu.com/s/14UweWwENPc0myDLezy8L3Q 提取码：7777> > > > > > 这个链接失效了，可以劳烦再发一次嘛> > 无需使用这个了，在本项目主页的tag当中，有作者发布的旧版本压缩包好的，非常感谢！这个模型我载入测试时，效果还是不好，目前我测试下来效果最好的还是作者发布的那个ceshi.pt，请问是有哪些地方配置不对吗？（另外，其实也很想知道作者训练ceshi.pt时的参数配置，如何训练重现，因为我训练的几个模型效果也是很差很差）> 这个模型我载入测试时，效果还是不好，目前我测试下来效果最好的还是作者发布的那个ceshi.pt，请问是有哪些地方配置不对吗？（另外，其实也很想知道作者训练ceshi.pt时的参数配置，如何训练重现，因为我训练的几个模型效果也是很差很差）这个模型实际上我未曾测试过，只是看着loss好看发出来。我建议你可以测试我另外一个aishell3 160K的，那个我有实测过效果不错。> > > 这个模型我载入测试时，效果还是不好，目前我测试下来效果最好的还是作者发布的那个ceshi.pt，请问是有哪些地方配置不对吗？（另外，其实也很想知道作者训练ceshi.pt时的参数配置，如何训练重现，因为我训练的几个模型效果也是很差很差）> > 这个模型实际上我未曾测试过，只是看着loss好看发出来。我建议你可以测试我另外一个aishell3 160K的，那个我有实测过效果不错。我就是下载的您云盘分享的“aishell3  160K  BS96模型”，这个是您说的实测过的吧？> > > > > > 这个模型我载入测试时，效果还是不好，目前我测试下来效果最好的还是作者发布的那个ceshi.pt，请问是有哪些地方配置不对吗？（另外，其实也很想知道作者训练ceshi.pt时的参数配置，如何训练重现，因为我训练的几个模型效果也是很差很差）> > > > > > 这个模型实际上我未曾测试过，只是看着loss好看发出来。我建议你可以测试我另外一个aishell3 160K的，那个我有实测过效果不错。> > 我就是下载的您云盘分享的“aishell3 160K BS96模型”，这个是您说的实测过的吧？这个在真人语音，只用9句话的情况下，跑40K即可有非常不错的效果。另外在游戏角色的语音克隆下也有一点勉强可用的效果。不知道你的自定义数据集多大，也许你用少量的自定义数据集跑一下会有不错的效果> > > > > > > > > > > 这个模型我载入测试时，效果还是不好，目前我测试下来效果最好的还是作者发布的那个ceshi.pt，请问是有哪些地方配置不对吗？（另外，其实也很想知道作者训练ceshi.pt时的参数配置，如何训练重现，因为我训练的几个模型效果也是很差很差）> > > > > > > > > 这个模型实际上我未曾测试过，只是看着loss好看发出来。我建议你可以测试我另外一个aishell3 160K的，那个我有实测过效果不错。> > > > > > 我就是下载的您云盘分享的“aishell3 160K BS96模型”，这个是您说的实测过的吧？> > 这个在真人语音，只用9句话的情况下，跑40K即可有非常不错的效果。另外在游戏角色的语音克隆下也有一点勉强可用的效果。不知道你的自定义数据集多大，也许你用少量的自定义数据集跑一下会有不错的效果感谢您的回复，我后来基于最新的代码又测了一下您的模型，是可以work的。同时，也想和您交流一下，请教一下您，我现在训练synthesizor模型，用aidatatang200Zh语料，batch_size为30（GPU的显存有限，只能用这么大的size了），loss基本就在0.37左右徘徊无法下降了。也正在其他的数据集，但我有个疑问，先不考虑其他因素，但从数据量来看，200h的数据loss无法下降，数据量的增大会不会对效果的影响也会有限呢。。。> > > > > > > > > > > > > > > 这个模型我载入测试时，效果还是不好，目前我测试下来效果最好的还是作者发布的那个ceshi.pt，请问是有哪些地方配置不对吗？（另外，其实也很想知道作者训练ceshi.pt时的参数配置，如何训练重现，因为我训练的几个模型效果也是很差很差）> > > > > > > > > > > > 这个模型实际上我未曾测试过，只是看着loss好看发出来。我建议你可以测试我另外一个aishell3 160K的，那个我有实测过效果不错。> > > > > > > > > 我就是下载的您云盘分享的“aishell3 160K BS96模型”，这个是您说的实测过的吧？> > > > > > 这个在真人语音，只用9句话的情况下，跑40K即可有非常不错的效果。另外在游戏角色的语音克隆下也有一点勉强可用的效果。不知道你的自定义数据集多大，也许你用少量的自定义数据集跑一下会有不错的效果> > 感谢您的回复，我后来基于最新的代码又测了一下您的模型，是可以work的。同时，也想和您交流一下，请教一下您，我现在训练synthesizor模型，用aidatatang200Zh语料，batch_size为30（GPU的显存有限，只能用这么大的size了），loss基本就在0.37左右徘徊无法下降了。也正在其他的数据集，但我有个疑问，先不考虑其他因素，但从数据量来看，200h的数据loss无法下降，数据量的增大会不会对效果的影响也会有限呢。。。作者本人倾向于，多数据集的混合训练，以及微调训练参数。所以如果你loss下不去，就考虑调整学习率（learning rate），亦或者换数据集、增加数据集这种手段来解决。至于效果到底如何，还是需要你去实测才知道。因为不同batch size的大小也能影响效果，我在训练数据集时发现，aidatatang200_zh混合aishell3，loss值的下降速度比单纯训练1个数据集，要快得多。但是loss也不代表最终效果，你可以看到我分享的这个70K虽然loss很好看，但据反馈效果不行，所以我个人觉得混合数据集也需要增加训练步数才会有更好的效果--以上基于本人纯猜想及经验，具体以实践为真。> 合数据集也需要增加训练步数才会有更好的效果好的，非常感谢您的回复和经验分享，我最近也在尝试，同时后面时间允许也会认真看下代码，期待与您做进一步交流，感谢！> ，提取码：7777> > aishell3数据集，Tesla V100 32G，BS 96训练的160K，loss值0.24> > 同时本人有两台V100 32G闲置，为BUG时撸的云服务器，有想训练啥的也可以提要求，反正闲着也是闲着。您好，我用您这个模型，报这样的错误，我该怎么办Caught exception: RuntimeError('Error(s) in loading state_dict for Tacotron:\n\tsize mismatch for encoder_proj.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for gst.stl.attention.W_query.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for decoder.attn_rnn.weight_ih: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([384, 1280]).\n\tsize mismatch for decoder.rnn_input.weight: copying a param with shape torch.Size([1024, 640]) from checkpoint, the shape in current model is torch.Size([1024, 1152]).\n\tsize mismatch for decoder.stop_proj.weight: copying a param with shape torch.Size([1, 1536]) from checkpoint, the shape in current model is torch.Size([1, 2048]).',)Restarting您好，请问训练的时候有遇到过训练过一段时间后，train loss跑飞了的情况吗ارحب 
用预训练的模型合成的语音问题
用的@miven的预训练模型，启动web，上传wav音频文件，合成的语音全是杂音，怎么解决#37 
启动报错
File "/Users/x/Documents/AudioCreate/MockingBird/web/__init__.py", line 33, in webApp    encoder.load_model(Path("encoder/saved_models/my_run.pt"))  File "/Users/x/Documents/AudioCreate/MockingBird/encoder/inference.py", line 34, in load_model    _model.load_state_dict(checkpoint["model_state"])  File "/Users/x/.conda/envs/MockingBird/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1052, in load_state_dict    self.__class__.__name__, "\n\t".join(error_msgs)))RuntimeError: Error(s) in loading state_dict for SpeakerEncoder: 
TypeError: lazy_pinyin() got an unexpected keyword argument 'neutral_tone_with_five'
TypeError: lazy_pinyin() got an unexpected keyword argument 'neutral_tone_with_five' 加载音频文件，点击Synthesize and vocode报这个错误，请问这个有可能是什么原因导致的> TypeError: lazy_pinyin() got an unexpected keyword argument 'neutral_tone_with_five' 加载音频文件，点击Synthesize and vocode报这个错误，请问这个有可能是什么原因导致的你本地是不是安装了 pypinyin?版本冲突了吧是的，重新安装pypinyin可以了，多谢了 
这个能支持多卡训练吗?
我对pytorch不太清楚...目前项目有跑过3090双卡，但性能貌似没被完全发挥出来，有兴趣可以联系我做后续验证。支持SLI的卡没试过，但我的双卡不支持，只能手动让pytorch调用其中一张卡，不然会出错> 支持SLI的卡没试过，但我的双卡不支持，只能手动让pytorch调用其中一张卡，不然会出错能请问大佬是如何指定的吗 
if np.isnan(grad_norm.cpu()): AttributeError: 'float' object has no attribute 'cpu'
运行环境：天池实验室notebook gpuhttps://tianchi.aliyun.com/https://dsw-dev.data.aliyun.com/run:!python synthesizer_train.py mandarin  /data/nas/workspace/jupyter/data/SV2TTS/synthesizer==========err:Arguments:    run_id:          mandarin    syn_dir:         /data/nas/workspace/jupyter/data/SV2TTS/synthesizer    models_dir:      synthesizer/saved_models/    save_every:      1000    backup_every:    25000    force_restart:   False    hparams:         Checkpoint path: synthesizer/saved_models/mandarin/mandarin.ptLoading training data from: /data/nas/workspace/jupyter/data/SV2TTS/synthesizer/train.txtUsing model: Tacotron[AMP WARNING][Frontend.cpp:121][1634655387:258952]Sleep 0.1s waiting for AMP Server socket: /tmp/harp/ccl_ipc_socket/system/server.socket[AMP INFO][Frontend.cpp:152][1634655387:359287]pid=741, start to allocate gpu resource ...Using device: cudaInitialising Tacotron Model...Trainable Parameters: 30.875MStarting the training of Tacotron from scratchUsing inputs from:	/data/nas/workspace/jupyter/data/SV2TTS/synthesizer/train.txt	/data/nas/workspace/jupyter/data/SV2TTS/synthesizer/mels	/data/nas/workspace/jupyter/data/SV2TTS/synthesizer/embedsFound 9845 samples+----------------+------------+---------------+------------------+| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |+----------------+------------+---------------+------------------+|   20k Steps    |     12     |     0.001     |        2         |+----------------+------------+---------------+------------------+ Traceback (most recent call last):  File "synthesizer_train.py", line 35, in <module>    train(**vars(args))  File "/data/nas/workspace/jupyter/MockingBird/synthesizer/train.py", line 200, in train    if np.isnan(grad_norm.cpu()):AttributeError: 'float' object has no attribute 'cpu'torch版本？import torchprint(torch.__version__)1.7.1用的model mandarin.pt是从哪里下载的呢？可能有损坏是训练合成器时的初始化生成的模型啊。最初是没有的。不过预训练python pre.py F:\data  -d BZNSYP -s，我是以前在其他windows server 2016训练成功后生成的SV2TTS目录下面的文件拷贝过去的。 
web使用合成出现报错
使用web版,选择提供的my_run.pt,点击"上传合成"出现以下的报错,state_dict和pt文件不匹配吗? 如何简单修改 看起来你用了最新代码，我fix一下这issue我应该已经fix了，你是不是拉一下最新代码试试？> 
ImportError: Failed to import any qt binding
运行时报错 不知道为什么Traceback (most recent call last):  File "demo_toolbox.py", line 2, in <module>    from toolbox import Toolbox  File "D:\pythonxiangmu\yuyinkelong\MockingBird\toolbox\__init__.py", line 1, in <module>    from toolbox.ui import UI  File "D:\pythonxiangmu\yuyinkelong\MockingBird\toolbox\ui.py", line 2, in <module>    from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas  File "D:\pythonxiangmu\anaconda\envs\yuyin\lib\site-packages\matplotlib\backends\backend_qt5agg.py", line 11, in <module>    from .backend_qt5 import (  File "D:\pythonxiangmu\anaconda\envs\yuyin\lib\site-packages\matplotlib\backends\backend_qt5.py", line 13, in <module>    import matplotlib.backends.qt_editor.figureoptions as figureoptions  File "D:\pythonxiangmu\anaconda\envs\yuyin\lib\site-packages\matplotlib\backends\qt_editor\figureoptions.py", line 11, in <module>    from matplotlib.backends.qt_compat import QtGui  File "D:\pythonxiangmu\anaconda\envs\yuyin\lib\site-packages\matplotlib\backends\qt_compat.py", line 179, in <module>    raise ImportError("Failed to import any qt binding")ImportError: Failed to import any qt bindingpython qt5 的问题，试着重装吗？你是什么系统呀？> python qt5的问题，星际重装吗？你是什么系统呀？Win10 试过重装了 还是不行的   我把anaconda直接卸载了  明天打算重装anaconda试一下 
Create SECURITY.md
Hey there!I belong to an open source security research community, and a member (@0xab3l) has found an issue, but doesn’t know the best way to disclose it.If not a hassle, might you kindly add a  file with an email, or another contact method? GitHub this best practice to ensure security issues are responsibly disclosed, and it would serve as a simple instruction for security researchers in the future.Thank you for your consideration, and I look forward to hearing from you!(cc @huntr-helper)Could you elaborate the issue? you can contact me by email: babysor00@gmail.com 
輸出音頻長短
Hi,我使用網頁版去測試，發現輸出的音頻無論字數多寡都只會輸出2秒鐘的音頻 如果字數太多就會讀不完。請問這樣是正常的嗎? 或是可以從哪裡去調整這個限制。Thanks我看到很多人有一样的问题，你有看到console的error吗？试的是哪个模型，ceshi.pt需要考虑 #37 同样的问题 试了ceshi.pt和train3_200k.pt 都是只输出2秒的音频同样的问题，另外，如果一段很长的话里面有多个句子（假定逗号分割），如果一个句子长度超过2秒，则这个句子也只会读2秒，然后就到下一句了。如果作者修了这个问题 无比踢我一脚> 如果作者修了这个问题 无比踢我一脚经过几根头发的思考，这个问题能通过改模型参数来调整而不是代码，你看下hparam里面 win size 可能需要重新训练 现有的模型都不行> > 如果作者修了这个问题 无比踢我一脚> > 经过几根头发的思考，这个问题能通过改模型参数来调整而不是代码，你看下hparam里面 win size 可能需要重新训练 现有的模型都不行这个可以改成无限大么？会有啥影响？已经修复 @kslz  ，并在uitoolbox加入了可配置项 
来个电报交流群
telegram 是不是需要twitter账号> hmm, telegram 是不是需要twitter账号你需要一个tg号，不要求twitter所以链接过期了啊> > > 请该项目负责人联系管理员权限。链接失效了有新链接么发现上面都过期了，我新建了一个长期channel @SeedKunY 
synthesizer训练时-b参数设置不当导致训练模型被覆盖
命令样例  训练synthesizer时，如果-b小于1000(不是1000的倍数)比如比如-b 500 （每500次备份一次模型)，由于模型名称的保存规则：name_1k.ptname_2k.ptname_3k.ptname_4k.pt这就导致了，，，，备份的模型被覆盖掉！！！500步的备份模型被1000步的备份模型覆盖了！！！修正方法：1. -b 参数值只能是1000的倍数，这样才不会覆盖2. 或者修改py文件里面的命名规则Fixed 
我来建个QQ交流群吧
之前有个微信群的二维码都过期了，自己加不上，所以想着建个QQ群，自己平时电脑一直开QQ，也方便经常上QQ的技友进行交流。群   
台湾口音模型网盘链接失效了，求最新链接
台湾口音模型网盘链接失效了，求最新链接https://u.teknik.io/AYxWf.pt 
docker 镜像
你好，可以提供个 docker 镜像吗，安装依赖、启动项目 可能对其他开发有一定成本项目比较依赖gpu，这个不知道可不可行https://github.com/babysor/MockingBird/issues/122#issuecomment-962391780@lyqscmy 大佬  能不能出个把此项目打包成docker镜像的教程  运维不太熟  尴尬> 
nn.functional.tanh is deprecated
E:\ProgramData\Anaconda3\envs\py39env\lib\site-packages\torch\nn\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")Don't worry. it won't mess up. Could you do a quick fix? 
集外数据测试，特别是男生，模仿效果不好
本人目前主要在做一句话语音克隆技术；对你的模型做了测试，感觉集外模仿效果，特别是男生，相似度不够；麻烦问下，这种问题怎么解决，就是单纯的增加相关预料吗？模型和数据集问题，可以增加物料Wa2 
运行报错
2333是numba版本不兼容 卸载后重装下就可以了 
选择加载本地录音文件m4a报错，请问是什么原因
Traceback (most recent call last):  File "D:\software\Anaconda3\envs\pytorch\lib\site-packages\librosa\core\audio.py", line 149, in load    with sf.SoundFile(path) as sf_desc:  File "D:\software\Anaconda3\envs\pytorch\lib\site-packages\soundfile.py", line 629, in __init__    self._file = self._open(file, mode_int, closefd)  File "D:\software\Anaconda3\envs\pytorch\lib\site-packages\soundfile.py", line 1184, in _open    "Error opening {0!r}: ".format(self.name))  File "D:\software\Anaconda3\envs\pytorch\lib\site-packages\soundfile.py", line 1357, in _error_check    raise RuntimeError(prefix + _ffi.string(err_str).decode('utf-8', 'replace'))RuntimeError: Error opening 'C:\\Users\\zzzz\\Desktop\\2021.m4a': File contains data in an unknown format.应该是soundfile不支持m4a格式，用pydub(需要依赖ffmpeg)转换为16kHz的wav格式后应该就可以了 
自定义训练音频相关
首先很感谢作者的付出，在这里，我想问下，如果我想训练自己的音频，是不是只能到你已经定义好的文件侠里面把原有的音频和对应的TXT替换？但这样操作起来真的很不方便啊，要是只要按照指定格式，然后自己随便指定文件名就好了。不知道这个作者能优化下吗？感激不尽啊！参考最近的代码提交，动手能力强的可以自己按这个逻辑和输出格式自动处理：https://github.com/babysor/MockingBird/pull/141谢谢 
项目名字的含义貌似有些许问题
“这个仓库的名字MockingBird是仿声鸟、反舌鸟，以善于模仿其他鸟类及昆虫、两栖动物的叫声而闻名，也是一种经常出现在西方文学或影视作品之中的鸟类，在生物学上是嘲鸫的俗称。著名的书的名字《杀死一只知更鸟》的英文就是To Kill a Mocking Bird，实际上属于翻译的错误，知更鸟的英文是Robin。”原文--https://mp.weixin.qq.com/s/SdwxkeVLBtjOvIb2-2xZgg刚看到公众号发的这个公众号怎么扒的信息啊。。。哈哈，那你建议什么新的名字吗？不如直接叫：Pseudo-Acoustic，伪声的英文词在整个GitHub只有两个项目且不相关 
关于中英混杂。
谢谢您的分享，我想做能支持中英混杂的语音克隆。现在中文除了你提到的语料，英文还有libritts, vctk等，另外我们自己有少量的中英混杂语音。如果想做中英混杂的语音克隆，请问您有什么建议吗？多谢！不好意思，另外想再问下，是否可以考虑使用VITS的模型结构，这样不用单独训练声学模型和vocoder,直接端到端训练模型。然后将语音克隆的相关的模块迁移到VITS模型上，训练效率应该能好一些，合成速度也更快。奇怪，我的回答好像被删了。理论只需要喂英文数据集混合去训练模型是完全可行的，（中文拼音涵盖了英文部分）VITS结构我也前段时间调研过，可能改动会比较大， 但如果很有兴趣可以联系我邮箱一起搞 babysor00@gmail.com好的，我跟您发信了，希望能有幸与您合作。https://github.com/babysor/MockingBird/issues/440 
支持data_aishell（SLR33）数据集
如题谢谢 
第三个预训练模型报错
RuntimeError: Error(s) in loading state_dict for Tacotron:	size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).请问是什么原因？#37 duplicated 
安装python出错！
教程能写细点不，有点大而泛了，我这种小白配置环境都没搞起来 ，哈哈哈哈！！！哈哈哈，敬请期待 #49 
librosa.load无法读取filestorage音频对象
我在web调用时提示File contains data in an unknown format.发现__init__py中使用librosa读取音频文件使用librosa.load(request.files['file'])我并没有找到librosa能直接读取filestorage对象的资料最后通过接收文件本地保存来处理这个问题同样问题加载的文件格式是？具体报错可以贴一下吗？这里可能是web框架依赖的librosa支持格式有限 
Add gst
 synthesizer模型新加入GlobalStyleToken（https://arxiv.org/pdf/1803.09017.pdf），明显增加韵律特征获取，稍微提升音色特征获取，可使用使用Global的Style大幅提升合成鲁棒性（减少鬼音等合成失败几率）。使用方法：gh pr checkout 137， 然后重新训练，运行时在demobox里style一栏填0~9尝试不同韵律风格。动手能力强的可以根据论文修改该部分风格，目前为训练自动分类的风格。 
Update ui.py
Add minimize and maximize button of window 
请问如何对多个数据集进行合并，并进行训练以降低Loss？
请问如何对多个数据集进行合并，并进行训练以降低Loss？我已经下载了三个数据集<datasets_root>├──aidatatang_200zh│  ├──corpus│  │  ├──dev│  │  ├──test│  │  └──train│  └──transcript├──data_aishell3│  ├──test│  │  └──wav│  └──train│     └──wav├──MAGICDATA│  └──train│     ├──14_3466│     ├──14_3664│   ........│     └──5_970└──SV2TTS   └──synthesizer      ├──audio      ├──embeds      └──mels现在我应该如何将这三个混合起来？按这个顺序运行pre：1.第一次直接运行pre.py，默认处理aidatatang 2. 第二次指定数据集为aishell3, 带上-d aishell3 -s 3. 第三次指定数据集为magicdata，带上-d magicdata -s > 按这个顺序运行pre： 1.第一次直接运行pre.py，默认处理aidatatang 2. 第二次指定数据集为aishell3, 带上-d aishell3 -s 3. 第三次指定数据集为magicdata，带上-d magicdata -s你好，请问 “在预处理数据（pre.py 或 synthesizer_preprocess_audio.py）的时候记得加入参数：--skip_existing”  这算是你提出的另一种方法吗？> > 按这个顺序运行pre： 1.第一次直接运行pre.py，默认处理aidatatang 2. 第二次指定数据集为aishell3, 带上-d aishell3 -s 3. 第三次指定数据集为magicdata，带上-d magicdata -s> > 你好，请问 “在预处理数据（pre.py 或 synthesizer_preprocess_audio.py）的时候记得加入参数：--skip_existing” 这算是你提出的另一种方法吗？不是，这是正确顺序 
请问如果我提供更长的原语音，对生成语音的质量会有改善么？
我尝试生成了一些语音，但是新语音和原语音的音色差别很大，不像是同一个人说的更长其实不会的，这个模型训练的时候主要用的非常短的语音，试图获取最突出的音色进行克隆。> 更长其实不会的，这个模型训练的时候主要用的非常短的语音，试图获取最突出的音色进行克隆。但是实际上，我分别使用了15秒、10分钟和20分钟的洛天依语调校作为语音输入进去，得到的结果显示显然20分钟的语音效果最好。> > 更长其实不会的，这个模型训练的时候主要用的非常短的语音，试图获取最突出的音色进行克隆。> > 但是实际上，我分别使用了15秒、10分钟和20分钟的洛天依语调校作为语音输入进去，得到的结果显示显然20分钟的语音效果最好。推理的时候吗？这个不合理把。。20分钟的语音会有很多奇怪的杂音，一下子就歪了 
报错：Model files not found. 
电脑为M1在运行时出现以下提示：(MockingBird) zsh@zshMacBook-Pro ~ % python /Users/zsh/Desktop/MockingBird-main/demo_toolbox.py -d /Users/zsh/DesktopArguments:    datasets_root:    /Users/zsh/Desktop    enc_models_dir:   encoder/saved_models    syn_models_dir:   synthesizer/saved_models    voc_models_dir:   vocoder/saved_models    cpu:              False    seed:             None    no_mp3_support:   False********************************************************************************Error: Model files not found. Follow these instructions to get and install the models:https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models********************************************************************************请问该如何解决呀？是不是没有下载预先训练好的模型放在： synthesizer/saved_models 下呀> 是不是没有下载预先训练好的模型放在： synthesizer/saved_models  而不是 models已新建synthesizer/saved_models文件夹，但是仍然报错，/哭了：Arguments:    datasets_root:    /Users/zsh/Desktop    enc_models_dir:   encoder/saved_models    syn_models_dir:   synthesizer/saved_models    voc_models_dir:   vocoder/saved_models    cpu:              False    seed:             None    no_mp3_support:   False********************************************************************************Error: Model files not found. Follow these instructions to get and install the #65 看起来路径格式M1可能也有问题麻烦大佬帮忙解决一下，手上真的只有这台电脑了😭参考一下上面这个 #65 链接 
训练到7344/20414错误提示：_pickle.PicklingError: Can't pickle <class 'MemoryError'>: it's not the same object as builtins.MemoryError
  File "C:\Users\86158\Anaconda3\envs\pytorch\lib\multiprocessing\queues.py", line 239, in _feed    obj = _ForkingPickler.dumps(obj)  File "C:\Users\86158\Anaconda3\envs\pytorch\lib\multiprocessing\reduction.py", line 51, in dumps    cls(buf, protocol).dump(obj)  File "C:\Users\86158\Anaconda3\envs\pytorch\lib\site-packages\torch\multiprocessing\reductions.py", line 319, in reduce_storage    metadata = storage._share_filename_()RuntimeError: Couldn't open shared file mapping: <0000029FFD7D77B2>, error code: <1455>{| Epoch: 1/1 (7341/20414) | Loss: 0.7548 | 0.86 steps/s | Step: 7k | }Traceback (most recent call last):  File "C:\Users\86158\Anaconda3\envs\pytorch\lib\multiprocessing\queues.py", line 239, in _feed    obj = _ForkingPickler.dumps(obj)  File "C:\Users\86158\Anaconda3\envs\pytorch\lib\multiprocessing\reduction.py", line 51, in dumps    cls(buf, protocol).dump(obj)  File "C:\Users\86158\Anaconda3\envs\pytorch\lib\site-packages\torch\multiprocessing\reductions.py", line 319, in reduce_storage    metadata = storage._share_filename_()RuntimeError: Couldn't open shared file mapping: <000001EBF5A60222>, error code: <1455>{| Epoch: 1/1 (7342/20414) | Loss: 0.7547 | 0.86 steps/s | Step: 7k | }Traceback (most recent call last):  File "C:\Users\86158\Anaconda3\envs\pytorch\lib\multiprocessing\queues.py", line 239, in _feed    obj = _ForkingPickler.dumps(obj)  File "C:\Users\86158\Anaconda3\envs\pytorch\lib\multiprocessing\reduction.py", line 51, in dumps    cls(buf, protocol).dump(obj)_pickle.PicklingError: Can't pickle <class 'MemoryError'>: it's not the same object as builtins.MemoryError{| Epoch: 1/1 (7343/20414) | Loss: 0.7564 | 0.87 steps/s | Step: 7k | }Traceback (most recent call last):  File "C:\Users\86158\Anaconda3\envs\pytorch\lib\multiprocessing\queues.py", line 239, in _feed    obj = _ForkingPickler.dumps(obj)  File "C:\Users\86158\Anaconda3\envs\pytorch\lib\multiprocessing\reduction.py", line 51, in dumps    cls(buf, protocol).dump(obj)**_pickle.PicklingError: Can't pickle <class 'MemoryError'>: it's not the same object as builtins.MemoryError**{| Epoch: 1/1 (7344/20414) | Loss: 0.7568 | 0.87 steps/s | Step: 7k | }Traceback (most recent call last):大佬们 帮忙解决  /(ㄒoㄒ)/~~  训练了一天了 从早上到晚上pytorch在并行处理时偶尔会出错，你有在同时运行什么程序吗？重新运行一下就好了应该+----------------+------------+---------------+------------------+| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |+----------------+------------+---------------+------------------+|   10k Steps    |     8      |    0.0001     |        2         |+----------------+------------+---------------+------------------+重新运行 卡在这里了 …… 所以 再执行相同的命令了  这已经是第72个小时了 哈哈把batch_size改小就可以了 
RuntimeError: PytorchStreamReader failed reading file data/39: invalid header or archive is corrupted
Traceback (most recent call last):  File "E:\PyProjects\git_projects\MockingBird\toolbox\__init__.py", line 122, in <lambda>    func = lambda: self.synthesize() or self.vocode()  File "E:\PyProjects\git_projects\MockingBird\toolbox\__init__.py", line 236, in synthesize    specs = self.synthesizer.synthesize_spectrograms(texts, embeds)  File "E:\PyProjects\git_projects\MockingBird\synthesizer\inference.py", line 87, in synthesize_spectrograms    self.load()  File "E:\PyProjects\git_projects\MockingBird\synthesizer\inference.py", line 65, in load    self._model.load(self.model_fpath)  File "E:\PyProjects\git_projects\MockingBird\synthesizer\models\tacotron.py", line 496, in load    checkpoint = torch.load(str(path), map_location=device)  File "D:\ProgramData\pyvenv\ml\lib\site-packages\torch\serialization.py", line 607, in load    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)  File "D:\ProgramData\pyvenv\ml\lib\site-packages\torch\serialization.py", line 882, in _load    result = unpickler.load()  File "D:\ProgramData\pyvenv\ml\lib\site-packages\torch\serialization.py", line 857, in persistent_load    load_tensor(data_type, size, key, _maybe_decode_ascii(location))  File "D:\ProgramData\pyvenv\ml\lib\site-packages\torch\serialization.py", line 845, in load_tensor    storage = zip_file.get_storage_from_record(name, size, dtype).storage()RuntimeError: PytorchStreamReader failed reading file data/39: invalid header or archive is corruptedsaved_models文件夹里的zip文件需要移除> saved_models文件夹里的zip文件需要移除不行，我文件夹里面没有zip，你们查看一下百度云train3_200k.pt这个文件是不是损坏了，无论是运行demo_toolbox.py，还是web.py，都会报上面的错，我用torch.load()加载的时候也报上面的错误，说明这个文件可能是损坏了。当然也可能是我下载过程损坏了，方便的话，可以提供一下train3_200k.pt文件的MD5值吗？谢谢。是这个链接? 链接：https://pan.baidu.com/s/1VHSKIbxXQejtxi2at9IrpA 提取码：i183 SHA256: 61450216577FD54B0658DADB4B562DCA5D62D6DBE54BA73B0A833FBA3D95A076 
need at least one array to concatenate
上传合成报错 need at least one array to concatenate文字是空的？看起来问题已经得到解决，如果有相关改进建议或仍存在问题可以重新打开 It seems this issue had been handled well, please be free to reopen if got any suggestion or new problems. 
pre.py處理後的train.txt為空文件導致error
我使用的數據集為Mozilla，依照PR去更改來處理資料，不過處理後  
RuntimeError: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 6.00 GiB total capacity; 1.83 GiB already allocated; 2.49 GiB free; 2.02 GiB reserved in total by PyTorch)
{| Epoch: 1/1 (121/15311) | Loss: 1.719 | 0.73 steps/s | Step: 0k | }Traceback (most recent call last):出现错误RuntimeError: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 6.00 GiB total capacity; 1.83 GiB already allocated; 2.49 GiB free; 2.02 GiB reserved in total by PyTorch)显示显存不足已经修改### Tacotron Training        tts_schedule = [(2,  1e-3,  10_000,  8),   # Progressive training schedule                        (2,  5e-4,  15_000,  8),   # (r, lr, step, batch_size)                        (2,  2e-4,  20_000,  8),   # (r, lr, step, batch_size)                        (2,  1e-4,  30_000,  8),   #                        (2,  5e-5,  40_000,  8),   #                        (2,  1e-5,  60_000,  8),   #                        (2,  5e-6, 160_000,  8),   # r = reduction factor (# of mel frames                        (2,  3e-6, 320_000,  8),   #     synthesized for each decoder iteration)                        (2,  1e-6, 640_000,  8)],  # lr = learning rate请问我要怎么调整才能继续训练合成器？感谢各位大佬batch_size 6，估计你这个显卡专用显存比较低NVIDIA GeForce GTX1060 6GB应该是够的 看看有没有其他进程占用有可能是在两个conda环境中都装了pytorch。我在base里配置完环境后，可以正常运行；但是新建一个环境然后同样再装一个pytorch，训练的时候就会报“CUDA out of memory”。把其中一个装了pytorch的环境删掉就可以了。 
区别
你好，这么多星感觉好棒。很早之前，就开始追 这个项目了，但是生成质量并没有很高，玩一下感觉挺清奇，但是达不到商用。看到你的主体架构跟 real-time的很相似。我暂时没细看你的代码，请教下：你这个项目跟real-time的区别是什么呢？ 做了什么改进呢？ 可以简单说下吗？非常感谢！hi 客气啦，其实都属于开源项目，应该也是达不到商用的。比较起来就是可玩性提高了一些，横向比较也刚被问过，我粘贴过来：1. 中文版是从英文版fork出来（readme有说明）encoder和synthesizer模型是完全一样的，只有参数和少量输入项有点区别，主要差异在数据集支持上，以便有足够多的中文说话人开源数据。2. 另外针对这个问题，基于社区与我提供的预训练模型，可以只进行中文数据集finetune即可，保证中文版的训练成功率。3. vocoder部分有HiFi-GAN 的引入，比起原英文版输入语音和文字后，经推理大概要7~15秒才能输出对应克隆的语音（vocoder部分使用的是推理过程比较慢的WaveRNN）基本可以在1秒内输出克隆的语音。4. web api与相应界面以便拓展开发5. 其他通过社区摸出来的坑也基本补上，也在考虑根据新的论文更替encoder和synthesizer部分了解了，谢谢！看起来问题已经得到解决，如果有相关改进建议或仍存在问题可以重新打开 It seems this issue had been handled well, please be free to reopen if got any suggestion or new problems. 
显示无权限怎么处理
<img width="1224" alt="截屏2021-10-08 下午5 08 57" src="https://user-images.githubusercontent.com/45282526/136530783-26c2011a-3cf8-4580-8383-eab2b15c4e06.png">在启动toolbox时指定一个路径你好，如何在启动toolbox时指定一个路径，谢谢python demo_toolbox.py -d <datasets_root><datasets_root> 替换为一个本地文件路径 
教程出错，声码器训练时batch_size设置错误
经我个人测试，我认为声码器训练时如果出现显存不足错误，报错退出后，应该修改vocoder\hifigan\config_16k_.json 文件中的batch_size，因为vocoder_train.py里这么写的：parser.add_argument("--config", type=str, default="vocoder/hifigan/config_16k_.json")显存不足报错退出后，修改vocoder\wavernn\hparams.py里面的voc_batch_size = 16应该是没效果的               感谢目前声码器如果选用 wavernn，则修改 hparamshifi-gan则修改json 
Support music
Let AI support the recognition of timbre, and then sing in the recognition of song scoreThere's an ongoing project to achieve this but with a totally new model, pls stay tuned. 
训练到485K loss压不下去了，徘徊在0.4，如何更换模型？
如题，我已经使用 aidatatang_200zh 训练了485K step 了，不知道这个训练量是否足够，但是loss下不去了，现在想在当前训练基础上，将数据集更换成aishell3和magicdata，请问我应该如何操作？‘ Epoch: 3/15 (1078/12249) | Loss: 0.4008 | 1.0 steps/s | Step: 485k | }’预处理使用-s 输出到同一路径> 预处理使用-s 输出到同一路径谢谢作者回复，但是抱歉我还是有点不懂，您是说我预处理aishell3时，加一个-s 指定到跟aidatatang相同的路径吗？但是这个路径是指<datasets_root>？还是说<datasets_root>\aidatatang_200zh\ 之类的呢？能否给我个命令行样例，或者详细说明一下呢？将 两个数据集都放在一个 <datasets_root> 下，并且在预处理的时候加入 -s 或者 skip_existing参数... --skip_existing这样结束后应该可以在 <datasets_root>/SV2TTS/ 文件夹中看到多个数据的处理结果@LxKxC 大佬  请教一下  在win系统上  如何手动结束训练  我按ctrl+c总是会损坏模型文件> @LxKxC 大佬 请教一下 在win系统上 如何手动结束训练 我按ctrl+c总是会损坏模型文件从没遇到过您这种情况，我建议训练命令增加 -b 1000 -s 1000  ，这样每1000步就自动保存和备份一次，当再次遇到模型损坏时，将最新的备份模型改名替换掉主模型即可继续训练。但是由于频繁的备份，您的硬盘可能很快就会爆满> @LxKxC 大佬 请教一下 在win系统上 如何手动结束训练 我按ctrl+c总是会损坏模型文件不科学呀，这个没发生过> > > @LxKxC 大佬 请教一下 在win系统上 如何手动结束训练 我按ctrl+c总是会损坏模型文件> > 不科学呀，这个没发生过我也是一样  win11系统  CPU是AMD的> @LxKxC 大佬 请教一下 在win系统上 如何手动结束训练 我按ctrl+c总是会损坏模型文件请问你解决了吗？> > @LxKxC 大佬 请教一下 在win系统上 如何手动结束训练 我按ctrl+c总是会损坏模型文件> > 请问你解决了吗？已解决 验证后发现是pytorch版本问题 
请问让程序能正常跑起来的步骤是怎样的？
2.1 使用数据集自己训练合成器模型（与2.2二选一）·下载 数据集并解压：确保您可以访问 train 文件夹中的所有音频文件（如.wav）·进行音频和梅尔频谱图预处理： python pre.py <datasets_root> 可以传入参数 --dataset {dataset} 支持 aidatatang_200zh, magicdata, aishell3假如你下载的 aidatatang_200zh文件放在D盘，train文件路径为 D:\data\aidatatang_200zh\corpus\train , 你的datasets_root就是 D:\data\·训练合成器： python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer当您在训练文件夹 synthesizer/saved_models/ 中看到注意线显示和损失满足您的需要时，请转到启动程序一步。**是必须要完成上面所有的步骤才能跑起来么？才能到QT5的界面么？感谢您的回复，谢谢！**> 2.1 使用数据集自己训练合成器模型（与2.2二选一）> > ·下载 数据集并解压：确保您可以访问 train 文件夹中的所有音频文件（如.wav）> > ·进行音频和梅尔频谱图预处理： python pre.py <datasets_root> 可以传入参数 --dataset {dataset} 支持 aidatatang_200zh, magicdata, aishell3 假如你下载的 aidatatang_200zh文件放在D盘，train文件路径为 D:\data\aidatatang_200zh\corpus\train , 你的datasets_root就是 D:\data\> > ·训练合成器： python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer 当您在训练文件夹 synthesizer/saved_models/ 中看到注意线显示和损失满足您的需要时，请转到启动程序一步。> > **是必须要完成上面所有的步骤才能跑起来么？才能到QT5的界面么？ 感谢您的回复，谢谢！**https://github.com/babysor/MockingBird/wiki/Quick-Start-(Newbie)我总结一下吧，本人是小白，找的各处的教程拼起来的，可能会有错漏望指正。（安装CUDA的教程就不写了）在cmd使用这个代码来检查你是否安装了CUDA 注意：本流程中的任何路径都不能有空格！路径不能有空格！路径不能有空格！（重要的事情说三遍）先下载python（百度搜索官网），win7最多支持3.8.x安装Anaconda3（https://www.anaconda.com/products/individual#download-section），安好后手动添加环境变量（Windows 添加环境变量需要在电脑->鼠标右键->属性->高级系统设置->环境变量->Path中设置。）添加两个地址到用户变量Path，添加地址如下（注意改成自己的路径）> D:\AnacondaD:\Anaconda\Scripts打开_Anaconda promot_创建一个环境。 这句话的意思是创建一个名字为 your_name 的虚拟环境，并且这个虚拟环境额外安装 jupyter notebook 第三方库。可以将 your_name 改为你自己喜欢的名字，这个名字是你的虚拟环境的名字，自己随便取，比如jack。> 请注意：这时候要打开Anaconda里面的Anaconda Navigator (pythonAnaconda3) — Environments，找到自己刚创建的环境，在里面找到python，点前面的小对勾，能打开个菜单，鼠标移动到最下方的那个Mark for啥啥啥的，那里面能选python版本，新创建的python环境用的不是我刚才安装的python版本就很怪...记得改成自己能用的版本。安装好环境后，我们可以使用指令激活 jack 环境： 下载作者的代码，解压后复制这个文件夹的路径。然后这个时候要去Anaconda菜单下面找到Jupyter Notebook (jack) <你刚创建的环境名字>，右键打开属性-快捷方式-（在起始位置这里粘贴刚才复制的路径，然后把路径里面的“\”这个符号换成“\\”）。然后执行这个代码 括号里的路径换成刚才你粘贴的那个下载安装 PyTorch，选择为（1.9.1、windows、Conda、python、CUDA 10.2），执行下面这个代码。 下载aidatatang_200zh，地址如下https://www.openslr.org/resources/62/aidatatang_200zh.tgz反复解压，直到没有压缩包为止，然后删掉里面的压缩包只留下解压的文件18个G的文件呢，给你的硬盘腾点地方吧（笑）注意：路径不能有空格！路径不能有空格！路径不能有空格！（重要的事情说三遍）下载作者贴的那个训练模型https://pan.baidu.com/s/1VHSKIbxXQejtxi2at9IrpA 百度盘链接 提取码：i183放到你下载的作者代码里面的synthesizer文件夹里面的saved_models（没有就创建一个）剩下的就是照着作者的步骤走一遍就可以运行叻...谢谢各位的回答我想知道：如果不使用训练好的模型我要怎么操作？如何依照     **2.1 使用数据集自己训练合成器模型**   来操作> 谢谢各位的回答 我想知道： 如果不使用训练好的模型 我要怎么操作？ 如何依照 **2.1 使用数据集自己训练合成器模型** 来操作请看这篇：https://github.com/babysor/MockingBird/wiki/Quick-Start-(Newbie) python必须3.8以上，我用的conda，python环境3.9.7首先安装cuda ，具体安装哪个版本，请看pytorch.org支持哪个版本在  这里显示，pytorch只支持 cuda10.2或者11.1，所以单独下载cuda11.1或者10.2，具体哪个版本，要看你显卡最高支持哪个CUDA版本。然后从https://pytorch.org/get-started/locally/ 获取安装命令：conda 环境，windows cuda 10.2>conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorchconda 环境，windows cuda 11.1 NOTE: 'conda-forge' channel is required for cudatoolkit 11.1>conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forgeconda 环境，windows 仅使用CPU(适合仅仅是使用训练好的模型的情况)>conda install pytorch torchvision torchaudio cpuonly -c pytorch不用conda：windows cuda 10.2>pip3 install torch==1.9.1+cu102 torchvision==0.10.1+cu102 torchaudio===0.9.1 -f cuda 11.1>pip3 install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio===0.9.1 -f 仅使用CPU(适合仅仅是使用训练好的模型的情况)>pip3 install torch torchvision torchaudio解压aidatatang_200zh数据集时，需要子目录里面的压缩包也解压，在Linux下解压命令是：  或者 这条命令需要进入每一个aidatatang_200zh/corpus/xxx/目录后，再执行解压命令如果你是windows，可以安装一个MINGW64，在MINGW64的终端里进入aidatatang_200zh/corpus/下每一个目录执行 命令解压后就需要按照https://github.com/babysor/MockingBird/blob/main/README-CN.md 2.1 的说明进行数据预处理后并训练合成器谢谢大家aidatatang_200zh, magicdata, aishell3   这里有三个训练集  我是全部都需要执行么 还是只需要执行其中一个> aidatatang_200zh先训练一个吧，把aidatatang_200zh训练十几万再考虑换别的我在mac上使用MacBook Pro (13-inch, 2018, Four Thunderbolt 3 Ports)，上传台湾录音的时候，界面显示没有权限怎么办？> 我在mac上使用MacBook Pro (13-inch, 2018, Four Thunderbolt 3 Ports)，上传台湾录音的时候，界面显示没有权限怎么办？我没有macbook，但是mac os 应该是类linux的，我觉得这应该是你下载下来的文件夹权限错误导致的，你可以试试给整个项目文件夹赋777权限> > 我在mac上使用MacBook Pro (13-inch, 2018, Four Thunderbolt 3 Ports)，上传台湾录音的时候，界面显示没有权限怎么办？> > 我没有macbook，但是mac os 应该是类linux的，我觉得这应该是你下载下来的文件夹权限错误导致的，你可以试试给整个项目文件夹赋777权限我使用命令 sudo  chmod -R 777 MockingBird-main没起作用> > > 我在mac上使用MacBook Pro (13-inch, 2018, Four Thunderbolt 3 Ports)，上传台湾录音的时候，界面显示没有权限怎么办？> > > > > > 我没有macbook，但是mac os 应该是类linux的，我觉得这应该是你下载下来的文件夹权限错误导致的，你可以试试给整个项目文件夹赋777权限> > 我使用命令 sudo chmod -R 777 MockingBird-main没起作用建议将报错内容在google上搜索下，或者贴出来问问作者只用于预测的容器镜像我已经构建好，内置预训练模型，本次测试跑过了。docker run -p 8080:8080 -it jiada/mocking_bird> 只用于预测的容器镜像我已经构建好，内置预训练模型，本次测试跑过了。 docker run -p 8080:8080 -it jiada/mocking_bird只有web界面吗？做了一个视频 试了一下基本上没效果哦 能跑起来> 只用于预测的容器镜像我已经构建好，内置预训练模型，本次测试跑过了。 docker run -p 8080:8080 -it jiada/mocking_bird请问这个有后续的toolbox界面吗> 我总结一下吧，本人是小白，找的各处的教程拼起来的，可能会有错漏望指正。（安装CUDA的教程就不写了） 在CMD使用这个代码来检查你是否安装了CUDA  注意：本流程中的任何路径都不能有空格！路径不能有空格！路径不能有空格！（重要的事情说三遍）> > 下载python（下载python），win7后最多支持3.8.x > > D:\Anaconda > > D:\Anaconda\Scripts> > 打开_promot_创建一个环境。  这句话的英文是为你的虚拟环境创建一个名字，并且这个虚拟环境安装jupyter notebook，额外的Anacon dacon库。 可以将你的名字改为你自己喜欢的名字，这个名字是你的虚拟环境的名字，自己随便取，比如jack。> > > 请注意：这时候要打开 Anaconda 里面的 Anaconda Navigator (pythonAnaconda3) — 环境，鼠标找到自己刚创建的环境，在里面 python，点前面的小对那个，可以打开，移动到最找到的菜单中的标记为啥啥啥的，那里面能选python版本，创造的python环境用的不是我刚刚安装的python版本就很奇怪……记得改成自己能用的版本。> > 安装好环境后，我们可以使用激活激活 jack 环境： > > 下载作者的代码，解压后复制文件夹的路径。 然后这个时候去Anaconda菜单（杰克）<你创建下面的环境名字>，查看这里的属性-快捷方式-（在此处找到Jupyter的位置）粘贴刚才复制的路径，然后把路径里面的“\”这个符号换成“\”）。> > 然后执行这个代码 里面那个的路径换成你刚才那个的> > 下载安装PyTorch，选择为（1.9.1、windows、Conda、python、CUDA 10.2），执行下面这个代码。 > > 下载aidatatang_200zh，地址为 反复压缩解压呢，直到没有压缩包为止，然后再把里面的压缩包只留下解压的文件18个G的文件，给你的硬盘腾点地方吧（笑） 注意：路径不能有空格！不能有空格！路径不能有空格！（重要的事情说三遍）> > 下载作者的贴子模型 把你下载的作者代码里面的synthesizer文件夹里面的saved_models（没有就创建一个）> > 剩下的就是照着的步骤走走就可以运行叻...可以加你个联系方式嘛，寻求一些安装指点！求助，按照QuickStart安装顺利，运行.py文件报错如下 > 求助，按照QuickStart安装顺利，运行.py文件报错如下 pytorch安装那一步没有成功> 只用于预测的容器镜像我已经构建好，内置预训练模型，本次测试跑过了。 docker run -p 8080:8080 -it jiada/mocking_birdThe docker image redislabs/redistimeseries latest throws an error on Jetson TX1:WARNING: The requested image's platform (linux / amd64) does not match the detected host platform (linux / arm64 / v8) and no specific platform was requestedstandard_init_linux.go: 228: exec user process caused: exec format errorThis image in dockerhub are built for the x86_64 architecture. Would you like to build ARM version? 
 点了录音后出现Audio buffer is not finite everywhere
free to add your own. You can still use the toolbox by recording samples yourself.Traceback (most recent call last):  File "J:\Code\Python\MockingBird\toolbox\__init__.py", line 182, in record    self.add_real_utterance(wav, name, speaker_name)  File "J:\Code\Python\MockingBird\toolbox\__init__.py", line 186, in add_real_utterance    spec = Synthesizer.make_spectrogram(wav)  File "J:\Code\Python\MockingBird\synthesizer\inference.py", line 168, in make_spectrogram    mel_spectrogram = audio.melspectrogram(wav, hparams).astype(np.float32)  File "J:\Code\Python\MockingBird\synthesizer\audio.py", line 60, in melspectrogram    D = _stft(preemphasis(wav, hparams.preemphasis, hparams.preemphasize), hparams)  File "J:\Code\Python\MockingBird\synthesizer\audio.py", line 121, in _stft    return librosa.stft(y=y, n_fft=hparams.n_fft, hop_length=get_hop_size(hparams), win_length=hparams.win_size)  File "J:\Anaconda3\envs\Test\lib\site-packages\librosa\core\spectrum.py", line 217, in stft    util.valid_audio(y)  File "J:\Anaconda3\envs\Test\lib\site-packages\librosa\util\utils.py", line 310, in valid_audio    raise ParameterError("Audio buffer is not finite everywhere")librosa.util.exceptions.ParameterError: Audio buffer is not finite everywhere重新装了下ffmpeg就好了 
请问训练模型的时候如何保存.....
我知道这个问题比较傻，但是我现在不知道怎么保存，每次训练都是从头开始，求大佬指点会隔1000步自动保存，用同一个id就可以继续谢谢> 会隔1000步自动保存，用同一个id就可以继续您好，我程序里面使用了显卡，但是速率还是只有0.13steps/s ,Using device: cuda，我调整了Batch Size到36显存占用率已经有80%了，但是显卡使用率只有12%左右，我还需要调整什么参数呢？主要消耗的是显存 gpu占有率不高。你这速率有点问题 什么显卡？> 主要消耗的是显存 gpu占有率不高。你这速率有点问题 什么显卡？GTX1080ti，已经安装了CUDA和CUDNN> > 主要是消耗显着存gpu占有率不高。你这有点问题什么显卡？> > GTX1080ti，已经安装了CUDA和CUDNNCUDA是10.2.87cuDNN是7.6.5我看到有篇文章遇到的问题和我差不多，都是显卡使用率忽高忽低，大约保持在10%左右>后来我偶然为了减小最后的tensorboard > 我看到有篇文章遇到的问题和我差不多，都是显卡使用率忽高忽低，大约保持在10%左右> > > 后来我偶然为了减小最后的tensorboard log文件，我把一些用不到的tensorboard的写入比如直方图给删掉了，发现速度瞬间提上来了，然后再减小训练时的输出间隔，速度也有提升。立刻明白了，是因为训练的主要时间都花在了写日志上，文件IO耗时特别多，尤其是我设置的写入间隔还很小，所以GPU计算一瞬间，然后写很久的记录，计算一瞬间，再写很久的记录，最终导致速度特别慢。这也正是为了速度和我笔记本的GTX1050一样，因为大家比的都是写文件的速度。> > 最后总结一下，有的时候模型训练慢并不是因为显卡不行或者模型太大，而是在跑模型过程中有一些其他的操作导致速度很慢，尤其是文件的IO操作，这会导致GPU得不到连续性使用，整体速度特别慢。模型提速技巧·减少日志IO操作频率·使用pin_memory和num_workers·使用半精度训练·更好的显卡，更轻的模型另外也可以通过增大batch size提高epoch速度，但是收敛速度也会变慢，需要再适当升高学习率感谢 你reopen一下 我晚点加个开关把写log关掉好的看起来问题已经得到解决，如果有相关改进建议或仍存在问题可以重新打开 It seems this issue had been handled well, please be free to reopen if got any suggestion or new problems.训练时输入 
训练出来的音频带电音正常吗
issue was closed because it has been inactive for 14 days since being marked as stale. 
请问社区预先训练好的合成器如何用？
看教程part2.2中提供了社区预先训练好的合成器，请问是否用了这个就无需再自行训练ai？点击了第一个网盘链接里面的文件叫「train3_200k.pt」，请问这个文件下载下来后放在哪个文件夹内？需要修改文件名称吗？是直接放在/synthesizer/saved_models/mandarin/ 下就可以吗？我这个位置下有个叫「mandarin.pt」的文件，下载下来的直接替换mandarin.pt这个文件对吗？如是，那替换以后直接运行工具就可以使用了对吗？问题比较小白，望各位别介意，感谢。不需要改名字 在toolbox里面下拉选择即可好的，感谢告知看起来问题已经得到解决，如果有相关改进建议或仍存在问题可以重新打开 It seems this issue had been handled well, please be free to reopen if got any suggestion or new problems. 
您好，我是纯小白，碰上点问题，望得到指教
根据教程 一路操作到进入http://localhost:8080/网页，结果在【选择Synthesizer模型】这一栏无法找到任何模型我跳过了2.1（完全跳过），选则了2.2别的训练好的模型，这个模型不知道放在哪里，不知道是不是和这个有关系目前进度卡到了这里，希望大哥们能指点一下同样碰到了这个问题，求懂的老哥解答而且我还打不开python demo_toolbox.py -d <datasets_root>这个下载的模型放在 synthesizer\saved_models 路径中好耶，已经正常运行了，谢谢大佬感谢！可以找到模型了，按照37号帖子的方法编辑过后ceshi可以正常出音，作者的模型不能正常出音，我今天再研究一下问题在哪。。找到问题了，把symbols.py改回原状态，作者的模型可以正常出音，按37号的方法改掉，ceshi可以正常出音看起来问题已经得到解决，如果有相关改进建议或仍存在问题可以重新打开 
Exception: No synthesizer models found in synthesizer\saved_models
Arguments:    datasets_root:    E:\Download\aidatatang_200zh    enc_models_dir:   encoder\saved_models    syn_models_dir:   synthesizer\saved_models    voc_models_dir:   vocoder\saved_models    cpu:              False    seed:             None    no_mp3_support:   FalseTraceback (most recent call last):  File "E:\Download\MockingBird-main\MockingBird-main\demo_toolbox.py", line 43, in <module>    Toolbox(**vars(args))  File "E:\Download\MockingBird-main\MockingBird-main\toolbox\__init__.py", line 74, in __init__    self.reset_ui(enc_models_dir, syn_models_dir, voc_models_dir, seed)  File "E:\Download\MockingBird-main\MockingBird-main\toolbox\__init__.py", line 142, in reset_ui    self.ui.populate_models(encoder_models_dir, synthesizer_models_dir, vocoder_models_dir)  File "E:\Download\MockingBird-main\MockingBird-main\toolbox\ui.py", line 339, in populate_models    raise Exception("No synthesizer models found in %s" % synthesizer_models_dir)Exception: No synthesizer models found in synthesizer\saved_models请问各位，这个可能是哪里的问题我没学过Python，对其中的报错也不是很理解没有下载或训练好的模型放在 synthesizer\saved_models 路径中看起来问题已经得到解决，如果有相关改进建议或仍存在问题可以重新打开 It seems this issue had been handled well, please be free to reopen if got any suggestion or new problems.> 没有下载或训练好的模型放在 synthesizer\saved_models 路径中非常感谢 
切换模型之后会出现ui向右延申的问题
> > It seems this issue had been handled well, please be free to reopen if got any suggestion or new problems. 
小白提问 图形界面出不来
(yuyi) F:\clonevoice\MockingBird>python demo_toolbox.py -d .\samplesArguments:    datasets_root:    samples    enc_models_dir:   encoder\saved_models    syn_models_dir:   synthesizer\saved_models    voc_models_dir:   vocoder\saved_models    cpu:              False    seed:             None    no_mp3_support:   FalseFeel free to add your own. You can still use the toolbox by recording samples yourself.Traceback (most recent call last):  File "demo_toolbox.py", line 43, in <module>    Toolbox(**vars(args))  File "F:\clonevoice\MockingBird\toolbox\__init__.py", line 75, in __init__    self.setup_events()  File "F:\clonevoice\MockingBird\toolbox\__init__.py", line 112, in setup_events    self.ui.setup_audio_devices(Synthesizer.sample_rate)  File "F:\clonevoice\MockingBird\toolbox\ui.py", line 148, in setup_audio_devices    for device in sd.query_devices():  File "e:\ProgramData\Anaconda3\envs\yuyi\lib\site-packages\sounddevice.py", line 559, in query_devices    return DeviceList(query_devices(i)  File "e:\ProgramData\Anaconda3\envs\yuyi\lib\site-packages\sounddevice.py", line 559, in <genexpr>    return DeviceList(query_devices(i)  File "e:\ProgramData\Anaconda3\envs\yuyi\lib\site-packages\sounddevice.py", line 573, in query_devices    name = name_bytes.decode('utf-8')UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 6: invalid continuation byte翻一下issue，有一个相关的 
請益文件處理方法
，把所有的txt文件拿到，组成map好的！謝謝 
add alternative download source for dataset (google drive)
add alternative download sources for all three dataset (on Google drive). (for OpenSLR can be very slow for some region, ~200KB for me)Edited both English and Chinese version of README. 
Idk
Hmmwhats the changes？ 
Update launch.json
This was my first time creating a pull request. Really enjoyed it ,hope you will like it and approve it. Thank you. 
Hay Update training sample plots to readme
🙌🙌🙌🙌🙌- Привет я ещё не понимаю в этом spam 
Babysitter will come a new issue
~~~~Iris a stfntleocgr star100Okay 
Problem
Sometimes while taking it will just shut off.Did you record what actions you took?Нет я пока ещё ни в чем не разбираюсь 😔😪Отправлено из мобильной Почты Mail.ruвторник, 28 сентября 2021 г., 06:48 +0300 от ***@***.***  ***@***.***>:>Did you record what actions you took?>—>You are receiving this because you commented.>Reply to this email directly,  view it on GitHub , or  unsubscribe .看起来问题已经得到解决，如果有相关改进建议或仍存在问题可以重新打开 It seems this issue had been handled well, please be free to reopen if got any suggestion or new problems. 
是否能训练其他语言
例如 西班牙语，韩语等能不能用 Mozilla Common Voice 的数据直接训练如果是多说话人的数据应该就可以。可以的看起来问题已经得到解决，如果有相关改进建议或仍存在问题可以重新打开 It seems this issue had been handled well, please be free to reopen if got any suggestion or new problems.请后续关注 
 MockingBird
I know sometimes things may not always make sense to you right nowBut hey, what daddy always tell you?Straighten up little soldierStiffen up that upper lipWhat you crying about?You got me. 
Support tensorboard to trace the training of Synthesizer

载入音频文件点synthesize and vocoder报如下错误
 torch==1.9.1望各位大神指教！参考 #37 
Web server: Add latest changes
Add latest changes 
init server.py
init server.py 
No synthesizer models found in synthesizer\saved_models
Arguments:    datasets_root:    None    enc_models_dir:   encoder\saved_models    syn_models_dir:   synthesizer\saved_models    voc_models_dir:   vocoder\saved_models    cpu:              False    seed:             None    no_mp3_support:   FalseWarning: you did not pass a root directory for datasets as argument.The recognized datasets are:        LibriSpeech/dev-clean        LibriSpeech/dev-other        LibriSpeech/test-clean        LibriSpeech/test-other        LibriSpeech/train-clean-100        LibriSpeech/train-clean-360        LibriSpeech/train-other-500        LibriTTS/dev-clean        LibriTTS/dev-other        LibriTTS/test-clean        LibriTTS/test-other        LibriTTS/train-clean-100        LibriTTS/train-clean-360        LibriTTS/train-other-500        LJSpeech-1.1        VoxCeleb1/wav        VoxCeleb1/test_wav        VoxCeleb2/dev/aac        VoxCeleb2/test/aac        VCTK-Corpus/wav48        aidatatang_200zh/corpus/dev        aidatatang_200zh/corpus/test        aishell3/test/wav        magicdata/trainFeel free to add your own. You can still use the toolbox by recording samples yourself.Traceback (most recent call last):  File "demo_toolbox.py", line 43, in <module>    Toolbox(**vars(args))  File "D:\Develop\Project\PycharmProjects\Languages\MockingBird-main\toolbox\__init__.py", line 74, in __init__    self.reset_ui(enc_models_dir, syn_models_dir, voc_models_dir, seed)  File "D:\Develop\Project\PycharmProjects\Languages\MockingBird-main\toolbox\__init__.py", line 142, in reset_ui    self.ui.populate_models(encoder_models_dir, synthesizer_models_dir, vocoder_models_dir)  File "D:\Develop\Project\PycharmProjects\Languages\MockingBird-main\toolbox\ui.py", line 339, in populate_models    raise Exception("No synthesizer models found in %s" % synthesizer_models_dir)Exception: No synthesizer models found in synthesizer\saved_models没有下载模型。请按教程放到指定位置OK，多谢，发现是程序中synthesizer目录下少了saved_models目录 
执行pip install -r requirements.txt报错
执行pip install -r requirements.txt报错ERROR: Cannot install autopep8==1.5.6, flake8==3.9.0, pycodestyle==2.7.0 and python-language-server[all]==0.36.2 because these package versions have conflicting dependencies.The conflict is caused by:    The user requested pycodestyle==2.7.0    autopep8 1.5.6 depends on pycodestyle>=2.7.0    flake8 3.9.0 depends on pycodestyle<2.8.0 and >=2.7.0    python-language-server[all] 0.36.2 depends on pycodestyle<2.7.0 and >=2.6.0; extra == "all" 
UnicodeEncodeError: 'charmap' codec can't encode characters in position 7-13: character maps to <undefined>
> mp3把名字改成纯字母的试试？> > > 好的，我试了一下我用另外一个文件用数字名字的也遇见同样的问题这个问题是第一次遇到，你有用anaconda吗？怀疑依赖包有被修改的情况> > > mp3把名字改成纯字母的试试？> > > > > > > > 好的，我试了一下我用另外一个文件用数字名字的也遇见同样的问题> > 这个问题是第一次遇到，你有用anaconda吗？怀疑依赖包有被修改的情况我有用anacdona, 我换成用普通 python venv就可以了! 谢谢babysor 
输出的音频的音高出现问题
> 按照教程的步骤一步步完成了，也根据issue里的内容对symbols.py文件进行了更改，但输出音频的音高却像是方言一样奇怪，如上是输出时的设置。你用的音频看起来像是个音乐，请输入正常说话的人声试试?> > > > > > 按照教程的步骤一步步完成了，也根据issue里的内容对symbols.py文件进行了更改，但输出音频的音高却像是方言一样奇怪，如上是输出时的设置。> > 你用的音频看起来像是个音乐，请输入正常说话的人声试试?并不是音乐，而且最奇怪的是，我和一个同样使用此程序的人用这同一句采样，却只有我的出现了这种问题https://user-images.githubusercontent.com/64080472/133915292-bf2390ca-b222-42e4-8e0e-1d33abfb0495.mp4又用那个200k台湾口音的合成器试了一下，似乎并没有什么问题，是这个合成器设置的问题吗兄弟，日语确实没测试过你这个方言真牛逼尖锐的年轻女性声线几乎无法克隆，如萝莉音，到现在也无法解决，但是看频谱图是完美的，请问是vocoder的问题嘛> 尖锐的年轻女性声线几乎无法克隆，如萝莉音，到现在也无法解决，但是看频谱图是完美的，请问是vocoder的问题嘛我觉得是的，我用那个台湾口音的就没出现问题 
运行pip install -r requirements.txt的时候提示未能找到该文件
如题，在运行这一命令时出现如下错误 你所在路径是在root吗> > > 你所在路径是在root吗如果您指的是打开Anaconda后的第一个目录的话是这样的。请问应该进入哪个路径才对呢？> > 你所在路径是在root吗> > 如果您指的是打开Anaconda后的第一个目录的话是这样的。> 请问应该进入哪个路径才对呢？代码所在的目录 
服务器无声卡，请教是否可导出合成的音频？
服务器无声卡，CentOS7，请教是否可导出合成的音频？抱歉，点击toolbox的export按钮 即可导出~ 
使用AISHELL3训练合成器，最终不能正确读出文本框中的内容，而是隔一个合成一个，这是什么问题？
使用之前的代码时一直运行没有问题。使用AISHELL3自己训练了合成器，使用自己的模型时出现了不能正确读出文本框里面的文字的问题。例如输入“欢迎使用工具箱”只能合成“迎 用 具箱”的mel谱。希望尽快得到答复，谢谢～查看了SV2TTS/synthesizer中的train.txt文件，发现在预处理数据集时将拼音转化成了下图中的形式。每个字的拼音都是重复了一个，所以导致最终测试时隔一个合成一个的问题。请问这个要如何解决？<img width="1604" alt="E171392F-A59C-42E9-BDDB-2BF7C81ACD38" src="https://user-images.githubusercontent.com/45085467/133578815-97450072-2bde-409f-90f2-b976a4ee4a70.png">解决了！！！由于aishell3的数据预处理延用的是aidatatang的，所以导致读取和转录的时候每个字的拼音出现了两次！在preprocess_speaker.py中新增AISHELL3的预处理函数，并将res列表中的数据隔一个读取一个即可～<img width="585" alt="571D848E-23C7-4D95-A0CA-5B6034026DC0" src="https://user-images.githubusercontent.com/45085467/133608437-d608863e-f801-4980-b537-01ee9de88ae5.png">作者使用aishell3数据集时是否出现这个问题？> 作者使用aishell3数据集时是否出现这个问题？你用的是我那个biaobei branch吗> 作者使用aishell3数据集时是否出现这个问题？> 你用的是我那个biaobei branch吗没有选择分支诶，我看数据那里都是使用的general，AISHELL的数据集要用标贝的branch处理是嘛～> > 作者使用aishell3数据集时是否出现这个问题？> > 你用的是我那个biaobei branch吗> > 没有选择分支诶，我看数据那里都是使用的general，AISHELL的数据集要用标贝的branch处理是嘛～不对不对，我看错了，你用aishell就正常用aishell，我处理一下代码最新代码已经修复。 
请问百度网盘下的训练好的模型怎么使用呢
要放到什么目录下呢, 怎么启动呢.Copy to synthesizer\saved_models > Copy to synthesizer\saved_modelsThank you 
Support train hifigan
Add code that you can train your own hifigan. fix bug in vocoder/vocoder_dataset.py 
这是Real-Time-Voice-Cloning项目的复刻版?
看起来是它的复刻版.从原始库抄过来也不fork, 也不说明出处, 还起了个名字 MockingBird, 建议改为 CopyBird 或者 叫 ChineseLocalizationBird. > 从原始库抄过来也不fork, 也不说明出处, 还起了个名字 MockingBird, 建议改为 CopyBird 或者 叫 ChineseLocalizationBird.所有Readme都注明了，也说明了区别。 
载入文件失败，载入的是mp3格式的，但是没有反应。
speaker utterance 都是灰的怎么弄呀并且出来的都是杂音> 我这个dataset speaker utterance 都是灰的怎么弄呀> 我这个dataset speaker utterance 都是灰的怎么弄呀没有对的数据集，你可以参数指向你的数据集或者任意一个文件夹，然后把音频拖进去> > > 我这个dataset speaker utterance 都是灰的怎么弄呀> > > > > > 我这个dataset speaker utterance 都是灰的怎么弄呀> > 没有对的数据集，你可以参数指向你的数据集或者任意一个文件夹，然后把音频拖进去具体是那一步呢，求教> > > > > 我这个dataset speaker utterance 都是灰的怎么弄呀> > > > > > > > > > > 我这个dataset speaker utterance 都是灰的怎么弄呀> > > > 没有对的数据集，你可以参数指向你的数据集或者任意一个文件夹，然后把音频拖进去> > 具体是那一步呢，求教我好像漏了一步，没下数据集> > > > > > > 我这个dataset speaker utterance 都是灰的怎么弄呀> > > > > > > > > > > > > > > > 我这个dataset speaker utterance 都是灰的怎么弄呀> > > > > > 没有对的数据集，你可以参数指向你的数据集或者任意一个文件夹，然后把音频拖进去> > > > 具体是那一步呢，求教> > you do not have any of the recognized datasets in C:\Users\Administrator\Desktop\MockingBird-main\data.你下载了什么数据集？> 你下载了什么数据集？> 正在下aidatatang，太大了18G下载好了，可以用了吗 
合成失败
你好 我使用sishell3数据集来进行合成 ，使用的是你提供的预训练模型出现如下错误。请问如何修改呢‘？Traceback (most recent call last):  File "G:\tj\anaconda\envs\gpu2\lib\tarfile.py", line 187, in nti    n = int(s.strip() or "0", 8)ValueError: invalid literal for int() with base 8: 'build_te'During handling of the above exception, another exception occurred:Traceback (most recent call last):  File "G:\tj\anaconda\envs\gpu2\lib\tarfile.py", line 2289, in next    tarinfo = self.tarinfo.fromtarfile(self)  File "G:\tj\anaconda\envs\gpu2\lib\tarfile.py", line 1095, in fromtarfile    obj = cls.frombuf(buf, tarfile.encoding, tarfile.errors)  File "G:\tj\anaconda\envs\gpu2\lib\tarfile.py", line 1037, in frombuf    chksum = nti(buf[148:156])  File "G:\tj\anaconda\envs\gpu2\lib\tarfile.py", line 189, in nti    raise InvalidHeaderError("invalid header")tarfile.InvalidHeaderError: invalid headerDuring handling of the above exception, another exception occurred:Traceback (most recent call last):  File "G:\tj\anaconda\envs\gpu2\lib\site-packages\torch\serialization.py", line 555, in _load    return legacy_load(f)  File "G:\tj\anaconda\envs\gpu2\lib\site-packages\torch\serialization.py", line 466, in legacy_load    with closing(tarfile.open(fileobj=f, mode='r:', format=tarfile.PAX_FORMAT)) as tar, \  File "G:\tj\anaconda\envs\gpu2\lib\tarfile.py", line 1593, in open    return func(name, filemode, fileobj, **kwargs)  File "G:\tj\anaconda\envs\gpu2\lib\tarfile.py", line 1623, in taropen    return cls(name, mode, fileobj, **kwargs)  File "G:\tj\anaconda\envs\gpu2\lib\tarfile.py", line 1486, in __init__    self.firstmember = self.next()  File "G:\tj\anaconda\envs\gpu2\lib\tarfile.py", line 2301, in next    raise ReadError(str(e))tarfile.ReadError: invalid headerDuring handling of the above exception, another exception occurred:Traceback (most recent call last):  File "G:\voice cloning\MockingBird-main-version2\toolbox\__init__.py", line 129, in <lambda>    func = lambda: self.synthesize() or self.vocode()  File "G:\voice cloning\MockingBird-main-version2\toolbox\__init__.py", line 243, in synthesize    specs = self.synthesizer.synthesize_spectrograms(texts, embeds)  File "G:\voice cloning\MockingBird-main-version2\synthesizer\inference.py", line 87, in synthesize_spectrograms    self.load()  File "G:\voice cloning\MockingBird-main-version2\synthesizer\inference.py", line 65, in load    self._model.load(self.model_fpath)  File "G:\voice cloning\MockingBird-main-version2\synthesizer\models\tacotron.py", line 496, in load    checkpoint = torch.load(str(path), map_location=device)  File "G:\tj\anaconda\envs\gpu2\lib\site-packages\torch\serialization.py", line 386, in load    return _load(f, map_location, pickle_module, **pickle_load_args)  File "G:\tj\anaconda\envs\gpu2\lib\site-packages\torch\serialization.py", line 559, in _load    raise RuntimeError("{} is a zip archive (did you mean to use torch.jit.load()?)".format(f.name))RuntimeError: synthesizer\saved_models\mandarin\ceshi.pt is a zip archive (did you mean to use torch.jit.load()?)继续点击合成出来都是杂音。目前看跟pytorch版本有关 你的是哪个版本1.2.0> 1.2.0試試1.9.0? 
UnicodeEncodeError: 'cp950' codec can't encode character '\u2f24' in position 153: illegal multibyte sequence
在執行  時遇到的錯誤，上網查詢一下發現是編碼問題，請問我可以更改程式裡的哪部分代碼去解決這個問題呢?![WeChat 查看  執行時停止在哪一行2. 根據上述查詢到的去尋找  中相同行數的資料3. 將沒轉換好的字手動轉換 （我這邊是「大」這個字沒轉好，所以改完後用 搜尋其他「大」字一併做更改）這是我的解決方法，我將關閉這個 issue> 問題已經解決，透過以下方法：> > 1. 查看  執行時停止在哪一行> 2. 根據上述查詢到的去尋找  中相同行數的資料> 3. 將沒轉換好的字手動轉換 （我這邊是「大」這個字沒轉好，所以改完後用 搜尋其他「大」字一併做更改）> > 這是我的解決方法，我將關閉這個 issue是因为用了繁体字吗？> 是因为用了繁体字吗？感覺不是，只有其中兩筆資料的「大」字在預處理時沒有轉換好，可能是格式問題但非常奇怪，這兩筆資料中只有「大」字沒被轉換，其他字都有成功轉換 
关于弃用百度网盘的提案
因为限制我强烈推荐训练后的数据集丢到 ‘Google网盘’ 、‘Dropbox’、及‘MEGA’ 甚至基于 IPFS的存储服务中.也推荐采用 Resilio Sync来同步数据集这样方便广大的国内外开发人员训练更好的模型或者基于模型进行实验这里是指预训练模型吧？数据集的话应该公开可以下载> 这里是指预训练模型吧？数据集的话应该公开可以下载对。百度网盘。。对国外用户不是很友好。将补充google 云盘+百度云盘的模型，可以关注一下今天更新的 
encoder加载速度很慢以及输出是杂音的问题
请问一下各位大佬，我是按照Quick Start (Newbie)一步一步来的，以前并没有怎么接触过python，目前安装的是python3.9+anaconda3.pytorch版本是1.9 cuda10.2，显卡3070ti，cuda驱动都安好了import torch;, 回车以后没有反应， torch.cuda.is_available() 以后会出来一个Trueffmpeg安装的是ffmpeg-n4.4-83-gb1f2d203c0-win64-gpl-4.4.zip这个版本运行pip install -r requirements.txt 来安装剩余的必要包。安装 webrtcvad 用 pip install webrtcvad-wheels。这两步都没有错误提示模型使用的miven大佬的包python demo_toolbox.py -d 第11行的symbols 改为：_characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz12340!\'(),-.:;? '这个解决方法已经试过了但是还是杂音经过研究，发现卸载pytorch1.9 cuda10.2版本，重新安装pytorch cpu版本以后加载encoder速度正常，生成的声音也不是杂音了。请问一下是程序对于cuda的支持有问题吗还是我本地cuda配置有问题又测试了一下，新建虚拟环境，安装CUDA11.4，pytorch 1.90版本好像是无法使用，pytorch更新到最新每日版以后可以正常使用，就是加载速度比cpu慢一点，大概需要2-3秒，cpu版本的加载速度只需要50ms。生成声音正常估计是cuda10.2版本有一些bug听着像你的显卡驱动也有问题，试着升级一下？ @576272658 > 听着像你的显卡驱动也有问题，试着升级一下？ @576272658驱动是最新版的，感觉可能是cuda10.2对3070ti支持还不太好 
Support "hifigan vocoder" for realtime synthesis
Add a hifigan vocoder trained on aidatatang.This vocoder can synthesis speech in realtime. 
Temp to support Mozilla Common Voice dataset
You **HAVE TO** rename Mozilla Common Voice to the below data structure- {dataset_name}/- - zh-TW/- - - clips/- - - - xxx.mp3- - - xxx.tsv* TODO: Currently only support , will also support other data like  or  in the future* TODO: Fix "no wordS" error when processing Mozilla Common Voice (cause only  for now) 
Program not stable , easy to crash
I am using wav files,it usually crashes after 4 or 5 times "synthesize and vocode".Btw I am in no mp3 support mode and wav files are 3-5 Mb size.Cmd message:"python: src/hostapi/alsa/pa_linux_alsa.c:3641: PaAlsaStreamComponent_BeginPolling: Assertion 
plz provide google drive link for pretrained model
the baidu netdisk download is terrible for ordinary users.You can try thishttps://github.com/miven/Realtime-Voice-Clone-Chinese/releases/tag/0.1> You can try this> is life savior,I have a questiion tho.Is the pretrained model folder structure the same as the original repo.i.e. create a folder name saved_models and a folder pretrained,then put the pt file in it.I get it now,it will search subfolders..After opening the GUI,I get the warning "you did not pass a root directory for datasets as argument" 
Support new dataset "biaobei" BZNSYP High quality single speaker for …
…Chinese有支持biaobei标贝数据集的正确版本吧。最好整合到主版本里面去。> 有支持biaobei标贝数据集的正确版本吧。最好整合到主版本里面去。因为biaobei是单人语音 导致模型不可用 所以不打算整合那么，单独支持标贝数据集，可行不？此分支已有的可以如何改进呢？ 
Windows11 配置FFmpeg环境变量没有用
-version提示找不到该命令,重启电脑也没用. 
Add Spanish language trainset support
Now I have a lot of demand for Spanish dubbing, so I hope someone add the support of Spanish language training set. Ahora tengo mucha demanda de doblaje en español, así que espero que alguien añada el soporte del set de formación en español.I was planning to support multi-language. If you were familiar with python, I could show you how you may try to support by changing couple files in this repo.> I was planning to support multi-language. If you were familiar with python, I could show you how you may try to support by changing couple files in this repo.Hi. My friend and I are trying to do in Turkish. Could you help us a little?> > I was planning to support multi-language. If you were familiar with python, I could show you how you may try to support by changing couple files in this repo.> > Hi. My friend and I are trying to do in Turkish. Could you help us a little?You could email me and setup a topic thread> > > I was planning to support multi-language. If you were familiar with python, I could show you how you may try to support by changing couple files in this repo.> > > > > > Hi. My friend and I are trying to do in Turkish. Could you help us a little?> > You could email me and setup a topic threadI sent an email titled "Help for Turkish Voice Cloning". I'll be glad if you could help us.> > > > I was planning to support multi-language. If you were familiar with python, I could show you how you may try to support by changing couple files in this repo.> > > > > > > > > Hi. My friend and I are trying to do in Turkish. Could you help us a little?> > > > > > You could email me and setup a topic thread> > I sent an email titled "Help for Turkish Voice Cloning". I'll be glad if you could help us.Did u receive my reply? 
M1芯片下安装包报错的暂时性解决方案
本人设备m1 macbookair  python3.9在使用pip安装包时疯狂报错，通过其他方法安装完运行时报错的部分信息有“have:arm need:x86_64"。怀疑与arm兼容性有关。解决方法：1.安装x86版本的包使用rosetta2运行在所需安装包命令前加上arch -x86_64如arch -x86_64 pip install -r requirements.txt最后启动时使用arch -x86_64 python demo_toolbox.py或2.安装windows虚拟机～-～pd17真香> 本人设备m1 macbookair python3.9> 在使用pip安装包时疯狂报错，通过其他方法安装完运行时报错的部分信息有“have:arm need:x86_64"。怀疑与arm兼容性有关。> 解决方法：> 1.安装x86版本的包使用rosetta2运行> 在所需安装包命令前加上arch -x86_64> 如arch -x86_64 pip install -r requirements.txt> 最后启动时使用arch -x86_64 python demo_toolbox.py> 或> 2.安装windows虚拟机> ～-～pd17真香m1 训练速度大概多少呀> > 本人设备m1 macbookair python3.9> > 在使用pip安装包时疯狂报错，通过其他方法安装完运行时报错的部分信息有“have:arm need:x86_64"。怀疑与arm兼容性有关。> > 解决方法：> > 1.安装x86版本的包使用rosetta2运行> > 在所需安装包命令前加上arch -x86_64> > 如arch -x86_64 pip install -r requirements.txt> > 最后启动时使用arch -x86_64 python demo_toolbox.py> > 或> > 2.安装windows虚拟机> > ～-～pd17真香> > m1 训练速度大概多少呀我用的网盘的数据集，没有在我的电脑上训练过。m1跑训练模型大概速度可以参考b站这个视频https://www.bilibili.com/video/BV1s54y167Cp这样安装也只是用的转译，最好还是安装native的库不然对效率有很大影响目前我这边测试（M1 MacBook Pro (13-inch, M1, 2020) Big Sur 11.5.1 Python 3.9 M1 Native），使用venv创建的虚拟环境，在网上使用pip来进行安装numpy sklearn的方法都不好使 只是想试试的不用折腾这样安装了最后还是conda安装一遍完成（主要是llvmlite numba sklearn numpy这几个）其他的依赖或多或少需要稍微折腾一下 不过大问题解决了其他都好说pyqt5添加清华源后，按以下步骤安装conda update --allconda install --yes qt pyqtPytorch：conda install -c conda-forge pytorch  sounddevice：conda install -c conda-forge python-sounddevice后期还手动装了几个包 不确定是我哪里改错了 反正一把梭 建议你看到了就跑一遍umap: conda install -c conda-forge umap-learn inflect  unidecode另外可能会遇到libsndfile 找不到的问题brew安装brew install libsndfile完成后把lib目录下的文件，全部复制到site-package目录的/_soundfile_data/    目录下 @ZoidbergPi missed some details, but can't remember, so I'm just pasting the history of my commands. Good luck.@mr-m0nst3r Excuse me, what's the macOS version run on your Mac, and the python version. Thank you. 
使用预训练集生成失败求助
想要尝试使用预训练集在本地测试效果，使用gui录音后点击synthesize & vocode生成的声音只有沙沙声。完整步骤1\ffmpeg已安装，创建虚拟环境 2\下载预训练文件，将encoder，synthesizer、vocoder三个文件夹覆盖粘贴至MockingBird3\运行gui 之后表现为，录音正常，生成结果只有沙沙声，是哪里错了吗？#37 试试这个感谢，已经解决。准备关闭issue了，另外能否解释一下出错原因？ 
dataset,speaker,utterance都是灰的，选不了
录音录不上，提示warning: you did not pass aroot directory for datasets as argument随便输入一个目录，用于录音存储 
使用aishell3數據集，關於Warning: you do not have any of the recognized datasets的問題
請問我在輸入  後卻沒有像視頻一樣預先載入訓練好的Dataset是哪邊出狀況了嗎?(我的檔案存放路徑: aidatatang_200zh 數據集發生一樣問題的，可以參考 #58。Fix in main branch, please verify好勒謝謝作者，我試試，這個issues先關閉我用magicdata的dev数据集也遇到了相同的问题，报错找不到可识别的数据集，但没加载数据集也能用，所以这一步到底有什么用呢？---我理解的是数据集只是用来训练；在GUI使用界面，数据集不是必须的，只是方便从数据集里选取输入音频，如果只是导入想要的外部音频，可以不用数据集-d <path>，是这样吗？ 
加载音频程序未响应
cuda 10.2python 3.9.6pytorch 1.9.0已添加训练好的数据集D:\Realtime-Voice-Clone-Chinese-main>python demo_toolbox.py -d D:\Realtime-Voice-Clone-Chinese-main --no_mp3_supportArguments:    datasets_root:    D:\Realtime-Voice-Clone-Chinese-main    enc_models_dir:   encoder\saved_models    syn_models_dir:   synthesizer\saved_models    voc_models_dir:   vocoder\saved_models    cpu:              False    seed:             None    no_mp3_support:   你的数据集都在这个文件夹下吗？通过重新安装pytorch解决了> > 你的数据集都在这个文件夹下吗？> > 通过重新安装pytorch解决了請問你也是訓練完模型後，鍵入  打開toolbox時沒有如視頻一樣加載數據集嗎如果是這樣你是安裝什麼版本的 pytorch 呢?> > > 你的数据集都在这个文件夹下吗？> > > > > > 通过重新安装pytorch解决了> > 請問你也是訓練完模型後，鍵入  打開toolbox時沒有如視頻一樣加載數據集嗎> 如果是這樣你是安裝什麼版本的 pytorch 呢?我用的是别人已经训练好的模型（README里面的Baidu Yun链接）， aishell3 數據集，在跑這段程式碼時沒有成功加載進 toolbox，不知道是不是代碼要修改一下才能適應 aishell3數據集，我再去試試 
我什么我使用首页提供的模型生成出来的音频都是杂音呢
我什么我使用首页提供的模型生成出来的音频都是杂音呢是不是在界面上没选择好模型> 是不是在界面上没选择好模型我选的是ceshi那个Duplicate of #37 
200zh数据集解压后，第一步预处理报错
(RVCC) D:\Realtime-Voice-Clone-Chinese-main\Realtime-Voice-Clone-Chinese-main>python synthesizer_preprocess_audio.py D:\data\Arguments:    datasets_root:   D:\data    out_dir:         D:\data\SV2TTS\synthesizer    n_processes:     None    skip_existing:   False    hparams:    no_alignments:   False    dataset:         aidatatang_200zhUsing data from:    D:\data\aidatatang_200zh\corpus\trainaidatatang_200zh: 100%|████████████████████████████████████████████████████████| 420/420 [01:02<00:00,  6.71speakers/s]The dataset consists of 0 utterances, 0 mel frames, 0 audio timesteps (0.00 hours).Traceback (most recent call last):  File "synthesizer_preprocess_audio.py", line 64, in <module>    preprocess_dataset(**vars(args))  File "D:\Realtime-Voice-Clone-Chinese-main\Realtime-Voice-Clone-Chinese-main\synthesizer\preprocess.py", line 76, in preprocess_dataset    print("Max input length (text chars): %d" % max(len(m[5]) for m in metadata))ValueError: max() arg is an empty sequence这个数据在哪里下载啊www.openslr.org/62/我也遇到了同样的问题> 我也遇到了同样的问题一般就是数据路径有误同样问题,解决了吗?怎么解决的?数据集的内容要解压吗?> aidatatang_200zh\corpus里的tar.gz格式内容要解压吗?> 同样问题,解决了吗?怎么解决的?数据集的内容要解压吗?> > > aidatatang_200zh\corpus> > 里的tar.gz格式内容要解压吗?对，全都解压遇到了同样的问题，我把corpus下所有压缩文件都解压为单独文件夹了，还是报错(yuyi_copy) E:\clonevoice\MockingBird>python pre.py F:\dataset\Using data from:    F:\dataset\aidatatang_200zh\corpus\trainaidatatang_200zh: 100%|████████████████████████████████████████████████████████| 185/185 [00:23<00:00,  7.77speakers/s]The dataset consists of 0 utterances, 0 mel frames, 0 audio timesteps (0.00 hours).Traceback (most recent call last):  File "pre.py", line 73, in <module>    preprocess_dataset(**vars(args))  File "E:\clonevoice\MockingBird\synthesizer\preprocess.py", line 82, in preprocess_dataset    print("Max input length (text chars): %d" % max(len(m[5]) for m in metadata))ValueError: max() arg is an empty 请问楼主解决了吗 
在生成录音时闪退
Building Wave-RNNTrainable Parameters: 4.481MLoading model weights at vocoder/saved_models/pretrained/pretrained.ptpython: src/hostapi/alsa/pa_linux_alsa.c:3641: PaAlsaStreamComponent_BeginPolling: Assertion 
请问如何恰当调整CPU和GPU的占用率呢 
请教一下，GPU和CPU利用率只有13%左右，该怎么调整训练参数？ 你是在train的阶段了吗？其实吃的很多是gpu内存是的，在训练的时候，只有50%-60%的GPU内存占用，不知道是出于系统保护还是程序设置emmmmm（谢谢回复）已更新到README。 可以调整 batchsize 
训练模型时 不调用GPU
但是又有了新的问题诶...> 成功调用了 但是又有了新的问题诶...看起来像手动停止了训练，你可以加微信群咨询> > > 成功调用了 但是又有了新的问题诶...> > 看起来像手动停止了训练，你可以加微信群咨询在哪里加群？> > > > > 成功调用了 但是又有了新的问题诶...> > > > > > 看起来像手动停止了训练，你可以加微信群咨询> > 已解决问一下怎么解决的> 已解决同样用不了GPU 可以给个群吗 
deploy as webservice
is there anyway to deploy it as  service ,we can call it remoteI have two computer~Great to hear this feedback. I’m planning this but I currently are focusing on other things. Do you want to have a try?I  am somekind of code idiot.scrape is the best i can do 
torch.Size的问题
有个问题，他显示Exception:Error(s) in loading state_dict for Tacotron : Size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70,512]) from checkpoint, the shape in current model is torch.Size([75,512])Duplicate of #37 
能出一个视频教程嘛
本人是一个小白，真的尝试去做了，好在一些安装下载配置别人有出教程，但不同人出的并不连贯，让我产生一种莫名其妙的感觉，很多东西在于细节，也许他所讲授的方法适用于这个特定的问题，但并不适用于项目，拜托了求网友贡献了一份…目前因为代码也在改进中 教程会有点跟不上 视频需要经常更新视频r地址地哪里啊？网友正在制作中，很快会发布了> 网友正在制作中，很快会发布了需要的话我可以帮忙制作视频，我可以熟练使用fcpx视频出来之后，能通知我一下吗 哈哈同求，小白，需要基础的安装视频以及操作视频，或者稍微详细点操作文档！亦或者有靓仔辅助可以有偿付些费用社区视频教程：https://www.bilibili.com/video/BV1DL4y1q7VL@FangPengbo @aiwibadi @lcp580 看看是否可以做个更好得  @kulu2001 
kiwisolver是个什么东西。。。。
> Traceback (most recent call last):  File "D:\code\Realtime-Voice-Clone-Chinese\demo_toolbox.py", line 2, in <module>    from toolbox import Toolbox  File "D:\code\Realtime-Voice-Clone-Chinese\toolbox\__init__.py", line 1, in <module>    from toolbox.ui import UI  File "D:\code\Realtime-Voice-Clone-Chinese\toolbox\ui.py", line 1, in <module>    import matplotlib.pyplot as plt  File "D:\software\install place\python3\lib\site-packages\matplotlib\__init__.py", line 157, in <module>    _check_versions()  File "D:\software\install place\python3\lib\site-packages\matplotlib\__init__.py", line 151, in _check_versions    module = importlib.import_module(modname)  File "D:\software\install place\python3\lib\importlib\__init__.py", line 127, in import_module    return _bootstrap._gcd_import(name[level:], package, level)ModuleNotFoundError: No module named 'kiwisolver'matplotlib没装好 
在 Preprocess the embeddings 時自動關機
有人有跟我一樣的問題嗎，剛執行  101度 小心着火啊 
Where can I download aidatatang_200zh dataset?
Where can I download aidatatang_200zh dataset?http://www.openslr.org/62/> 
Suggestion! Maybe you can list the basic hardware requirements of this project.
Just as the title.https://vaj2fgg8yn.feishu.cn/docs/doccn7kAbr3SJz0KM0SIDJ0Xnhd#JHOpmf 
bilibili 演示视频已消失
> 演示视频已B站下架....> 考虑到前台封面的展示，黑屏，边框，角标等情况，建议进行更换 查阅 哔哩哔哩创作公约 了解更多 @babysor 可以将视频重新发布一下嘛。或者上传到github或者百度云> > 莫名其妙的原因:> 已退回 考虑到前台封面的展示，黑屏，边框，角标等情况，建议进行更换 查阅 哔哩哔哩创作公约 了解更多很常见的，就是封面不符合规范。换一个封面即可再提交，最简单的单色背景加文字都行，注意别撞色（大红大紫）。实在不会必剪上做个封面也成。毕竟这视频昨天还是系统推给我的，有系统推荐流量的，刚在pad上看了，上电脑就肥肠抱歉了。顺便说一句，站内那两条私信可以忽略了。> > > > > 莫名其妙的原因:> > 已退回 考虑到前台封面的展示，黑屏，边框，角标等情况，建议进行更换 查阅 哔哩哔哩创作公约 了解更多> > 很常见的，就是封面不符合规范。换一个封面即可再提交，最简单的单色背景加文字都行，注意别撞色（大红大紫）。实在不会必剪上做个封面也成。毕竟这视频昨天还是系统推给我的，有系统推荐流量的，刚在pad上看了，上电脑就肥肠抱歉了。顺便说一句，站内那两条私信可以忽略了。谢谢老哥，经过修改，视频已经通过。 
使用百度网盘最新预训练模型，spectrogram不正常，只有两秒杂音
中的解决方法> 我刚才同样遇到了杂音问题，你可以试试参考#37 中的解决方法谢谢！已经解决 
这次训练一半会出现这个EOFError: Ran out of input，怎么回事  PermissionError: [WinError 5] 拒绝访问。
建议查看是否有进程冲突 
如何使用训练好的数据集呢
如题~我将百度云下载好的训练结果放在E:\Voice\trainmodel，执行python demo_toolbox.py -d E:\Voice\trainmodelc好像并不能成功运行下载好的数据集是 .pt 文件，把那些里面的什么 saved_models 文件夹分别丢到项目对应文件夹里面 
Backend Qt5Agg is interactive backend. Turning interactive mode on.
直接运行没有问题，但是debug demo_toolbox.py时 报错：Traceback (most recent call last):  File "D:\work\python\ide\pycharm\PyCharm 2020.1.2\plugins\python\helpers\pydev\pydevd.py", line 1438, in _exec    pydev_imports.execfile(file, globals, locals)  # execute the script  File "D:\work\python\ide\pycharm\PyCharm 2020.1.2\plugins\python\helpers\pydev\_pydev_imps\_pydev_execfile.py", line 18, in execfile    exec(compile(contents+"\n", file, 'exec'), glob, loc)  File "E:/instance/tts/Realtime-Voice-Clone-Chinese-main/demo_toolbox.py", line 43, in <module>    Toolbox(**vars(args))  File "E:\instance\tts\Realtime-Voice-Clone-Chinese-main\toolbox\__init__.py", line 75, in __init__    self.ui = UI()  File "E:\instance\tts\Realtime-Voice-Clone-Chinese-main\toolbox\ui.py", line 450, in __init__    self.projections_layout.addWidget(FigureCanvas(fig))TypeError: addWidget(self, QWidget, stretch: int = 0, alignment: Union[Qt.Alignment, Qt.AlignmentFlag] = Qt.Alignment()): argument 1 has unexpected type 'FigureCanvasQTAgg'Backend Qt5Agg is interactive backend. Turning interactive mode on.环境问题，将python从3.7 换成3.8 后解决了 
更新 README
有 Fork 新的檔案再更新過的，順便把英文版的也補齊 Install webrtcvad @babysor 雖然通過了，但是不是沒有 Merge 進去，是我這邊哪裡做錯了嗎我试一下 
sounddevice报错问题
在win10默认情况下系统编码格式为gbk，在运行demo_toolbox.py时会报错： 打开 移动到573行，有相关报错的issue，更改为 后错误变成： 运行 就没报错了 再次运行demo_toolbox.py就能正常打开Merged to 常见错误 
声音样本
大佬想问下若声音样本是歌曲的话，能不能克隆出其声音主人的声音出来？这个比较困难，最好能去掉背景的音乐，只提取人声可以试着用SpleeterGui分离人声 
是否能让生成出来的语音语速变慢
用了readme网友的数据集，感觉语速过快有点不自然> 用了readme网友的数据集，感觉语速过快有点不自然音频降速不变调可以参考pyrubberband，https://github.com/bmcfee/pyrubberband如果你这边用aidatatang的数据慢慢跑，后面会比较自然 
使用预训练模型获得了奇怪的mel spectrogram和杂音
但声音还是杂音~~你是不是没有加载数据集?~~草 是不是因为修bug导致了新bug我加载了以前训练的一个模型  > ~你是不是没有加载数据集?~> 草 是不是因为修bug导致了新bug> 我加载了以前训练的一个模型> 哈 最新commit里面symbols的文件可以考虑回滚一下，这个会破坏训练模型的兼容的这个东西怎么说呢，多点点这个毛病就没有了所以我就没太在意它。让我看看嗯，好了50%用ceshi来做可以成功，但有时候有些分句还是杂音用mandarin就基本还是issue里这个情况emm这么说也不对，换了点东西，现在ceshi的成功率还是挺高的，但是mandarin出现了空句子的问题，大概跟预期结果比较相似了。 
Update README-CN.md
fix : 解决在未安装vs时会报错"Microsoft Visual C++ 14.0 is required"的问题 
將 issues#21當中提及的解決方法更新到 readme
中文和英文版的 readme 都有做更新，請作者再看一下用詞與內容，特別是我有找文章與影片來加以補充說明的地方，請作者再確認一下，謝謝。 
RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`
一般出现这种问题都是cuda版本的问题..你用了lts版本吗我用的10.2版本啊      Its 是什么cuda版本更新后是不是就修复了？看起来问题已经得到解决，如果有相关改进建议或仍存在问题可以重新打开 
训练模型时显存爆了
 能不能提供一个调batch_size的参数? 我目前用的显卡显存只有4G(GTX1050Ti)，默认参数正常训练时经常爆掉显存....https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/664你可以找找这个https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/700嗯嗯,如上所示,我之前用940MX, batch size在2是可以跑的,你这个话可以试下4.> 嗯嗯,如上所示,我之前用940MX, batch size在2是可以跑的,你这个话可以试下4.我试着通过修改部分文件将batch_size调到6，跑的很稳定。> > 嗯嗯,如上所示,我之前用940MX, batch size在2是可以跑的,你这个话可以试下4.> > 我试着通过修改部分文件将batch_size调到6，跑的很稳定。可以請問是修改什麼文件呢，我看了上面的引用，研究了半天看不大懂。> > > 嗯嗯,如上所示,我之前用940MX, batch size在2是可以跑的,你这个话可以试下4.> > > > > > 我试着通过修改部分文件将batch_size调到6，跑的很稳定。> > 可以請問是修改什麼文件呢，我看了上面的引用，研究了半天看不大懂。需要修改 [./synthesizer/hparams.py 这一部分好的我試試，謝謝!> 嗯嗯,如上所示,我之前用940MX, batch size在2是可以跑的,你这个话可以试下4.請問訓練 vocoder 時也遇到  同樣更改batch size來解決嗎> > 嗯嗯,如上所示,我之前用940MX, batch size在2是可以跑的,你这个话可以试下4.> > 請問訓練 vocoder 時也遇到  同樣更改batch size來解決嗎我把下面代码的 12 to 2 暂时不报错了文件:yourmainfolder\synthesizer\hparams.py中的 好勒，謝謝你已经update到 Readme 
LibriSpeech alignments?
(base) F:\Realtime-Voice-Clone-Chinese-main>python synthesizer_preprocess_audio.py "F:\Realtime-Voice-Clone-Chinese-main/data1"Arguments:    datasets_root:   F:\Realtime-Voice-Clone-Chinese-main\data1    out_dir:         F:\Realtime-Voice-Clone-Chinese-main\data1\SV2TTS\synthesizer    n_processes:     None    skip_existing:   False    hparams:    no_alignments:   False    dataset:         aidatatang_200zhUsing data from:    F:\Realtime-Voice-Clone-Chinese-main\data1\aidatatang_200zh\corpus\trainaidatatang_200zh: 100%|████████████████████████████████████████████████████████| 420/420 [02:47<00:00,  2.51speakers/s]The dataset consists of 0 utterances, 0 mel frames, 0 audio timesteps (0.00 hours).Traceback (most recent call last):  File "synthesizer_preprocess_audio.py", line 64, in <module>    preprocess_dataset(**vars(args))  File "F:\Realtime-Voice-Clone-Chinese-main\synthesizer\preprocess.py", line 76, in preprocess_dataset    print("Max input length (text chars): %d" % max(len(m[5]) for m in metadata))ValueError: max() arg is an empty 你的意思是解压所有压缩包吗？解压train目录下的压缩包可解决对，别让 wav移出来 
在文字框写了文字，可是出來的是其它聲音，是不是文字框bug了
谁能解决看demo影片好像是要直接用拼音输入拼音也不行，出现的声音根本不是文字框里的，> 输入拼音也不行，出现的声音根本不是文字框里的，你有訓練完模型對吧没有訓练了一半，试了一下> 訓练了一半，试了一下如果拿百度雲上的模型來用，看一下 #24 討論的可能是答案那可能要訓練完看看了那我試一下，训练完看看> 訓练了一半，试了一下1. 试着用更长的句子2. 你训练的时候,可以看下synthesizer目录下对应的plot文件夹,看看是否出现attention 对齐线,如下所示:https://user-images.githubusercontent.com/7423248/128587252-f669f05a-f411-4811-8784-222156ea5e9d.png如果没有,说明还没训练到位好，我试试，这个问题先不关闭已经修复了，可以拉最新代码试一下 @wangkewk 
使用百度云上的模型，训练播放后都是杂音
#### 环境Windows 10Python 3.7#### 描述百度云的pt模型放入 后， 可执行，但产生结果都是杂音，中文和拼音都不太行#### synthesizer_preprocess_audio.py E:\data\aidatatang_200zhArguments:    datasets_root:   E:\data\aidatatang_200zh    out_dir:         E:\data\aidatatang_200zh\SV2TTS\synthesizer    n_processes:     None    skip_existing:   False    hparams:    no_alignments:   False    dataset:         aidatatang_200zhUsing data from:    E:\data\aidatatang_200zh\aidatatang_200zh\corpus\trainaidatatang_200zh:  62%|█████████████████████████████████████████████████████████████████████████████▉                                               | 524/840 [46:39<29:46,  5.65s/speakers]> 这一步有问题吗> C:\Users\Administrator\Desktop\Realtime-Voice-Clone-Chinese-main\Realtime-Voice-Clone-Chinese-main>python synthesizer_preprocess_audio.py E:\data\aidatatang_200zh> Arguments:> datasets_root: E:\data\aidatatang_200zh> out_dir: E:\data\aidatatang_200zh\SV2TTS\synthesizer> n_processes: None> skip_existing: False> hparams:> no_alignments: False> dataset: aidatatang_200zh> > Using data from:> E:\data\aidatatang_200zh\aidatatang_200zh\corpus\train> aidatatang_200zh: 62%|█████████████████████████████████████████████████████████████████████████████▉ | 524/840 [46:39<29:46, 5.65s/speakers]没有，我是直接拿pt放入目录下，没有执行这些命令数据集得加载，下载数据集   #### 环境> Windows 10> Python 3.7> > #### 描述> 百度云的pt模型放入 后， 可执行，但产生结果都是杂音，中文和拼音都不太行> > #### 问题截图> > 本人纯小白，希望大佬有空给予指点。最新代码的话是可以输入中文的,你用pinyin试过吗?另外我看了一下图,你的录音输入好像也不太清晰,尽可能在5秒内说清晰2~3句话按照B站上演示的视频，我进行了以下尝试：- 更换拾音设备。原视频的输入图长得也差不多，应该不是主要原因- 更换拼音形式。完全按照视频输入框内容，依然都是滋滋声我考虑是直接使用模型的姿势不对，所以有使用百度云上模型文件成功的，请分享下经验，几天后issue我会关闭> 按照B站上演示的视频，我进行了以下尝试：> > * 更换拾音设备。原视频的输入图长得也差不多，应该不是主要原因> * 更换拼音形式。完全按照视频输入框内容，依然都是滋滋声>   我考虑是直接使用模型的姿势不对，所以有使用百度云上模型文件成功的，请分享下经验，几天后issue我会关闭貌似找到问题了，图中你应该选择了英文版的synthesizer了，要在 遇到了情况类似的问题，似乎是因为模型尺寸不匹配这个是输入框的问题，我也碰到过，对于某些中文或者字符不能很好识别，更换为其他短句试试> > > 按照B站上演示的视频，我进行了以下尝试：> >     * 更换拾音设备。原视频的输入图长得也差不多，应该不是主要原因> >     * 更换拼音形式。完全按照视频输入框内容，依然都是滋滋声>       我考虑是直接使用模型的姿势不对，所以有使用百度云上模型文件成功的，请分享下经验，几天后issue我会关闭我想请教下 这个录音可以用现成的mp3文件吗> > 按照B站上演示的视频，我进行了以下尝试：> > > > 我想请教下 这个录音可以用现成的mp3文件吗可以的.> > > 遇到了情况类似的问题，似乎是因为模型尺寸不匹配> > 训练到0.4以下试试分享下我训练的模型和效果：链接：https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ 提取码：2021 训练效果视频https://www.bilibili.com/video/BV1uh411B7AD/> LOSS太高了 训练到0.4以下试试> 分享下我训练的模型和效果：> 链接：https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ> 提取码：2021> 训练效果视频> LOSS太高了 训练到0.4以下试试> 分享下我训练的模型和效果：> 链接：https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ> 提取码：2021> 训练效果视频> 路过留痕一下commit 😄> > > > LOSS太高了 训练到0.4以下试试> > 分享下我训练的模型和效果：> > 链接：https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ> > 提取码：2021> > 训练效果视频> > > 方便分享到readme吗？你可以直接提 路过留痕一下commit 😄可以啊，方便大家测试，您可以直接加到readme中😄> LOSS太高了 训练到0.4以下试试> 分享下我训练的模型和效果：> 链接：https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ> 提取码：2021> 训练效果视频> > > LOSS太高了 训练到0.4以下试试> > > 分享下我训练的模型和效果：> > > 链接：https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ> > > 提取码：2021> > > 训练效果视频> > > > > > > > 方便分享到readme吗？你可以直接提 路过留痕一下commit 😄> > 可以啊，方便大家测试，您可以直接加到readme中😄提交啦还顺便@你了哈我也使用的miven大佬提供的模型，但是和你一样的问题，出来是杂音不知道是什么原因，加载encoder也很慢> 我也使用的miven大佬提供的模型，但是和你一样的问题，出来是杂音不知道是什么原因，加载encoder也很慢参考置顶的issue哈> #### 环境> Windows 8.1 Python 3.7.9> > #### 描述> 你好，运行  demo_toolbox.py  出现以下错误是什么原因呢？> > #### 问题截图> 
输入一个mp3报错了，请问是啥原因？
试一下: 在线转wav?> 看报错应该是格式不支持, 试一下: 在线转wav?您好，试过了，还是不行。> > 看报错应该是格式不支持, 试一下: 在线转wav?> > 您好，试过了，还是不行。我看了一下sample里的文件里头文件的采样率是16Khz可以试试转一下Duplicated with #37 
關於 Train synthesizer 的問題，求指導 !
你好我已經下載了aidatatang_200zh這個數據集，並且把 aidatatang_200zh\corpus\train 底下的檔案都解壓縮完畢但是當我要開始執行 (我把檔案放在 D:\google download 這個路徑下 )卻發生以下狀況: 請問我可以怎麼解決問題呢? 我有查看之前 issues 的討論並沒有發現有類似問題，以下是我想到可能有問題的地方，還請作者為我解答，謝謝！1.我只有解壓縮 aidatatang_200zh\corpus\train 底下的檔案，是否其他資料夾下的檔案也要解壓縮?2.是不是只需要將所有 wav 檔單獨拉出來放在 aidatatang_200zh\corpus\train 底下然後再執行  ?3. 輸入的指令不對4. wav 檔 與 txt 檔是不是要預先處理，而我沒有進行處理?3。文件路径有空格 要么加双引号要么把文件夹名称空格去掉> 3。文件路径有空格 要么加双引号要么把文件夹名称空格去掉我按照你說的把空格去掉，不過又出現以下錯誤 請問有可能是發生什麼事情嗎?> > 3。文件路径有空格 要么加双引号要么把文件夹名称空格去掉> > 我按照你說的把空格去掉，不過又出現以下錯誤> > > > 請問有可能是發生什麼事情嗎?這邊問題已經解決，其中  這個錯誤只要用  就能解決，之後如遇到  可以參考 #5 其中有人提到把內存改為100G就可以@babysor 另外我想詢問您，之前有在 #5 當中說到你是用 100% cpu 跑一小時，是因為無法用GPU嗎(因為我目前也是 100% CPU運轉中)，我不太清楚在這邊詢問，有方法是用GPU跑嗎?> @babysor 另外我想詢問您，之前有在 #5 當中說到你是用 100% cpu 跑一小時，是因為無法用GPU嗎(因為我目前也是 100% CPU運轉中)，我不太清楚在這邊詢問，有方法是用GPU跑嗎?有部分处理是必须通过CPU的，所以其中有一段cpu 100%很正常，但如果很多处理都是100%，那可能有问题。> > > 3。文件路径有空格 要么加双引号要么把文件夹名称空格去掉> > > > > > 我按照你說的把空格去掉，不過又出現以下錯誤> > > > 請問有可能是發生什麼事情嗎?> > 這邊問題已經解決，其中  這個錯誤只要用  就能解決，之後如遇到  可以參考 #5 其中有人提到把內存改為100G就可以你可以提个quick change到readme嘛> > > > 3。文件路径有空格 要么加双引号要么把文件夹名称空格去掉> > > > > > > > > 我按照你說的把空格去掉，不過又出現以下錯誤> > > > > > 請問有可能是發生什麼事情嗎?> > > > > > 這邊問題已經解決，其中  這個錯誤只要用  就能解決，之後如遇到  可以參考 #5 其中有人提到把內存改為100G就可以> > 你可以提个quick change到readme嘛不好意思，我是 github 的新手，我非常樂意提供我遇到難題的解決方法，不過要怎麼提到 readme 裡面讓遇到困難的人可以直接看到呢?> > @babysor 另外我想詢問您，之前有在 #5 當中說到你是用 100% cpu 跑一小時，是因為無法用GPU嗎(因為我目前也是 100% CPU運轉中)，我不太清楚在這邊詢問，有方法是用GPU跑嗎?> > 有部分处理是必须通过CPU的，所以其中有一段cpu 100%很正常，但如果很多处理都是100%，那可能有问题。了解，謝謝!> > > > > 3。文件路径有空格 要么加双引号要么把文件夹名称空格去掉> > > > > > > > > > > > 我按照你說的把空格去掉，不過又出現以下錯誤> > > > > > > > 請問有可能是發生什麼事情嗎?> > > > > > > > > 這邊問題已經解決，其中  這個錯誤只要用  就能解決，之後如遇到  可以參考 #5 其中有人提到把內存改為100G就可以> > > > > > 你可以提个quick change到readme嘛> > 不好意思，我是 github 的新手，我非常樂意提供我遇到難題的解決方法，不過要怎麼提到 readme 裡面讓遇到困難的人可以直接看到呢?你需要folk，修改之后提交git，申请pull request就好> > > > > > 3。文件路径有空格 要么加双引号要么把文件夹名称空格去掉> > > > > > > > > > > > > > > 我按照你說的把空格去掉，不過又出現以下錯誤> > > > > > > > > > 請問有可能是發生什麼事情嗎?> > > > > > > > > > > > 這邊問題已經解決，其中  這個錯誤只要用  就能解決，之後如遇到  可以參考 #5 其中有人提到把內存改為100G就可以> > > > > > > > > 你可以提个quick change到readme嘛> > > > > > 不好意思，我是 github 的新手，我非常樂意提供我遇到難題的解決方法，不過要怎麼提到 readme 裡面讓遇到困難的人可以直接看到呢?> > 你需要folk，修改之后提交git，申请pull request就好我花了一點時間研究，已經申請pull request了，再請作者看一下> > > > > > > 3。文件路径有空格 要么加双引号要么把文件夹名称空格去掉> > > > > > > > > > > > > > > > > > 我按照你說的把空格去掉，不過又出現以下錯誤> > > > > > > > > > > > 請問有可能是發生什麼事情嗎?> > > > > > > > > > > > > > > 這邊問題已經解決，其中  這個錯誤只要用  就能解決，之後如遇到  可以參考 #5 其中有人提到把內存改為100G就可以> > > > > > > > > > > > 你可以提个quick change到readme嘛> > > > > > > > > 不好意思，我是 github 的新手，我非常樂意提供我遇到難題的解決方法，不過要怎麼提到 readme 裡面讓遇到困難的人可以直接看到呢?> > > > > > 你需要folk，修改之后提交git，申请pull request就好> > 我花了一點時間研究，已經申請pull request了，再請作者看一下通过了 不过这个好像是另一个修改？什麼意思?> 什麼意思?我好像懂了 我再去研究一下 這個issue先關閉 
保姆级别教程（持续更新各类社区/非官方教程----
（作者借楼编辑ing社区视频教程：奶糖 再分享 synthesizer_train.py mandarin E:\datat\rain_set\train\Arguments:&nbsp; &nbsp; run_id:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mandarin&nbsp; &nbsp; syn_dir:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;E:\datat\rain_set\train\&nbsp; &nbsp; models_dir:&nbsp; &nbsp; &nbsp; synthesizer/saved_models/&nbsp; &nbsp; save_every:&nbsp; &nbsp; &nbsp; 1000&nbsp; &nbsp; backup_every:&nbsp; &nbsp; 25000&nbsp; &nbsp; force_restart:&nbsp; &nbsp;False&nbsp; &nbsp; hparams:Checkpoint path: synthesizer\saved_models\mandarin\mandarin.ptLoading training data from: E:\datat\rain_set\train\train.txtUsing model: TacotronUsing device: cpuInitialising Tacotron Model...Trainable Parameters: 30.872MLoading weights at synthesizer\saved_models\mandarin\mandarin.ptTacotron weights loaded from step 0Using inputs from:&nbsp; &nbsp; &nbsp; &nbsp; E:\datat\rain_set\train\train.txt&nbsp; &nbsp; &nbsp; &nbsp; E:\datat\rain_set\train\mels&nbsp; &nbsp; &nbsp; &nbsp; E:\datat\rain_set\train\embedsTraceback (most recent call last):&nbsp; File "synthesizer_train.py", line 35, in <module&gt;&nbsp; &nbsp; train(**vars(args))&nbsp; File "C:\Users\Administrator\Desktop\Realtime-Voice-Clone-Chinese-main\Realtime-Voice-Clone-Chinese-main\synthesizer\train.py", line 111, in train&nbsp; &nbsp; dataset = SynthesizerDataset(metadata_fpath, mel_dir, embed_dir, hparams)&nbsp; File "C:\Users\Administrator\Desktop\Realtime-Voice-Clone-Chinese-main\Realtime-Voice-Clone-Chinese-main\synthesizer\synthesizer_dataset.py", line 12, in __init__&nbsp; &nbsp; with metadata_fpath.open("r", encoding="utf-8") as metadata_file:&nbsp; File "C:\ProgramData\Anaconda3\lib\pathlib.py", line 1221, in open&nbsp; &nbsp; return io.open(self, mode, buffering, encoding, errors, newline,&nbsp; File "C:\ProgramData\Anaconda3\lib\pathlib.py", line 1077, in _opener&nbsp; &nbsp; return self._accessor.open(self, flags, mode)FileNotFoundError: [Errno 2] No such file or directory: 'E:\\datat\\rain_set\\train\\train.txt'C:\Users\Administrator\Desktop\Realtime-Voice-Clone-Chinese-main\Realtime-Voice-Clone-Chinese-main&gt;python demo_toolbox.py -d E:\data\train_set\train2021-08-19 17:56:17.809226: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found2021-08-19 17:56:17.809396: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.Arguments:&nbsp; &nbsp; datasets_root:&nbsp; &nbsp; E:\data\train_set\train&nbsp; &nbsp; enc_models_dir:&nbsp; &nbsp;encoder\saved_models&nbsp; &nbsp; syn_models_dir:&nbsp; &nbsp;synthesizer\saved_models&nbsp; &nbsp; voc_models_dir:&nbsp; &nbsp;vocoder\saved_models&nbsp; &nbsp; cpu:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; False&nbsp; &nbsp; seed:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;None&nbsp; &nbsp; no_mp3_support:&nbsp; &nbsp;FalseWarning: you do not have any of the recognized datasets in E:\data\train_set\train.The recognized datasets are:&nbsp; &nbsp; &nbsp; &nbsp; LibriSpeech/dev-clean&nbsp; &nbsp; &nbsp; &nbsp; LibriSpeech/dev-other&nbsp; &nbsp; &nbsp; &nbsp; LibriSpeech/test-clean&nbsp; &nbsp; &nbsp; &nbsp; LibriSpeech/test-other&nbsp; &nbsp; &nbsp; &nbsp; LibriSpeech/train-clean-100&nbsp; &nbsp; &nbsp; &nbsp; LibriSpeech/train-clean-360&nbsp; &nbsp; &nbsp; &nbsp; LibriSpeech/train-other-500&nbsp; &nbsp; &nbsp; &nbsp; LibriTTS/dev-clean&nbsp; &nbsp; &nbsp; &nbsp; LibriTTS/dev-other&nbsp; &nbsp; &nbsp; &nbsp; LibriTTS/test-clean&nbsp; &nbsp; &nbsp; &nbsp; LibriTTS/test-other&nbsp; &nbsp; &nbsp; &nbsp; LibriTTS/train-clean-100&nbsp; &nbsp; &nbsp; &nbsp; LibriTTS/train-clean-360&nbsp; &nbsp; &nbsp; &nbsp; LibriTTS/train-other-500&nbsp; &nbsp; &nbsp; &nbsp; LJSpeech-1.1&nbsp; &nbsp; &nbsp; &nbsp; VoxCeleb1/wav&nbsp; &nbsp; &nbsp; &nbsp; VoxCeleb1/test_wav&nbsp; &nbsp; &nbsp; &nbsp; VoxCeleb2/dev/aac&nbsp; &nbsp; &nbsp; &nbsp; VoxCeleb2/test/aac&nbsp; &nbsp; &nbsp; &nbsp; VCTK-Corpus/wav48&nbsp; &nbsp; &nbsp; &nbsp; aidatatang_200zh/corpus/dev&nbsp; &nbsp; &nbsp; &nbsp; aidatatang_200zh/corpus/testFeel free to add your own. You can still use the toolbox by recording samples yourself.Loaded encoder "pretrained.pt" trained to step 1564501Synthesizer using device: cpuTrainable Parameters: 30.872MTraceback (most recent call last):&nbsp; File "C:\Users\Administrator\Desktop\Realtime-Voice-Clone-Chinese-main\Realtime-Voice-Clone-Chinese-main\toolbox\__init__.py", line 122, in <lambda&gt;&nbsp; &nbsp; func = lambda: self.synthesize() or self.vocode()&nbsp; File "C:\Users\Administrator\Desktop\Realtime-Voice-Clone-Chinese-main\Realtime-Voice-Clone-Chinese-main\toolbox\__init__.py", line 229, in synthesize&nbsp; &nbsp; specs = self.synthesizer.synthesize_spectrograms(texts, embeds)&nbsp; File "C:\Users\Administrator\Desktop\Realtime-Voice-Clone-Chinese-main\Realtime-Voice-Clone-Chinese-main\synthesizer\inference.py", line 86, in synthesize_spectrograms&nbsp; &nbsp; self.load()&nbsp; File "C:\Users\Administrator\Desktop\Realtime-Voice-Clone-Chinese-main\Realtime-Voice-Clone-Chinese-main\synthesizer\inference.py", line 64, in load&nbsp; &nbsp; self._model.load(self.model_fpath)&nbsp; File "C:\Users\Administrator\Desktop\Realtime-Voice-Clone-Chinese-main\Realtime-Voice-Clone-Chinese-main\synthesizer\models\tacotron.py", line 497, in load&nbsp; &nbsp; self.load_state_dict(checkpoint["model_state"])&nbsp; File "C:\ProgramData\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1223, in load_state_dict&nbsp; &nbsp; raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(RuntimeError: Error(s) in loading state_dict for Tacotron:&nbsp; &nbsp; &nbsp; &nbsp; size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([75, 512]) from checkpoint, the shape in current model is torch.Size([70, 512]).| Generating 1/1Done.------------------&nbsp;原始邮件&nbsp;------------------发件人:                                                                                                                        "babysor/Realtime-Voice-Clone-Chinese"                                                                                    ***@***.***&gt;;发送时间:&nbsp;2021年8月19日(星期四) 下午5:57***@***.***&gt;;***@***.***&gt;;"State ***@***.***&gt;;主题:&nbsp;Re: [babysor/Realtime-Voice-Clone-Chinese] 来个保姆级别教程@@ (#20) 别害羞，快分享一下卡在哪里啦，我再优化优化 —You are receiving this because you modified the open/close state.Reply to this email directly, view it on GitHub, or unsubscribe.Triage notifications on the go with GitHub Mobile for iOS or Android.出一个详细的教程吧，大佬👍---原始邮件---发件人: ***@***.***&gt;发送时间: 2021年8月19日(周四) 下午5:57收件人: ***@***.***&gt;;抄送: ***@***.***&gt;;"State ***@***.***&gt;;主题: Re: [babysor/Realtime-Voice-Clone-Chinese] 来个保姆级别教程@@ (#20) 别害羞，快分享一下卡在哪里啦，我再优化优化 —You are receiving this because you modified the open/close state.Reply to this email directly, view it on GitHub, or unsubscribe.Triage notifications on the go with GitHub Mobile for iOS or Android.这。。看起来你都没train起来synthesizer啊同求，比如数据集在哪里下载> 同求，比如数据集在哪里下载#14 closed裡面有同樣的問題，有放下載連結E:\data\aidatatang_200zh\aidatatang_200zh\corpus\train   数据集解压路径       这一步synthesizer_preprocess_audio.py有问题吗C:\Users\Administrator\Desktop\Realtime-Voice-Clone-Chinese-main\Realtime-Voice-Clone-Chinese-main>python synthesizer_preprocess_audio.py E:\data\aidatatang_200zh\aidatatang_200zhArguments:    datasets_root:   E:\data\aidatatang_200zh\aidatatang_200zh    out_dir:         E:\data\aidatatang_200zh\aidatatang_200zh\SV2TTS\synthesizer    n_processes:     None    skip_existing:   False    hparams:    no_alignments:   False    dataset:         aidatatang_200zhUsing data from:    E:\data\aidatatang_200zh\aidatatang_200zh\aidatatang_200zh\corpus\trainTraceback (most recent call last):  File "synthesizer_preprocess_audio.py", line 63, in <module>    preprocess_dataset(**vars(args))  File "C:\Users\Administrator\Desktop\Realtime-Voice-Clone-Chinese-main\Realtime-Voice-Clone-Chinese-main\synthesizer\preprocess.py", line 32, in preprocess_dataset    assert all(input_dir.exists() for input_dir in input_dirs)AssertionError> E:\data\aidatatang_200zh\aidatatang_200zh\corpus\train 数据集解压路径 这一步synthesizer_preprocess_audio.py有问题吗> C:\Users\Administrator\Desktop\Realtime-Voice-Clone-Chinese-main\Realtime-Voice-Clone-Chinese-main>> python synthesizer_preprocess_audio.py E:\data\aidatatang_200zh\aidatatang_200zh> Arguments:> datasets_root: E:\data\aidatatang_200zh\aidatatang_200zh> out_dir: E:\data\aidatatang_200zh\aidatatang_200zh\SV2TTS\synthesizer> n_processes: None> skip_existing: False> hparams:> no_alignments: False> dataset: aidatatang_200zh> > Using data from:> E:\data\aidatatang_200zh\aidatatang_200zh\aidatatang_200zh\corpus\train> Traceback (most recent call last):> File "synthesizer_preprocess_audio.py", line 63, in> preprocess_dataset(**vars(args))> File "C:\Users\Administrator\Desktop\Realtime-Voice-Clone-Chinese-main\Realtime-Voice-Clone-Chinese-main\synthesizer\preprocess.py", line 32, in preprocess_dataset> assert all(input_dir.exists() for input_dir in input_dirs)> AssertionError  不用多一层python synthesizer_preprocess_audio.py E:\data\aidatatang_200zh 不用多一层解决了python synthesizer_preprocess_audio.py E:\data\aidatatang_200zh 不用多一层 @解决了大佬这步也太慢了。。。C:\Users\lxd\Desktop\Realtime-Voice-Clone-Chinese-main\Realtime-Voice-Clone-Chinese-main>python synthesizer_train.py mandarin D:\data\aidatatang_200zh\SV2TTS\synthesizerArguments:    run_id:          mandarin    syn_dir:         D:\data\aidatatang_200zh\SV2TTS\synthesizer    models_dir:      synthesizer/saved_models/    save_every:      1000    backup_every:    25000    force_restart:   False    hparams:Checkpoint path: synthesizer\saved_models\mandarin\mandarin.ptLoading training data from: D:\data\aidatatang_200zh\SV2TTS\synthesizer\train.txtUsing model: TacotronUsing device: cpuInitialising Tacotron Model...Trainable Parameters: 30.872MLoading weights at synthesizer\saved_models\mandarin\mandarin.ptTacotron weights loaded from step 0Using inputs from:        D:\data\aidatatang_200zh\SV2TTS\synthesizer\train.txt        D:\data\aidatatang_200zh\SV2TTS\synthesizer\mels        D:\data\aidatatang_200zh\SV2TTS\synthesizer\embedsFound 122482 samples+----------------+------------+---------------+------------------+| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |+----------------+------------+---------------+------------------+|   20k Steps    |     12     |     0.001     |        2         |+----------------+------------+---------------+------------------+{| Epoch: 1/2 (500/10207) | Loss: 0.9025 | 0.065 steps/s | Step: 0k | }Input at step 500: wo3 yao4 gei3 wang2 ming2 da3 dian4 hua4~__________________________________________________________{| Epoch: 1/2 (1000/10207) | Loss: 0.8266 | 0.071 steps/s | Step: 1k | }Input at step 1000: na4 me wo3 jiu4 chong2 xin1 ren4 shi2 ni3~______________________________________________________________{| Epoch: 1/2 (1500/10207) | Loss: 0.7602 | 0.074 steps/s | Step: 1k | }Input at step 1500: mei3 tian1 dou1 na4 me wan3 shui4 jiao4~___________________________________{| Epoch: 1/2 (2000/10207) | Loss: 0.7415 | 0.075 steps/s | Step: 2k | }Input at step 2000: da3 dian4 hua4 gei3 deng4 han4 ling2~_________________________________________________________________________{| Epoch: 1/2 (2500/10207) | Loss: 0.6921 | 0.068 steps/s | Step: 2k | }Input at step 2500: zhen1 xiang4 yong3 yuan3 zhi3 you3 yi2 ge4~___________________________________{| Epoch: 1/2 (3000/10207) | Loss: 0.6741 | 0.072 steps/s | Step: 3k | }Input at step 3000: xia4 men2 wai4 guo2 yu3 xue2 xiao4 chu1 er4 nian2 ji2 chen2 xiao3 qi2 jia1 de zhu4 zhi3~{| Epoch: 1/2 (3500/10207) | Loss: 0.6499 | 0.070 steps/s | Step: 3k | }Input at step 3500: ru2 guo3 wo3 he2 ni3 zai4 yi4 qi3~_______________________________________________________{| Epoch: 1/2 (4000/10207) | Loss: 0.6679 | 0.073 steps/s | Step: 4k | }Input at step 4000: fu4 jin4 de ping2 an1 yin2 hang2~_________________________________{| Epoch: 1/2 (4500/10207) | Loss: 0.6349 | 0.069 steps/s | Step: 4k | }Input at step 4500: ming2 zi4 shi4 hui3 guo4 cheng2 nuo4 shu1~_____________________________________________________{| Epoch: 1/2 (5000/10207) | Loss: 0.6392 | 0.073 steps/s | Step: 5k | }Input at step 5000: wo3 shen2 me shi2 hou4 cai2 neng2 chong1 man3 dian4~_______________________{| Epoch: 1/2 (5500/10207) | Loss: 0.6293 | 0.073 steps/s | Step: 5k | }Input at step 5500: wo3 da3 ni3 hao3 bu4 hao3 ma ge2 shi4 chong2 fu4~___________{| Epoch: 1/2 (6000/10207) | Loss: 0.6715 | 0.077 steps/s | Step: 6k | }Input at step 6000: ci3 ji4 hao3 wu2 liao2 da3 yi1 dian4 ying3 ming2~___________________________________{| Epoch: 1/2 (6500/10207) | Loss: 0.6446 | 0.075 steps/s | Step: 6k | }Input at step 6500: wo3 gei3 ni3 fa1 de ni3 shou1 dao4 le ma~___________________________________________________________{| Epoch: 1/2 (7000/10207) | Loss: 0.6022 | 0.068 steps/s | Step: 7k | }Input at step 7000: ning4 que1 wu2 lan4 zhi3 wei4 yi3 hou4 de du2 yi1 wu2 er4~________________________{| Epoch: 1/2 (7500/10207) | Loss: 0.6178 | 0.067 steps/s | Step: 7k | }Input at step 7500: mei2 you3 wang3 luo4 ni3 hai2 hui4 liao2 tian1 ma~______________________{| Epoch: 1/2 (8000/10207) | Loss: 0.6041 | 0.068 steps/s | Step: 8k | }Input at step 8000: wo3 bu4 fa1 le wo3 yao4 shui4 jiao4 le~____________________________________________________________________________________________{| Epoch: 1/2 (8500/10207) | Loss: 0.6078 | 0.072 steps/s | Step: 8k | }Input at step 8500: ni3 cai1 lai2 cai1 qu4 ye3 cai1 bu4 ming2 bai2~____________________________________________________{| Epoch: 1/2 (9000/10207) | Loss: 0.6055 | 0.072 steps/s | Step: 9k | }Input at step 9000: ni3 wen4 le wo3 tou2 dou1 da4 le~_______________________________________{| Epoch: 1/2 (9500/10207) | Loss: 0.5816 | 0.069 steps/s | Step: 9k | }Input at step 9500: xia4 ban1 mei2 you3 mei2 chu1 qu4 guang4~_______________________________________{| Epoch: 1/2 (10000/10207) | Loss: 0.5664 | 0.068 steps/s | Step: 10k | }Input at step 10000: ni3 jin1 tian1 bu2 shi4 bu4 shang4 ban1 ma~__________________________________________________{| Epoch: 1/2 (10207/10207) | Loss: 0.5879 | 0.071 steps/s | Step: 10k | }{| Epoch: 2/2 (293/10207) | Loss: 0.5840 | 0.070 steps/s | Step: 10k | }Input at step 10500: ai4 qing2 xiao3 shuo1 ma2 que4 gao3 ding4 hua1 mei3 nan2~_________________________________{| Epoch: 2/2 (322/10207) | Loss: 0.5892 | 0.070 steps/s | Step: 10k | }@zhuzaileiting 你这是用cpu训练的，GPU速度大概在1.3-2 steps/s...GPU，怎么配置显卡全局配置了呀。。Arguments:    datasets_root:   D:\Data\aidatatang_200zh    out_dir:         D:\Data\aidatatang_200zh\SV2TTS\synthesizer    n_processes:     None    skip_existing:   False    hparams:    no_alignments:   False    dataset:         aidatatang_200zhUsing data from:    D:\Data\aidatatang_200zh\aidatatang_200zh\corpus\trainTraceback (most recent call last):  File "synthesizer_preprocess_audio.py", line 63, in <module>    preprocess_dataset(**vars(args))  File "D:\Realtime-Voice-Clone-Chinese\synthesizer\preprocess.py", line 32, in preprocess_dataset    assert all(input_dir.exists() for input_dir in 7天有效二维码过期了> > > 7天有效> > 群二维码过期了，求更见上> > 群二维码过期了，求更> > multilang writer preferred. 包教会和调优，整理后输出教程到社区。群200人了 求群成员邀请进群。。我的wx是Sahvyhsu用telegram更好> 用telegram更好辛苦创建一个？For non-trainer: trainer: demo_toolbox.py -d "D:\Downloads\aidatatang_200zh"2021-10-01 14:32:20.420728: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found2021-10-01 14:32:20.420998: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.Arguments:    datasets_root:    D:\Downloads\aidatatang_200zh    enc_models_dir:   encoder\saved_models    syn_models_dir:   synthesizer\saved_models    voc_models_dir:   vocoder\saved_models    cpu:              False    seed:             None    no_mp3_support:   FalseTraceback (most recent call last):  File "D:\Downloads\MockingBird-main\demo_toolbox.py", line 43, in <module>    Toolbox(**vars(args))  File "D:\Downloads\MockingBird-main\toolbox\__init__.py", line 74, in __init__    self.reset_ui(enc_models_dir, syn_models_dir, voc_models_dir, seed)  File "D:\Downloads\MockingBird-main\toolbox\__init__.py", line 142, in reset_ui    self.ui.populate_models(encoder_models_dir, synthesizer_models_dir, vocoder_models_dir)  File "D:\Downloads\MockingBird-main\toolbox\ui.py", line 339, in populate_models    raise Exception("No synthesizer models found in %s" % synthesizer_models_dir)Exception: No synthesizer models found in synthesizer\saved_models求群成员邀请入群，wx是Sinohoney0002求群成员邀请进群。。我的wx是myggzhsdd1python demo_toolbox.py -d E:\Fun\datasetsArguments:    datasets_root:    E:\Fun\datasets    enc_models_dir:   encoder\saved_models    syn_models_dir:   synthesizer\saved_models    voc_models_dir:   vocoder\saved_models    cpu:              False    seed:             None    no_mp3_support:   FalseWarning: you do not have any of the recognized datasets in E:\Fun\datasets.The recognized datasets are:        LibriSpeech/dev-clean        LibriSpeech/dev-other        LibriSpeech/test-clean        LibriSpeech/test-other        LibriSpeech/train-clean-100        LibriSpeech/train-clean-360        LibriSpeech/train-other-500        LibriTTS/dev-clean        LibriTTS/dev-other        LibriTTS/test-clean        LibriTTS/test-other        LibriTTS/train-clean-100        LibriTTS/train-clean-360        LibriTTS/train-other-500        LJSpeech-1.1        VoxCeleb1/wav        VoxCeleb1/test_wav        VoxCeleb2/dev/aac        VoxCeleb2/test/aac        VCTK-Corpus/wav48        aidatatang_200zh/corpus/dev        aidatatang_200zh/corpus/test        aishell3/test/wav        magicdata/trainFeel free to add your own. You can still use the toolbox by recording samples yourself.这里出了什么问题，进去之后数据那两行是灰色的。求教QAQ> @zhuzaileiting 你这是用cpu训练的，GPU速度大概在1.3-2 steps/s您好，我根据您的文档修改了Batch 建了二群 方便交流田渣渣 ***@***.***> 于 2021年10月9日周六 下午1:23写道：> 求群成员邀请入群，wx是Mr-Sandman___>> —> You are receiving this because you commented.> Reply to this email directly, view it on GitHub> <https://github.com/babysor/MockingBird/issues/20#issuecomment-939230399>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AS6A2HF5F57YOZ4N2754TLDUF7GUVANCNFSM5CN2A35Q>> .> Triage notifications on the go with GitHub Mobile for iOS> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>> or Android> > @zhuzaileiting 你这是用cpu训练的，GPU速度大概在1.3-2 steps/s> > 您好，我根据您的文档修改了Batch size调整到36，现在显存已经使用80%，但是显卡使用率还是只有12%，我还需要调整哪个参数呢？我3090，设置为48，0.86 setps/s，达不到1.3-2> > > @zhuzaileiting 你这是用cpu训练的，GPU速度大概在1.3-2 steps/s> > > > > > 您好，我根据您的文档修改了Batch size调整到36，现在显存已经使用80%，但是显卡使用率还是只有12%，我还需要调整哪个参数呢？> > 我3090，设置为48，0.86 setps/s，达不到1.3-2不同batch size，step速度没有可比较性。> > > > @zhuzaileiting 你这是用cpu训练的，GPU速度大概在1.3-2 steps/s> > > > > > > > > 您好，我根据您的文档修改了Batch size调整到36，现在显存已经使用80%，但是显卡使用率还是只有12%，我还需要调整哪个参数呢？> > > > > > 我3090，设置为48，0.86 setps/s，达不到1.3-2> > 不同batch size，step速度没有可比较性。和数据集大小也有关吧> > > > > @zhuzaileiting 你这是用cpu训练的，GPU速度大概在1.3-2 steps/s> > > > > > > > > > > > 您好，我根据您的文档修改了Batch size调整到36，现在显存已经使用80%，但是显卡使用率还是只有12%，我还需要调整哪个参数呢？> > > > > > > > > 我3090，设置为48，0.86 setps/s，达不到1.3-2> > > > > > 不同batch size，step速度没有可比较性。> > 和数据集大小也有关吧是的能再发一下群吗 - vega的文章 https://zhuanlan.zhihu.com/p/425692267> 别害羞，快分享一下卡在哪里啦，我再优化优化作者你好，有没有考虑出一个docker镜像，所有搭建问题都迎刃而解。。。能再发一下群吗 过期了耶同求> > 别害羞，快分享一下卡在哪里啦，我再优化优化> > > > 别害羞，快分享一下卡在哪里啦，我再优化优化> > > > > > 作者你好，有没有考虑出一个docker镜像，所有搭建问题都迎刃而解。。。> > 需要考虑显卡部分，现在确实有支持cuda的docker，你能帮我research一下吗？只用于预测的容器镜像我已经构建好，内置预训练模型，本次测试跑过了。  > > > > 别害羞，快分享一下卡在哪里啦，我再优化优化> > > > > > > > > 作者你好，有没有考虑出一个docker镜像，所有搭建问题都迎刃而解。。。> > > > > > 需要考虑显卡部分，现在确实有支持cuda的docker，你能帮我research一下吗？> > 只用于预测的容器镜像我已经构建好，内置预训练模型，本次测试跑过了。 有测试的相关信息吗？如果方便的话发起pr？> > > > 别害羞，快分享一下卡在哪里啦，我再优化优化> > > > > > > > > 作者你好，有没有考虑出一个docker镜像，所有搭建问题都迎刃而解。。。> > > > > > 需要考虑显卡部分，现在确实有支持cuda的docker，你能帮我research一下吗？> > 只用于预测的容器镜像我已经构建好，内置预训练模型，本次测试跑过了。 我试用了一下，有一些问题1. 基于浏览器的限制，不是https没法获取麦克风权限，本人不擅长python，只好另辟蹊径在不改动镜像的情况下，在宿主机上搭了个nginx随便起了个端口号666 配好ssl证书来代理80802. 第二个问题那应该真的是一个问题，我进到容器里执行demo_toolbox报错，我有个疑问，是不是这个demo_tool只针对windows啊（我是mac）。报错如所示 > > > > > 别害羞，快分享一下位置啦，我再优化优化> > > > > > > > > > > > 作者你好，有没有考虑出一个码头工人假象，所有搭建的问题都迎刃而解。。。> > > > > > > > > 需要考虑显瘦部分，现在确实有支持 cuda 的码头工人，你能帮我研究一下吗？> > > > > > 仅用于预测的假设我已经构建好，构建了预训练模型，本次测试跑过了。 > > 我试用了一下，有一些问题> > 1. 基于浏览器的限制，不是https无法获取获取权限，只有自己不擅长蟒蛇，好另一种方法可以在不伪装镜像的下，在智能机上机上了几个nginx随便找个端口号666情况配好ssl来代理8080> 2. 第二个问题那应该是一个问题，进入容器里执行demo_toolbox，我这个问题，是不是demo_tool只针对windows啊（我是mac）。报错如所示> > 我也是这个问题 怎么解决啊C:\mockingbird\MockingBird-main>python demo_toolbox.pyTraceback (most recent call last):  File "demo_toolbox.py", line 2, in <module>    from toolbox import Toolbox  File "C:\mockingbird\MockingBird-main\toolbox\__init__.py", line 1, in <module>    from toolbox.ui import UI  File "C:\mockingbird\MockingBird-main\toolbox\ui.py", line 7, in <module>    from encoder.inference import plot_embedding_as_heatmap  File "C:\mockingbird\MockingBird-main\encoder\inference.py", line 2, in <module>    from encoder.model import SpeakerEncoder  File "C:\mockingbird\MockingBird-main\encoder\model.py", line 5, in <module>    from torch.nn.utils import clip_grad_norm_ModuleNotFoundError: No module named 'torch'- [ ] 我想了下 这个docker里运行GUI的软件 也许还得参考https://www.cloudsavvyit.com/10520/how-to-run-gui-applications-in-a-docker-container/求更新群二维码求更新群二维码求更新群二维码> PLAY ***@***.***> 于 2021年12月6日周一 下午2:45写道：> 还有入群码吗>> —> You are receiving this because you commented.> Reply to this email directly, view it on GitHub> <https://github.com/babysor/MockingBird/issues/20#issuecomment-986484977>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AS6A2HGNFSX4JFJUGARV5J3UPRLXTANCNFSM5CN2A35Q>> .> Triage notifications on the go with GitHub Mobile for iOS> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>> or Android> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.>>做了一个视频  能重新发一下入群码吗 @babysor 求大佬更新二维码 @babysor求大佬更新二维码不是特别清楚什么文件放什么位置大佬更新一下二维码dadayefeng ***@***.***> 于 2021年12月13日周一 下午9:33写道：> 大佬更新一下二维码>> —> You are receiving this because you commented.> Reply to this email directly, view it on GitHub> <https://github.com/babysor/MockingBird/issues/20#issuecomment-992480848>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/AS6A2HCZLPN5QBZIMOQAOHTUQXY2JANCNFSM5CN2A35Q>> .> Triage notifications on the go with GitHub Mobile for iOS> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>> or Android> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.>>大佬能更新下二维码吗，现在总是崩溃找不到问题二维码没更新啊> 做了一个视频 还算详细，应该只漏了ffmpeg的安装方便指定放到readme吗？![1081639659515_ @babysor 大佬求新群号，谢啦😘 @babysor求新群码或者QQ群号 联系我&nbsp;------------------&nbsp;原始邮件&nbsp;------------------发件人: ***@***.***&gt;; 发送时间: 2022年1月9日(星期天) 晚上10:20收件人: ***@***.***&gt;; 抄送: ***@***.***&gt;; ***@***.***&gt;; 主题: Re: [babysor/MockingBird] 保姆级别教程（持续更新各类社区/非官方教程---- (#20)  —Reply to this email directly, view it on GitHub, or unsubscribe.Triage notifications on the go with GitHub Mobile for iOS or Android. You are receiving this because you commented.Message ID: ***@***.***&gt;> pre.py D:\数据集 -d aidatatang_200zh -n 7Using data from:    D:\数据集\aidatatang_200zh\corpus\trainTraceback (most recent call last):  File "D:\M\MockingBird-main\pre.py", line 74, in <module>    preprocess_dataset(**vars(args))  File "D:\M\MockingBird-main\synthesizer\preprocess.py", line 45, in preprocess_dataset    assert all(input_dir.exists() for input_dir in input_dirs)AssertionError有大佬帮帮我吗更新一下二维码谢谢> 更新一下二维码谢谢+1在输入音频后出现了这个报错……Feel free to add your own. You can still use the toolbox by recording samples yourself.Traceback (most recent call last):  File "D:\mokingbird\MockingBird-main\MockingBird-main\toolbox\__init__.py", line 103, in <lambda>    func = lambda: self.load_from_browser(self.ui.browse_file())  File "D:\mokingbird\MockingBird-main\MockingBird-main\toolbox\__init__.py", line 170, in load_from_browser    wav = Synthesizer.load_preprocess_wav(fpath)  File "D:\mokingbird\MockingBird-main\MockingBird-main\synthesizer\inference.py", line 146, in load_preprocess_wav    wav = librosa.load(str(fpath), hparams.sample_rate)[0]TypeError: load() takes 1 positional argument but 2 were given> 在输入音频后出现了这个问题……> > 随意添加您自己的。您仍然可以通过自己录制样本来使用工具箱。 Traceback（最近一次调用最后）： 文件“D:\mokingbird\MockingBird-main\MockingBird-main\toolbox_init __.py_ ”，第 103 行，在 func = lambda: self.load_from_browser(self.ui.browse_file()) 文件“D:\mokingbird\MockingBird-main\MockingBird-main\toolbox_init __.py_ ”，第 170 行，在 load_from_browser wav = Synthesizer.load_preprocess_wav(fpath) 文件“D:\mokingbird\MockingBird-main\MockingBird-main\ synthesizer\inference.py"，第 146 行，在 load_preprocess_wav wav = librosa.load(str(fpath), hparams.sample_rate)[0] TypeError: load() 采用 1 个位置参数，但给出了 2 个已解决在命令的命令输入pip install librosa==0.8.1请问可以更新一下二维码吗，谢谢各位大佬！！帮我看看这是什么错误啊，，我已经把模板放入相应的文件夹里了可是仍然不行额D:\迅雷下载\MockingBird-main>python demo_toolbox.pyArguments:datasets_root: Noneenc_models_dir: encoder\saved_modelssyn_models_dir: synthesizer\saved_modelsvoc_models_dir: vocoder\saved_modelscpu: Falseseed: Noneno_mp3_support: FalseWarning: you did not pass a root directory for datasets as argument.The recognized datasets are:LibriSpeech/dev-cleanLibriSpeech/dev-otherLibriSpeech/test-cleanLibriSpeech/test-otherLibriSpeech/train-clean-100LibriSpeech/train-clean-360LibriSpeech/train-other-500LibriTTS/dev-cleanLibriTTS/dev-otherLibriTTS/test-cleanLibriTTS/test-otherLibriTTS/train-clean-100LibriTTS/train-clean-360LibriTTS/train-other-500LJSpeech-1.1VoxCeleb1/wavVoxCeleb1/test_wavVoxCeleb2/dev/aacVoxCeleb2/test/aacVCTK-Corpus/wav48aidatatang_200zh/corpus/devaidatatang_200zh/corpus/testaishell3/test/wavmagicdata/trainFeel free to add your own. You can still use the toolbox by recording samples yourself.二维码需要更新求个新的群二维码大佬，我requirements安装没报错，但是运行程序时会报错:Traceback (most recent call last):  File "E:\MockingBird\demo_toolbox.py", line 2, in <module>    from toolbox import Toolbox  File "E:\MockingBird\toolbox\__init__.py", line 6, in <module>    import ppg_extractor as extractor  File "E:\MockingBird\ppg_extractor\__init__.py", line 6, in <module>    from .frontend import DefaultFrontend  File "E:\MockingBird\ppg_extractor\frontend.py", line 5, in <module>    from torch_complex.tensor import ComplexTensorModuleNotFoundError: No module named 'torch_complex'pip install torchcomplex&nbsp;&nbsp;&nbsp;------------------&nbsp;原始邮件&nbsp;------------------发件人: ***@***.***&gt;; 发送时间: 2022年3月7日(星期一) 凌晨1:21收件人: ***@***.***&gt;; 抄送: ***@***.***&gt;; ***@***.***&gt;; 主题: Re: [babysor/MockingBird] 保姆级别教程（持续更新各类社区/非官方教程---- (#20) 大佬，我requirements安装没报错，但是运行程序时会报错: Traceback (most recent call last): File "E:\MockingBird\demo_toolbox.py", line 2, in  from toolbox import Toolbox File "E:\MockingBird\toolbox_init_.py", line 6, in  import ppg_extractor as extractor File "E:\MockingBird\ppg_extractor_init_.py", line 6, in  from .frontend import DefaultFrontend File "E:\MockingBird\ppg_extractor\frontend.py", line 5, in  from torch_complex.tensor import ComplexTensor ModuleNotFoundError: No module named 'torch_complex' —Reply to this email directly, view it on GitHub, or unsubscribe.Triage notifications on the go with GitHub Mobile for iOS or Android. You are receiving this because you commented.Message ID: ***@***.***&gt;大佬们  问个问题  我自己的数据太小了  中途更换别的数据集进行训练  但是出现了这样的错误代码 warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")Traceback (most recent call last):  File "E:\数据集制作\MockingBird-main\synthesizer_train.py", line 37, in <module>    train(**vars(args))  File "E:\数据集制作\MockingBird-main\synthesizer\train.py", line 208, in train    optimizer.step()  File "C:\Users\11351\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\optimizer.py", line 88, in wrapper    return func(*args, **kwargs)  File "C:\Users\11351\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\autograd\grad_mode.py", line 28, in decorate_context    return func(*args, **kwargs)  File "C:\Users\11351\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\adam.py", line 133, in step    F.adam(params_with_grad,  File "C:\Users\11351\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\_functional.py", line 86, in adam    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)RuntimeError: The size of tensor a (1024) must match the size of tensor b (3) at non-singleton dimension 3想问一下有大佬遇见过么   这种应该怎么办啊 
Pre Trained Model
Hi, I am from outside Chinais it possible to have the pre-trained model download from google drive?我的google drive一直有点问题，哪位朋友可以帮忙上传一波，方便一下国际友人？ 
关于该项目的一些想法。
目前来看，该项目在实际使用的时候远达不到“可用”的程度。包括以下几种问题：1、合成的音频会出现不包含正常人声，而是噪声和残缺的声音。2、合成的音色跟目标音色不一致，差别很大。目前分析出现问题一的原因应该是因为1、asr数据中有些数据存在明显过强底噪，音频和文本或者音素数据无法对齐。（加入一些数据清洗的手段）2、目前的d-vector和vocoder部分都是使用的英文数据集上训练的universal的版本，在中文数据集上使用肯定会出现mismatch的问题。（我理解d-vector和vocoder应该需要在中文数据集上重新训练以获得更好的结果）3、数据集中音色过少，导致很难找到跟目标音色较为一致的”参考音色“用于生成。（混合多种asr和tts数据集，构建一个大型的数据集，以提高对目标音色的适配程度）。这块我应该也会着手做一些工作以尝试改进模型，希望有机会和作者合作。> 目前来看，该项目在实际使用的时候远达不到“可用”的程度。包括以下几种问题：> 1、合成的音频会出现不包含正常人声，而是噪声和残缺的声音。> 2、合成的音色跟目标音色不一致，差别很大。> > 目前分析出现问题一的原因应该是因为> 1、asr数据中有些数据存在明显过强底噪，音频和文本或者音素数据无法对齐。（加入一些数据清洗的手段）> 2、目前的d-vector和vocoder部分都是使用的英文数据集上训练的universal的版本，在中文数据集上使用肯定会出现mismatch的问题。（我理解d-vector和vocoder应该需要在中文数据集上重新训练以获得更好的结果）> 3、数据集中音色过少，导致很难找到跟目标音色较为一致的”参考音色“用于生成。（混合多种asr和tts数据集，构建一个大型的数据集，以提高对目标音色的适配程度）。> > 这块我应该也会着手做一些工作以尝试改进模型，希望有机会和作者合作。欢迎贡献，可以联系我邮箱，我这边尽量及时回复跟你建立联系。（广告太多，就不放微信号了） 
speaker encoder的输出向量是什么样的？
SVT2TTS的评论区过来的，自己训练的speaker encoder，因为用的aishell3数据集，214个说话人，而输出的speaker embedding是256维的，这就导致每个说话人的向量很稀疏，大部分维度是0，几乎是one-hot形式的。所以用来训练synthesizer的话根本训练不了，loss是Nan。你这个模型训练synthesizer时有注意到speaker 参考论文额。。我是指你得到的speaker embedding的数值是怎样的 ，不是问的结构和维度。> > > 参考论文> > 额。。我是指你得到的speaker embedding的数值是怎样的 > > > > 参考论文> > > > > > 额。。我是指你得到的speaker embedding的数值是怎样的 ，不是问的结构和维度。> > embedding我发一个截图给你，你跑的是aishell我记得不是我这个repo支持的吧？> 
請問有aadatatang_200zh數據集的下載網址嗎?
請問有aadatatang_200zh數據集的下載網址嗎http://www.openslr.org/62/謝謝! 
调用作者提供的预训练模型出错。
 里的 改为 因为是早期训练的模型，所以配置不太一样> 如果复用我的模型，需要把  里的> > 改为> > 因为是早期训练的模型，所以配置不太一样1、如果自己训练的话，需要更改这块的配置吗？2、在其他issue中看到按照这种方法修改后的效果好像不是很好，是否有计划提供别的pretrained model~> > 如果复用我的模型，需要把  里的> > > > 改为> > > > 因为是早期训练的模型，所以配置不太一样> > 1、如果自己训练的话，需要更改这块的配置吗？> 2、在其他issue中看到按照这种方法修改后的效果好像不是很好，是否有计划提供别的pretrained model~别的model从0开始的话需要比较长的时间之后测试新的数据集的时候，我再试试，训练好的话就提供 
关于训练和推理的疑问
据我了解，datatang和slr68数据集都是针对ASR的数据，所以没有标注phoneme，那训练的时候是直接使用文字token还是先将文字转换成phoneme在进行训练。另外在您的演示视频中，我貌似看到是使用phoneme作为输入，如果是使用文字训练，inference的时候用phoneme，这之间又有什么样的处理。可以看下code，用了pypinyin进行了phoneme转换。演示视频那个时候只支持phoneme输入，现在有个github友今晚会提交支持中文输入> 可以看下code，用了pypinyin进行了phoneme转换。�> 演示视频那个时候只支持phoneme输入，现在有个github友今晚会提交支持中文输入也就是说训练的过程还是 phoneme -> mel -> wav的过程吧~是的， 
想要支持更多数据集？在这里提建议
已支持的有 aidatatang（已验证200zh）, Magic Data(已验证open SLR68)需要更多请在这里提建议，并+1投票，将为大家补充支持朋友，你是怎么跑起来的，我运行python synthesizer_preprocess_audio.py <datasets_root> 就迷惑了，这个datasets_root是指什么呢？假如你下载的 aidatatang_200zh文件放在D盘，train文件路径为  , 你的 就是 推荐aishell3数据集，稍微干净一些，但是数据量很少。另外datasets_root确定是 而不是 > 推荐aishell3数据集，稍微干净一些，但是数据量很少。另外datasets_root确定是 而不是 是的，如果理解参数名，这里原本是希望同时支持多个dataset,所以叫datasets_root。> > 推荐aishell3数据集，稍微干净一些，但是数据量很少。另外datasets_root确定是 而不是 > > 是的，如果理解参数名，这里原本是希望同时支持多个dataset,所以叫datasets_root。那这里如果我把aishell3和slr68的数据都放在datasets_root文件夹内，就可以同时跑两个数据集吗> > > 推荐aishell3数据集，稍微干净一些，但是数据量很少。另外datasets_root确定是 而不是 > > > > > > 是的，如果理解参数名，这里原本是希望同时支持多个dataset,所以叫datasets_root。> > 那这里如果我把aishell3和slr68的数据都放在datasets_root文件夹内，就可以同时跑两个数据集吗程序逻辑还没实现 囧 目前我是手动分开跑> 已支持的有 aidatatang（已验证200zh）, Magic Data(已验证open SLR68)> 需要更多请在这里提建议，并+1投票，将为大家补充支持推薦標貝數據集> 已支持的有 aidatatang（已验证200zh）, Magic Data(已验证open SLR68)> 需要更多请在这里提建议，并+1投票，将为大家补充支持aishell3 和 Mozilla Common Voice 数据集aidatatang_200zh在哪里下载呢> aidatatang_200zh在哪里下载呢這裡 > aidatatang_200zh在哪里下载呢> > 這裡 > 已支持的有 aidatatang（已验证200zh）, Magic Data(已验证open SLR68)> > 需要更多请在这里提建议，并+1投票，将为大家补充支持> > aishell3 和 Mozilla Common Voice 数据集https://github.com/babysor/Realtime-Voice-Clone-Chinese/issues/59 已支持aishell3，不过训练效果没增强大佬，現在你提供的版本還需要使用原項目的 encoder 和 vocoder 嗎> 大佬，現在你提供的版本還需要使用原項目的 encoder 和 vocoder 嗎已经不再需要下载了> > 大佬，現在你提供的版本還需要使用原項目的 encoder 和 vocoder 嗎> > 已经不再需要下载了好的，謝謝大佬的回覆，另外推薦 zhvoice數據集: 請問有沒有大佬能提供 Mozilla Common Voice 架構的支援... 原 repo 中有人也發過類似的 issue 可以參考  資料夾內容包含各項  希望日後能支持，謝謝老哥，我留意到你的aidatatang200 数据集里声音背景噪音非常重，而且大部分是男声，我对这个项目非常感兴趣，计划按照你的重新找女声（涵盖不同音色声线，萝莉，少女，御姐）重新录制干净无噪声的数据集，我也在思考男女声分开训练的可能性。此外我有一块A100显卡可以在较短时间内完成各种计算。我也愿意分享我的成果。 我的问题是，1. 我对音频文件的录制格式，编码，没有经验，可以简单讲一下和这个aidatatang数据集相同的音频格式是有什么参数需要我在录制和process的过程中需要注意的嘛？ 2. 我没有过多去了解aidatatang 数据组里 .metadata 和 .trn 文件的用途，可以大致说一下么？ 3. 有更多细节我们可以私信交流一下么> 老哥，我留意到你的aidatatang200 数据集里声音背景噪音非常重，而且大部分是男声，我对这个项目非常感兴趣，计划按照你的重新找女声（涵盖不同音色声线，萝莉，少女，御姐）重新录制干净无噪声的数据集，我也在思考男女声分开训练的可能性。此外我有一块A100显卡可以在较短时间内完成各种计算。我也愿意分享我的成果。 我的问题是，1. 我对音频文件的录制格式，编码，没有经验，可以简单讲一下和这个aidatatang数据集相同的音频格式是有什么参数需要我在录制和process的过程中需要注意的嘛？ 2. 我没有过多去了解aidatatang 数据组里 .metadata 和 .trn 文件的用途，可以大致说一下么？ 3. 有更多细节我们可以私信交流一下么1. 录制的话，建议是5-10秒长度的大于等于16khz的音频，每个语音单一说话人，尽可能多的覆盖男女生、不同声色（不要单独分开训练，混一起效果好）。预处理的话也就是把音频去噪，切割满足上面这个需求。2. 你指下载下来的数据集吗？3. 私信的话，可以走邮件，或者扫这里的二维码： > > Mon, 6 Sept 2021 at 01:54, Vega ***@***.***> wrote:> 老哥，我留意到你的aidatatang200> 数据集里声音背景噪音非常重，而且大部分是男声，我对这个项目非常感兴趣，计划按照你的重新找女声（涵盖不同音色声线，萝莉，少女，御姐）重新录制干净无噪声的数据集，我也在思考男女声分开训练的可能性。此外我有一块A100显卡可以在较短时间内完成各种计算。我也愿意分享我的成果。> 我的问题是，1.> 我对音频文件的录制格式，编码，没有经验，可以简单讲一下和这个aidatatang数据集相同的音频格式是有什么参数需要我在录制和process的过程中需要注意的嘛？> 2. 我没有过多去了解aidatatang 数据组里 .metadata 和 .trn 文件的用途，可以大致说一下么？ 3.> 有更多细节我们可以私信交流一下么>>>    1.>    录制的话，建议是5-10秒长度的大于等于16khz的音频，每个语音单一说话人，尽可能多的覆盖男女生、不同声色（不要单独分开训练，混一起效果好）。预处理的话也就是把音频去噪，切割满足上面这个需求。>    2. 你指下载下来的数据集吗？>    3. 私信的话，可以走邮件，或者扫这里的二维码： > > [image: WechatIMG54]>    <https://user-images.githubusercontent.com/7423248/130546423-5588bc46-af02-4eda-a122-f64d8f4b7627.jpeg>>> —> You are receiving this because you commented.> Reply to this email directly, view it on GitHub> <https://github.com/babysor/MockingBird/issues/10#issuecomment-913363163>,> or unsubscribe> <https://github.com/notifications/unsubscribe-auth/ALODQYY2IKTK5WWYPXGOSL3UARJPVANCNFSM5CG2ISUQ>> .> Triage notifications on the go with GitHub Mobile for iOS> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>> or Android> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.>>#20 这里有新的二维码https://github.com/fighting41love/zhvoicez这个看着很厉害的样子这边在研究改代码跑跑看不过都是mp3的很麻烦> 这个看着很厉害的样子> 这边在研究改代码跑跑看> 不过都是mp3的很麻烦等你好消息，不过我访问不了> > > > > 这个看着很厉害的样子> > 这边在研究改代码跑跑看> > 不过都是mp3的很麻烦> > 等你好消息，不过我访问不了是指数据集不能访问吗？如果是百度云不能访问的话我这边可以转mega或者GD链接: 提取码: dwet-----------------更新---------------------访问不了是链接不知怎的最后多了个z，删掉就行了https://github.com/fighting41love/zhvoice写了貌似可以直接用于zhrtvc，同一个分支出来的https://github.com/xingmegshuo/zhrtvc突然发现上面已经有人推荐过了...@babysor 老哥，我留意到你的aidatatang200 数据集里声音背景噪音非常重，而且大部分是男声，我对这个项目非常感兴趣，计划按照你的重新找女声（涵盖不同音色声线，萝莉，少女，御姐）重新录制干净无噪声的数据集，我也在思考男女声分开训练的可能性。此外我有一块A100显卡可以在较短时间内完成各种计算。我也愿意分享我的成果。 我的问题是，1. 我对音频文件的录制格式，编码，没有经验，可以简单讲一下和这个aidatatang数据集相同的音频格式是有什么参数需要我在录制和process的过程中需要注意的嘛？ 2. 我没有过多去了解aidatatang 数据组里 .metadata 和 .trn 文件的用途，可以大致说一下么？ 3. 有更多细节我们可以私信交流一下么有没有可能提取游戏里的音频素材，或者关闭背景音乐后用软件录制> > 老哥，我留意到你的aidatatang200 数据集里声音背景噪音非常重，而且大部分是男声，我对这个项目非常感兴趣，计划按照你的重新找女声（涵盖不同音色声线，萝莉，少女，御姐）重新录制干净无噪声的数据集，我也在思考男女声分开训练的可能性。此外我有一块A100显卡可以在较短时间内完成各种计算。我也愿意分享我的成果。 我的问题是，1. 我对音频文件的录制格式，编码，没有经验，可以简单讲一下和这个aidatatang数据集相同的音频格式是有什么参数需要我在录制和process的过程中需要注意的嘛？ 2. 我没有过多去了解aidatatang 数据组里 .metadata 和 .trn 文件的用途，可以大致说一下么？ 3. 有更多细节我们可以私信交流一下么> > 有没有可能提取游戏里的音频素材，或者关闭背景音乐后用软件录制理论可行，可以联系我讨论> > > 老哥，我留意到你的aidatatang200 数据集里声音背景噪音非常重，而且大部分是男声，我对这个项目非常感兴趣，计划按照你的重新找女声（涵盖不同音色声线，萝莉，少女，御姐）重新录制干净无噪声的数据集，我也在思考男女声分开训练的可能性。此外我有一块A100显卡可以在较短时间内完成各种计算。我也愿意分享我的成果。 我的问题是，1. 我对音频文件的录制格式，编码，没有经验，可以简单讲一下和这个aidatatang数据集相同的音频格式是有什么参数需要我在录制和process的过程中需要注意的嘛？ 2. 我没有过多去了解aidatatang 数据组里 .metadata 和 .trn 文件的用途，可以大致说一下么？ 3. 有更多细节我们可以私信交流一下么> > > > > > 有没有可能提取游戏里的音频素材，或者关闭背景音乐后用软件录制> > 理论可行，可以联系我讨论你好 已发到你的gmail里 babysor00@gmail.com 上面的二维码过期了新增标贝数据支持BZNSYPMozillaCommonVoice以上两个数据集支持在最新的主版本中没有看到？难道还没能合并进来吗？> BZNSYP都在分支上， 给个思路，那些galgame都是几十万字的文本与语音对应的数据集牵涉版权问题哦 🌱> 3\. 话，可以走邮件，或者扫这里的二维码： > >> > > > 推荐aishell3数据集，稍微干净一些，但是数据量很少。另外datasets_root确定是 而不是 > > > > > > > > > 是的，如果理解参数名，这里原本是希望同时支持多个dataset,所以叫datasets_root。> > > > > > 那这里如果我把aishell3和slr68的数据都放在datasets_root文件夹内，就可以同时跑两个数据集吗> > 程序逻辑还没实现 囧 目前我是手动分开跑作者您好，请问现在支持自动跑多个数据集了吗？如果手动分开跑，是如何操作的呢？> > 3. 话，可以走邮件，或者扫这里的二维码： > >> > > > > > 推荐aishell3数据集，稍微干净一些，但是数据量很少。另外datasets_root确定是 而不是 > > > > > > > > > > > > 是的，如果理解参数名，这里原本是希望同时支持多个dataset,所以叫datasets_root。> > > > > > > > > 那这里如果我把aishell3和slr68的数据都放在datasets_root文件夹内，就可以同时跑两个数据集吗> > > > > > 程序逻辑还没实现 囧 目前我是手动分开跑> > 作者您好，请问现在支持自动跑多个数据集了吗？如果手动分开跑，是如何操作的呢？可以， 分开跑比较麻烦，因为混合起来就不太能区分了请问有粤语数据集吗？https://www.openslr.org/18/THCHS-30 数据集，体量较小，想拿来练手用> THCHS-30 数据集，体量较小，想拿来练手用最好确保有100hrs级别的语音 
这个项目需要自己训练吗？
Pretrained-models下了放根目录不行  拷贝了到相对应的文件目录才能启动工具箱只能load数据集的语音无法使用解析功能说明不详细不会用啊出一个详细的步骤文档吧不着急，这个确实要自己的训练的。你已经成功迈出第一步，你可以把这一步替换的步骤细化直接在项目修改 成为项目贡献者 感谢> 不着急，这个确实要自己的训练的。你已经成功迈出第一步，你可以把这一步替换的步骤细化直接在项目修改 成为项目贡献者 感谢你应该把文档做的详细些，做到按步骤来就可以跑起来。接你文档来python synthesizer_preprocess_audio.py <datasets_root>这里就报错了也不知道哪些步骤是否是必须的既然选择了开源，其时你也可以提供一些训练好的模型直接搭好就可以演示start就上来了> > 不着急，这个确实要自己的训练的。你已经成功迈出第一步，你可以把这一步替换的步骤细化直接在项目修改 成为项目贡献者 感谢> > 你应该把文档做的详细些，做到按步骤来就可以跑起来。> 接你文档来python synthesizer_preprocess_audio.py <datasets_root>这里就报错了> 也不知道哪些步骤是否是必须的> 既然选择了开源，其时你也可以提供一些训练好的模型> 直接搭好就可以演示start就上来了感谢你的反馈，已经补充使用预训练步骤的描述（源wiki里面英文版描述较为详细）同时，训练模型有个早期训练后上传的下载Link：https://pan.baidu.com/s/10t3XycWiNIg5dN5E_bMORQCode：aid4 
Supporting new dataset SLR68 ! try python synthesizer_preprocess_audi…
…o.py ...\slr --dataset SLR68 
在运行 python synthesizer_preprocess_audio.py 时报错 
duplicated 
如何解决运行python synthesizer_preprocess_audio.py时报错 DLL load failed:页面文件太小，无法完成操作
我在运行 python synthesizer_preprocess_audio.py时遇到如上错误 ，在CSDN上找到解决方法：1.如果python 运行环境不在C盘  查看高级系统设置->高级->性能 设置->高级->虚拟内存->更改 ->取消自动管理所有驱动器的分页文件大小-> 自定义大小 ->初始大小和最大值设为10240    2. 更改DateLoade 中的参数num_worker 改为0 但我现在不清楚具体怎样把参数设为0 是不是python版本的问题，本机python3.7 和colab没有出现这个问题> 是不是python版本的问题，本机python3.7 和colab没有出现这个问题我用的是  python 3.8.5 > > > > 是不是python版本的问题，本机python3.7 和colab没有出现这个问题> > 我用的是  python 3.8.5那你换3.7试试 我用的anaconda试过查看机器上是否有过多的python进程吗》虚拟内存改成100G就好了新笔记本电脑5800h  Cpu%2使用  运行python synthesizer_preprocess_audio.py直接到100% 裂开> 新笔记本电脑5800h Cpu%2使用 运行python synthesizer_preprocess_audio.py直接到100%> 裂开兄弟我也笔记本，只能跑小数据集> > 新笔记本电脑5800h Cpu%2使用 运行python synthesizer_preprocess_audio.py直接到100%> > 裂开> > 兄弟我也笔记本，只能跑小数据集项目启不起来，你给的模型不能用，，看你视频也是选了两个模型啊，你只传了一个。不知道是哪里没弄好，跑不起来自己运行python synthesizer_preprocess_audio.py 电脑就炸> > > 新笔记本电脑5800h Cpu%2使用 运行python synthesizer_preprocess_audio.py直接到100%> > > 裂开> > > > > > 兄弟我也笔记本，只能跑小数据集> > 项目启不起来，你给的模型不能用，，看你视频也是选了两个模型啊，你只传了一个。> > 不知道是哪里没弄好，跑不起来> > 自己运行python synthesizer_preprocess_audio.py 电脑就炸我是cpu 100%跑一小时预热> 我是cpu 100%跑一小时预热我是加载数据集处理的时候就炸了，根本到不了训练的那里感觉aidatatang这个数据集质量不太行啊 全是噪音，有些人的话也说不清楚自己找了个很小的数据集，不知道会不会卡，因为不会处理数据 0.0能说说处理数据的原理吗> > 我是cpu 100%跑一小时预热> > 我是加载数据集处理的时候就炸了，根本到不了训练的那里> 感觉aidatatang这个数据集质量不太行啊 全是噪音，有些人的话也说不清楚> 自己找了个很小的数据集，不知道会不会卡，因为不会处理数据 0.0> 能说说处理数据的原理吗看这一段 中文字幕转 phoneme -> mel, 其他就是把phoneme和音频文件对应起来可以加一下我wechat一起沟通一下，1866198四 补充一个新数据集 
关于aidatatang_200zh的问题
我尝试从aidatatang_200zh的官网上下载，是要把aidatatang_200zh\aidatatang_200zh\aidatatang_200zh\corpus\train下的文件全部解压吗？是的，建议路径不要这么深，到时候需要拼一下。这里的解压可以用bash的pipeline: 指定 datasets 所在的根路径就行了，也可以用这个pipeline： 
请问可以提供预训练的编码器/声码器吗？
 Output: 这个地方有：https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models> 这个地方有：> @miven 方便pr一个 补充一下使用指引吗？ 
在运行demo_cli.py时出错
我同时下载了原模型和你的模型，但是在运行demo_cli.py时出现以下错误：RuntimeError: Error(s) in loading state_dict for Tacotron:        size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([66, 512]) from checkpoint, the shape in current model is torch.Size([70, 512]).> 我同时下载了原模型和你的模型，但是在运行demo_cli.py时出现以下错误：> > RuntimeError: Error(s) in loading state_dict for Tacotron:> size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([66, 512]) from checkpoint, the shape in current model is torch.Size([70, 512]).demo_cli.py没有被完整测试，你有试过demo_toolbox吗？ 之前我遇到过这样的问题，都是pytorch版本不同导致的，临时解决方式如果只是应用模型，就用先用cpu好的，我去试一试，十分感谢遇到同样的问题，看了一下代码，应该是因为你没有配置用 mandarin.pt 做 synthesizer，还是用了 default 从 原来 model 下载的 synthesizer/saved_models/pretrained/pretrained.ptTry this： Dig 了一下问题，应该是，英文原版的 symbol list 是:  长度是 66这个repo 的 symbol list是   长度是 70.所以在 Construct Tacotron 的时候会报 dimension not correct 的错误> 遇到同样的问题，看了一下代码，应该是因为你没有配置用 mandarin.pt 做 synthesizer，还是用了 default 从 原来 model 下载的 synthesizer/saved_models/pretrained/pretrained.pt> > Try this：> > > Dig 了一下问题，应该是，英文原版的 symbol list 是:>  长度是 66> > 这个repo 的 symbol list是>  长度是 70.> 所以在 Construct Tacotron 的时候会报 dimension not correct 的错误最新公布的模型已经长度是75了，代码里的  长度是 70，两者又不匹配了。修改代码为  ，程序可以跑通，但是合成的中文要么是纯噪音，要么就是质量很差，不知道是我哪里使用有问题：使用如下：python3 demo_cli.py --syn_model_fpath=synthesizer/saved_models/mandarin/mandarin.pt> > 遇到同样的问题，看了一下代码，应该是因为你没有配置用 mandarin.pt 做 synthesizer，还是用了 default 从 原来 model 下载的 synthesizer/saved_models/pretrained/pretrained.pt> > Try this：> > > > Dig 了一下问题，应该是，英文原版的 symbol list 是:> >  长度是 66> > 这个repo 的 symbol list是> >  长度是 70.> > 所以在 Construct Tacotron 的时候会报 dimension not correct 的错误> > 最新公布的模型已经长度是75了，代码里的  长度是 70，两者又不匹配了。修改代码为  ，程序可以跑通，但是合成的中文要么是纯噪音，要么就是质量很差，不知道是我哪里使用有问题：> 使用如下：> python3 demo_cli.py --syn_model_fpath=synthesizer/saved_models/mandarin/mandarin.pt可以加我微信看一下 1866198四 我猜是没有训练出来attention 线> mandarin.pt> > 遇到同样的问题，看了一下代码，应该是因为你没有配置用 mandarin.pt 做 synthesizer，还是用了 default 从 原来 model 下载的 synthesizer/saved_models/pretrained/pretrained.pt> > Try this：> > > > Dig 了一下问题，应该是，英文原版的 symbol list 是:> >  长度是 66> > 这个repo 的 symbol list是> >  长度是 70.> > 所以在 Construct Tacotron 的时候会报 dimension not correct 的错误> > 最新公布的模型已经长度是75了，代码里的  长度是 70，两者又不匹配了。修改代码为  ，程序可以跑通，但是合成的中文要么是纯噪音，要么就是质量很差，不知道是我哪里使用有问题：> 使用如下：> python3 demo_cli.py --syn_model_fpath=synthesizer/saved_models/mandarin/mandarin.pt你train的mandarin.pt的记录plot里面最新的两个截图我看看？隐藏未测试的demo_cli文件，关闭本issue> > mandarin.pt> > > > 遇到同样的问题，看了一下代码，应该是因为你没有配置用 mandarin.pt 做 synthesizer，还是用了 default 从 原来 model 下载的 synthesizer/saved_models/pretrained/pretrained.pt> > > Try this：> > > > > > Dig 了一下问题，应该是，英文原版的 symbol list 是:> > >  长度是 66> > > 这个repo 的 symbol list是> > >  长度是 70.> > > 所以在 Construct Tacotron 的时候会报 dimension not correct 的错误> > > > > > 最新公布的模型已经长度是75了，代码里的  长度是 70，两者又不匹配了。修改代码为  ，程序可以跑通，但是合成的中文要么是纯噪音，要么就是质量很差，不知道是我哪里使用有问题：> > 使用如下：> > python3 demo_cli.py --syn_model_fpath=synthesizer/saved_models/mandarin/mandarin.pt> > 你train的mandarin.pt的记录plot里面最新的两个截图我看看？mandarin.pt就是你放在百度网盘上的模型Link：https://pan.baidu.com/s/10t3XycWiNIg5dN5E_bMORQCode：aid4有没有闪退呢，我之前启动GUI闪退了然后按照作者这个方法就可以了，链接：https://github.com/babysor/MockingBird/wiki/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF 
